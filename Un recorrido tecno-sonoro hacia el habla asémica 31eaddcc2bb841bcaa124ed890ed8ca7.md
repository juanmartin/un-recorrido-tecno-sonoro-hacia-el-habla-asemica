# Un recorrido tecno-sonoro hacia el habla as√©mica

> Trabajo Final de Grado para la Licenciatura en Artes Electr√≥nicas de la Universidad de Tres de Febrero
> 

> por Juan Mart√≠n Sesali Maydana. Con la tutor√≠a de mi amigo y referente Leandro Garber.
> 
- **Agradecimientos:**
    
    - A mi mam√°, Marisa Maydana, por encontrar esta carrera en un momento clave en mi vida. Por su apoyo amoroso incondicional y la confianza en mis delirios.
    - A Lean, por guiarme en este camino.
    - A Gallardo, espacio donde descubr√≠ la felicidad nerdeando con amigxs. A toda la crew: Pilu, Polo, Fran, Peli, +.
    - A Gala Luc√≠a Gonz√°lez Barrios, amiga, colega, referente, y compa√±era de tecno-aventuras a lo largo de todos estos a√±os.
    - A Tom√°s Ciccola y Nahuel Rodrigues, amigos, hermanos con quien compart√≠ un sinfin de experiencias.
    - A Margarita Molfino, mi referente en materia de la sensibilidad corporal.
    - A Violeta Guitart, por todo su amor y ayuda te√≥rica en este √∫ltimo tramo.
    - A toda la gente incre√≠ble que conoc√≠ en o gracias a la UNTREF.
    //PASAR A P√ÅRRAFO
    

> Dedicado a mi viejo, Jorge A. Sesali (1958-2020).
> 

# √çndice

# Abstract

Este trabajo investiga el concepto del *habla as√©mica*, el habla sin sentido sem√°ntico, a trav√©s de la exploraci√≥n y aplicaci√≥n de diversas tecnolog√≠as incluyendo inteligencia artificial. Centr√°ndose en la sonoridad del lenguaje, se plantea una aventura tecnol√≥gica que parte de la idea de crear un sintetizador de habla as√©mica que puede generar habla que se asemeja a varias lenguas sin contenido sem√°ntico, es decir, sin decir nada concretamente. El desarrollo tambi√©n aborda la posibilidad de mezclar o interpolar lenguas y explorar lo que existe *entre* ellas a trav√©s de la inyecci√≥n de datos en el espacio latente de una red neuronal o mezclando din√°micamente fragmentos de audio de voces habladas. A trav√©s de la investigaci√≥n y el desarrollo de software, se busca proporcionar una nueva forma de entender los aspectos sonoros formales que identifican a un lenguaje particular.

# Introducci√≥n

## Corpus Art√≠stico???

completar

La operaci√≥n recurrente que realizo en las piezas y obras que hice a lo largo de mi carrera es la de despojar de sentido -o de al menos el sentido conocido o cotidiano- a la mirada que tenemos sobre el mundo, sus cosas, la existencia. Al liberar el contenido sem√°ntico de las cosas, da lugar a re-generar el sentido, desde una perspectiva nueva.

mi cuerpo de obra relacionado a mirar las cosas desde otro punto de vista, proponiendo una mirada despojada de sentido *o del sentido conocido/cotidiano* para volver a generar un nuevo sentido desde esta perspectiva "virgen‚Äù.

obras afines que hice:

- museo mar anamorfismos

[No son se√±ales](https://www.juanm.art/in/projects/nosonseniales)

- parallax cryptography web

[Parallax-Cryptography](https://www.juanm.art/in/projects/paracrypt)

SIMBOLOS ARBITRARIOS LENGUAJE.

- 
- serie alineaciones fotos no publicadas
- patentes (mensajes)

[Instagram (@patentepatente)](https://www.instagram.com/patentepatente)

- sonido que no se percibe y solo se percibe cuando se detiene: fenomeno heladera. se hizo en PD para materia de varchausky.

entonces el desaf√≠o final es despojar el lenguaje de sentido (y por eso es mi tesis de grado) porque el lenguaje es el medio fundamental/principal a trav√©s del cual producimos sentido. el mundo de las ideas plat√≥nico no ser√≠a posible sino por el lenguaje. si bien existen s√≠mbolos y otras formas de comunicar alg√∫n sentido, todo termina siendo explicado por el lenguaje en su medio visual o sonoro.

ver si este p√°rrafo va ac√° aparte o en prop√≥sito y motivaci√≥n.

## Prop√≥sito y Motivaci√≥n

El sonido en el habla comenz√≥ a interesarme a partir de un viaje en el que recorr√≠ varios pa√≠ses europeos. Escuchar a la gente a mi alrededor despert√≥ una inquietud acerca de c√≥mo sonaban los distintos idiomas. Grabando material y mediante una escucha reducida comenc√© a establecer ciertas relaciones entre lenguas que, en un plano geogr√°fico y/o cultural, no ten√≠an relaci√≥n alguna. ¬øQu√© suceder√≠a si se plantea un mapa donde las distancias entre territorios est√©n representadas acorde a la similitud de sus *fonemas*? Siendo fonemas una unidad m√≠nima de sonido para formar palabras.

Resulta curioso cuando las personas pueden identificar los idiomas sin saber hablarlos; quiz√°s escuchando m√∫sica de alg√∫n lugar, TV o cualquier material donde se escuche gente hablando, algunas personas logran identificar correctamente qu√© idioma est√°n escuchando. ¬øPor qu√© sucede esto? **¬øQu√© caracter√≠sticas de una lengua sintetizan su identidad?** ¬øCu√°les son los aspectos que los vuelven *familiares*?

Me interesa en particular el habla que no hace sentido, as√≠ como existe la escritura as√©mica, por qu√© no pensar en *habla as√©mica*, como un balbuceo. Entre estas preguntas e inquietudes, m√°s el uso de tecnolog√≠as de inteligencia artificial surge la idea de crear un *sintetizador de habla as√©mica*, es decir, habla que carezca de sentido, lo cual lo diferencia de un texto a habla (TTS) en el que el usuario escribe un texto y luego es reproducido por una voz sintetizada. Este software ser√≠a capaz de generar habla que nos remita a cierta lengua que el usuario pueda elegir, incluso la posibilidad de *mezclar* varias, sin ingresar contenido sem√°ntico. La mezcla consistir√≠a en la interpolaci√≥n dentro de la red neuronal generadora buscando lo que hay dentro del *espacio latente* luego de haber sido entrenada con grandes datasets de audio clasificado en cada lengua. De esta manera podr√≠amos establecer la *forma* conocida, o familiar, y la *forma* desconocida, que viene de esta interpolaci√≥n.

Quiz√°s esta tecnolog√≠a pueda dar al menos un acercamiento a conocer los aspectos formales sonoros que identifican a un lenguaje particular, y adem√°s descubrir qu√© hay, por ejemplo, *entre* algunos idiomas en los que intuitivamente podemos hallar similitudes como fonemas coincidentes o aquellos que sean completamente ajenos entre s√≠. ¬øQu√© sucede entre lenguas? ¬øEsos resultados nos ser√°n familiares? ¬øPor qu√©? ¬øQu√© otras preguntas pueden llegar a surgir a partir de escuchar estos nuevos sonidos?

El proyecto pretende investigar este aspecto y as√≠ generar espacios para resultados concretos como el software de habla propuesto anteriormente, recreaci√≥n de voces como familiares y la propia, performances de *sono-lenguas,* piezas sonoras, instalaciones, personajes virtuales que las hablen, entre otros. Esta exploraci√≥n propone un recorrido donde sus ramas se retroalimenten con estos resultados para continuar profundizando, generando un mapa o mejor a√∫n, una red de conocimiento.

## Alcance de la investigaci√≥n

En principio este trabajo fue pensado como un recorrido an√°logo al de una investigaci√≥n art√≠stica, guiado por intereses personales acerca del lenguaje y su sonoridad. El objetivo material fue el desarrollo de un software capaz de generar *habla as√©mica*, es decir, aquella que simule habla humana eliminando el contenido sem√°ntico y dejando s√≥lo su sonoridad.

A partir de este desarrollo como gu√≠a del proceso, se espera -y se celebra- la emergencia de resultados o salidas tanto materiales como te√≥ricas. Este escrito es el relato del recorrido que fue d√°ndose teniendo como objetivo dicho software, incluyendo las ideas y los desaf√≠os que surgieron, los experimentos y sus peque√±as piezas, y por supuesto este mismo texto como parte de una obra en constante evoluci√≥n y proceso que no concluye con el fin de esta narraci√≥n, sino que es puntapi√© para seguir investigando.

# Estado de la Cuesti√≥n

// REVISAR REDUNDANCIA EN EL CONTENIDO //

Este trabajo realiza una operaci√≥n de interpelaci√≥n al imaginario de quienes lo experimentan, buscando empatizar con ciertos sectores de nuestra memoria para evocar una sensaci√≥n de familiaridad, acoplarse a una parte de nuestra experiencia que no alcanza a ser racionalizada. Ese lugar intermedio es de sumo inter√©s para mi trabajo. A continuaci√≥n, dar√© a conocer algunas referencias que considero relevantes para llevar a cabo este experimento, parte del camino de la investigaci√≥n, incluyendo aspectos t√©cnicos -tanto tecnol√≥gicos como te√≥ricos del habla y la lengua- que lo posibilitan, experiencias tangenciales al objeto de estudio, y algunas referencias dentro del campo del arte sonoro donde podr√≠a inscribirse este desarrollo.

### Tecnolog√≠a involucrada y su aplicaci√≥n en desarrollos creativos vinculados al arte

La generaci√≥n de audio por inteligencia artificial (IA) ha experimentado un r√°pido avance en los √∫ltimos a√±os, lo que ha llevado al desarrollo de diversas tecnolog√≠as disponibles en este campo. Estas tecnolog√≠as aprovechan algoritmos de aprendizaje autom√°tico y modelos de redes neuronales entrenados con grandes cantidades de datos de voz para generar pronunciaci√≥n, entonaci√≥n y cadencia similares a las de un hablante humano. Esto ha abierto nuevas posibilidades en √°reas como asistentes virtuales, audiolibros, narraci√≥n de texto y accesibilidad. Adem√°s permiten realizar tareas como la eliminaci√≥n de ruido de grabaciones, la separaci√≥n de fuentes de audio, la mejora de calidad de audio y la conversi√≥n de voz a diferentes estilos y timbres.

Las tecnolog√≠as de conversi√≥n texto a voz (TTS, por sus siglas en ingl√©s) tambi√©n han experimentado mejoras significativas gracias a la IA. Estos sistemas pueden convertir texto escrito en voz de forma automatizada y natural, lo que ha impulsado aplicaciones como audiolibros, asistentes virtuales y sistemas de accesibilidad. En el marco de mi investigaci√≥n elimino la parte del texto en la generaci√≥n de habla, ya que no existe dimensi√≥n sem√°ntica en el habla a generar.

### Dataset

Un dataset de audio es de vital importancia para la generaci√≥n de habla humana mediante inteligencia artificial. Este tipo de dataset consiste en una colecci√≥n de grabaciones de voz humana que se utilizan para entrenar modelos de generaci√≥n de habla.

La calidad y la diversidad del dataset son aspectos cruciales. Es importante que el dataset contenga una amplia variedad de voces, acentos, g√©neros y edades para garantizar que el modelo pueda generar habla que sea representativa de la diversidad humana. Adem√°s, es necesario contar con una alta calidad de grabaci√≥n para capturar los detalles y matices del habla, incluso aunque no tenga la necesidad de alcanzar un nivel de especificidad que permita inteligibilidad y sentido en la posterior generaci√≥n de muestras de habla.

Uno de los desaf√≠os principales al trabajar con un dataset de audio para la generaci√≥n de habla humana es el problema de variaci√≥n de canal. Este problema se refiere a las diferencias ac√∫sticas y de calidad que surgen debido a las variaciones en el sistema y los entornos de grabaci√≥n y dem√°s condiciones t√©cnicas. Estas variaciones pueden afectar la calidad del habla generada, quiz√°s provocando ruido indeseado.

Para abordar el problema de variaci√≥n de canal, es importante recopilar un dataset que contemple una amplia gama de condiciones de grabaci√≥n. Esto implica capturar grabaciones en diferentes entornos, utilizando diferentes micr√≥fonos y configuraciones t√©cnicas. Adem√°s, se pueden aplicar t√©cnicas de preprocesamiento y normalizaci√≥n para reducir las variaciones no deseadas en el audio y garantizar una mayor consistencia en los datos. Es importante controlar estas condiciones en la mayor medida posible para que el modelo sea capaz de generalizar sobre las caracter√≠sticas del habla, y descartar las variables de entorno.

### T√©cnicas espec√≠ficas Tecnolog√≠as de generaci√≥n de audio por inteligencia artificial

Para entrar en detalle acerca de las t√©cnicas disponibles, har√© una breve descripci√≥n centrada en la generaci√≥n de sonido de cada una de las investigadas:

- Generative Adversarial Networks (GAN)

Las GAN son redes neuronales que consisten en un generador y un discriminador. El generador crea muestras de audio a partir de ruido aleatorio, mientras que el discriminador eval√∫a la autenticidad de las muestras generadas. Una ventaja de las GAN es su capacidad para generar muestras de audio de alta calidad y realistas. Sin embargo, pueden ser dif√≠ciles de entrenar y pueden sufrir de inestabilidad en el aprendizaje. [(Goodfellow et al., 2014)](https://www.zotero.org/google-docs/?BPDIgP).

- **Autoencoders**

Los autoencoders son redes neuronales que aprenden a comprimir y descomprimir datos. En el contexto del audio pueden aprender a representar caracter√≠sticas de audio en un espacio de menor dimensi√≥n y generar nuevas muestras de audio. Una ventaja de los autoencoders es su capacidad para aprender representaciones compactas y generar muestras de audio con coherencia estructural a trav√©s del recorrido del espacio latente, que es esta representaci√≥n en la m√≠nima dimensi√≥n posible para luego generar las nuevas muestras. Sin embargo, la calidad del audio generado puede no ser tan alta como en otras t√©cnicas m√°s avanzadas. [(Bengio et al., 2013)](https://www.zotero.org/google-docs/?2xQwzm)

A√∫n as√≠, esta ser√° la t√©cnica elegida para el desarrollo del software central de este trabajo. Recorrer la representaci√≥n comprimida de un modelo entrenado -su espacio latente- puede resultar en audios de habla donde las caracter√≠sticas principales del idioma sean evidentes, pero nunca llegar√°n a ser lo suficientemente espec√≠ficas para expresar contenido sem√°ntico. Por √∫ltimo, pero no por eso menos importante, esta t√©cnica es bastante eficiente en cuanto a recursos por lo que su costo y tiempo de entrenamiento son accesibles en el marco de esta investigaci√≥n.

- Stable Diffusion

El Stable Diffusion es un enfoque que utiliza modelos generativos de difusi√≥n para la generaci√≥n de audio. Se basa en la difusi√≥n de un proceso estoc√°stico en el tiempo para generar muestras de audio realistas. Una ventaja del Stable Diffusion es su capacidad para generar muestras de audio de alta calidad y con mayor diversidad. Sin embargo, su implementaci√≥n puede ser compleja y requerir una mayor capacidad computacional. [(Rombach et al., 2022)](https://www.zotero.org/google-docs/?4Wt8nj)

- WaveNet

WaveNet es un modelo generativo basado en redes neuronales recurrentes que utiliza una arquitectura de onda a onda para generar audio de alta calidad. Una ventaja de WaveNet es su capacidad para generar audio extremadamente realista y detallado, capturando incluso peque√±as variaciones en el sonido. Sin embargo, su entrenamiento y generaci√≥n son computacionalmente intensivos, lo que puede requerir recursos significativos. [(Oord et al., 2016)](https://www.zotero.org/google-docs/?iM6gtl).

- Transformadores

Los modelos basados en transformadores, como GPT (Generative Pre-trained Transformer), se han aplicado con √©xito a la generaci√≥n de audio. Utilizan una atenci√≥n global sobre el contexto para generar secuencias de audio coherentes y realistas. Como ventaja de los transformadores es su capacidad para capturar relaciones de largo alcance en el audio y generar muestras de alta calidad. Sin embargo, pueden requerir una gran cantidad de datos de entrenamiento y tiempo computacional. [(Vaswani et al., 2017)](https://www.zotero.org/google-docs/?tFHNDX).

- Variational Autoencoders (VAE)

Los VAE son modelos generativos que permiten aprender y muestrear el espacio latente de una distribuci√≥n de datos. En el contexto del audio, los VAE se han utilizado para generar nuevos sonidos y efectos de sonido creativos. Los VAE poseen gran capacidad para generar muestras de audio diversas y explorar el espacio latente de manera controlada. Sin embargo, pueden generar muestras de menor calidad en comparaci√≥n con otras t√©cnicas m√°s avanzadas. [(Kingma & Welling, 2022)](https://www.zotero.org/google-docs/?0npyeW).

- Redes Generativas de Flujos (Generative Flows)

Esta t√©cnica se basa en el aprendizaje de transformaciones invertibles que mapean una distribuci√≥n simple a una distribuci√≥n de audio m√°s compleja. Las redes generativas de flujos pueden generar muestras de audio de alta calidad y permiten un muestreo y manipulaci√≥n m√°s flexible del espacio de audio. Una ventaja de las redes generativas de flujos es su capacidad para generar muestras de audio realistas y permitir una mayor manipulaci√≥n creativa. Sin embargo, su entrenamiento puede ser computacionalmente costoso. [(Papamakarios et al., 2021)](https://www.zotero.org/google-docs/?K3KCJi).

### Casos de aplicaci√≥n

**Granma MagNet** [(Akten, n.d.)](https://www.zotero.org/google-docs/?Mnuh5z) es un proyecto de s√≠ntesis de audio profundo con redes neuronales. Su objetivo no es crear sonidos que sean indistinguibles de los datos de entrenamiento (es decir, capaces de pasar una "prueba de Turing de audio"), sino crear sonidos que sean de alguna manera reminiscentes de los datos de entrenamiento, pero tambi√©n novedosos y no humanos, pero a√∫n interesantes, y, lo m√°s importante, permitir a un usuario humano manipular de manera expresiva y significativa la salida en tiempo real.

No s√≥lo es interesante este proyecto por la generaci√≥n de sonidos que hagan referencia a su entrada al sistema, sino por la acci√≥n de interpolar naturalmente entre sonidos con la ayuda de IA, lo cual es un recurso que aplica a mi investigaci√≥n en la b√∫squeda de nuevos lenguajes que existan *en el medio* de otros preexistentes.

**Dadabots** [(*Dadabots*, n.d.)](https://www.zotero.org/google-docs/?WReq2I) es un proyecto de generaci√≥n de m√∫sica a trav√©s de herramientas de IA. Con modelos de aprendizaje autom√°tico, espec√≠ficamente SampleRNN han logrado recrear estilos cl√°sicos de *m√∫sica humana* como punk, hard rock, metal, hardcore, math rock, entre otros. Esto es debido a que ciertos g√©neros musicales poseen sonoridades bien distinguibles y relativamente f√°ciles de generalizar. Con tan solo alimentar los modelos con influencias del estilo, ciertos g√©neros son reproducibles de manera fiel. Fueron de los primeros en realizar este tipo de operaciones con tanta precisi√≥n, y adem√°s publican sus papers en investigaci√≥n acerca del asunto. La relevancia de este proyecto es que es uno de los primeros antecedentes de generaci√≥n de audio tan fiel a su fuente, cumpliendo su objetivo, publicando en el √°mbito acad√©mico y compartiendo su c√≥digo en su GitHub para poder recrearlo.

**Wekinator** [(Wekinator, n.d.)](https://www.zotero.org/google-docs/?yQPOjV) es un software creado por Rebecca Fiebrink en 2009 que permite a los usuarios entrenar modelos de aprendizaje autom√°tico en tiempo real para crear aplicaciones interactivas y sistemas de control personalizados. Su enfoque en la interactividad y la creatividad lo hace especialmente adecuado para procesos creativos en m√∫sica, arte, dise√±o interactivo y performance, ofreciendo a los artistas y dise√±adores nuevas formas de explorar la interacci√≥n humano-m√°quina.

Este software fue uno de los primeros encuentros que tuve con IA y vale la pena mencionarlo ya que en el campo de las artes electr√≥nicas podr√≠a considerarse como una navaja suiza de la inteligencia artificial, dado que de manera muy intuitiva uno puede *mapear* gestos desde alguna entrada como una c√°mara o sensor hacia datos f√°cilmente traducibles en audio, imagen o la interacci√≥n con actuadores f√≠sicos.

**AudioStellar** es un software de exploraci√≥n de *constelaciones* conformadas por una gran cantidad de muestras de sonido. Puede utilizarse tanto como un visualizador para encontrar r√°pidamente los *samples* que uno necesita para la realizaci√≥n de piezas sonoras, simplemente organizaci√≥n visual, o mejor a√∫n, como herramienta perform√°tica gracias a sus intuitivos y creativos modos de recorrer estos espacios sonoros.

En principio debe generarse el mapa basado en una colecci√≥n de muestras -preferentemente de corta duraci√≥n- a trav√©s de alguno de los algoritmos de clasificaci√≥n disponibles. Una vez terminado el proceso, el software muestra cada sonido como un punto en un mapa bidimensional. La idea es que est√©n organizados por similitud, es decir, por caracter√≠sticas t√≠mbricas que los ubican en un √°rea cercana. Por ejemplo, cortes de voces -vocal chops- estar√≠an en un sector lejos de otros como golpes de bater√≠a.

Imagino este software como una utilidad para un posible output de mi trabajo. La idea de organizar fonemas, sin importar el lenguaje, en un mapa resulta en una clara forma de visibilizar cercan√≠as o lejan√≠as entre sonidos de distintos idiomas. ¬øQu√© escucharemos al recorrerlos en las novedosas formas que propone este software? ¬øPodr√≠amos pensar en nuevas sonoridades ling√º√≠sticas a partir de organizar recorridos a trav√©s de fonemas provenientes de diferentes idiomas?

### Cultura popular

En el √°mbito de la m√∫sica popular, las voces sintetizadas han desempe√±ado un papel importante en la creaci√≥n de estilos musicales √∫nicos y reconocibles. Estas voces sint√©ticas, generadas mediante el uso de hardware y software de s√≠ntesis de voz, han sido utilizadas por diversos artistas para agregar un elemento distintivo a sus producciones musicales.

Un ejemplo destacado de la incorporaci√≥n de voces sintetizadas en la m√∫sica popular es el grupo alem√°n Kraftwerk. Pioneros en el g√©nero de la m√∫sica electr√≥nica, utilizaron voces robotizadas y sintetizadas en muchas de sus canciones. Su √°lbum ic√≥nico "Autobahn" (1974) presenta voces sintetizadas que aportan a su est√©tica futurista y mec√°nica, complementando su estilo musical innovador.

Otro ejemplo relevante es el d√∫o franc√©s Daft Punk. Con su enfoque √∫nico en la m√∫sica electr√≥nica y el uso de voces rob√≥ticas, Daft Punk ha logrado crear un sonido √∫nico y reconocible en algunos de sus √©xitos como "Around the World" y "Harder, Better, Faster, Stronger".

La utilizaci√≥n de voces sintetizadas en la m√∫sica popular permite a los artistas explorar nuevas formas de expresi√≥n y crear atm√≥sferas √∫nicas en sus composiciones. Al eliminar las limitaciones vocales humanas tradicionales, las voces sint√©ticas ofrecen un amplio abanico de posibilidades creativas y permiten a los m√∫sicos experimentar con diferentes estilos y g√©neros musicales. ¬øC√≥mo podr√≠amos insertar una nueva herramienta de generaci√≥n de habla sint√©tica, que no apele al significado si no a una pura propiedad morfol√≥gica del lenguaje en aporte a su est√©tica?

Probablemente conozcan el videojuego **Los Sims** lanzado en el a√±o 2000. Fue desarrollado por Maxis Studios y publicado por Electronic Arts. En este juego de simulaci√≥n de vida, los personajes se expresan y conversan entre s√≠ utilizando un idioma que no existe, con reminiscencias a una combinaci√≥n de varios que s√≠ existen.

En los videojuegos y otras ficciones encontramos repetidas iteraciones de esta operaci√≥n, pero considero este caso bien claro y lo suficientemente insertado en el imaginario colectivo para darle especial relevancia. Que sea parte de nuestro imaginario, por ejemplo, en las generaciones que lo jugamos, es importante para el reconocimiento de este tipo de sonoridades. Lo mismo ocurre con los idiomas del mundo.

Una idea interesante es que cuanto m√°s abstracta sea una representaci√≥n, m√°s se puede insertar en eso la propia subjetividad. Cuanto m√°s detalle se agregue, menos lugar queda a la intuici√≥n y la imaginaci√≥n. ¬øCu√°l es el punto de equilibrio entre abstracci√≥n y especificidad que necesitamos para reconocer un idioma y que al mismo tiempo permita que nuestra creatividad complete los huecos sem√°nticos?

### Ling√º√≠stica

Comenzando por el universo textual, es relevante mencionar la escritura as√©mica, que es una forma de escritura que se caracteriza por su falta de contenido sem√°ntico o legibilidad convencional. El mismo recurso que tratar√© en este trabajo en la dimensi√≥n sonora. Esta se trata de una forma de expresi√≥n en la que los trazos, s√≠mbolos y gestos de la escritura se utilizan para crear una experiencia est√©tica y emocional en lugar de transmitir un significado ling√º√≠stico claro.

**"The Sounds of the World's Languages" Peter Ladefoged e Ian Maddieson (2018)**

Eleg√≠ este texto como referencia en el estudio de la fon√©tica y la fonolog√≠a [(Ladefoged & Maddieson, 1996)](https://www.zotero.org/google-docs/?d042SC). En principio me result√≥ interesante entender la cuesti√≥n t√©cnica del lenguaje para tener herramientas a la hora de trabajar con el mismo. Pasar√© a explicar brevemente estas dos materias de estudio.

El estudio de la fon√©tica y la fonolog√≠a no solo ayuda a entender c√≥mo los sonidos se producen en el habla humana, sino tambi√©n c√≥mo los idiomas se diferencian entre s√≠. Cada idioma tiene su propio conjunto de sonidos distintivos, y la fon√©tica y la fonolog√≠a nos permiten identificar y comparar estos sonidos.

La fon√©tica se enfoca en la producci√≥n de sonidos individuales, lo que significa que podemos identificar y comparar los sonidos de diferentes idiomas. Por ejemplo, la pronunciaci√≥n de la "r" en espa√±ol es diferente a la pronunciaci√≥n de la "r" en ingl√©s. Adem√°s, algunos idiomas tienen sonidos que no existen en otros idiomas. Por ejemplo, el idioma Xhosa hablado en Sud√°frica tiene tres clics distintos que no existen en otros idiomas.

Por otro lado, la fonolog√≠a se enfoca en c√≥mo los sonidos se combinan para formar palabras y oraciones, lo que significa que podemos identificar patrones distintivos en diferentes idiomas. Por ejemplo, en ingl√©s, la palabra "cat" tiene una estructura sil√°bica CV (consonante-vocal), mientras que en espa√±ol, la palabra "gato" tiene una estructura sil√°bica VCV (vocal-consonante-vocal). La fonolog√≠a tambi√©n puede ayudar a identificar caracter√≠sticas distintivas en diferentes idiomas, como la distinci√≥n entre consonantes sonoras y sordas en alem√°n.

En general, el estudio de la fon√©tica y la fonolog√≠a es crucial para distinguir y comparar diferentes idiomas. Al identificar los sonidos distintivos y los patrones de sonidos en diferentes idiomas, podemos entender mejor c√≥mo se diferencian y c√≥mo se relacionan entre s√≠. Esta comprensi√≥n es esencial para la comunicaci√≥n efectiva entre personas que hablan diferentes idiomas, as√≠ como para la investigaci√≥n y el aprendizaje de idiomas adicionales.

**Gram√°tica Generativa de Noam Chomsky**

La Gram√°tica Generativa, propuesta por Noam Chomsky en la d√©cada de 1950 [(Chomsky, 2002)](https://www.zotero.org/google-docs/?enHCw6), es un enfoque te√≥rico dentro de la ling√º√≠stica que busca describir y explicar la estructura de los lenguajes humanos. Se basa en la idea de que la capacidad para adquirir y producir lenguaje es innata en los seres humanos y se rige por reglas gramaticales universales.

En el marco de la Gram√°tica Generativa, Chomsky introdujo el concepto de una "gram√°tica generativa-transformacional", que consta de varios componentes. El componente fonol√≥gico se ocupa de los sonidos del lenguaje, el componente l√©xico aborda el significado de las palabras, y el componente sint√°ctico se centra en la estructura y la formaci√≥n de las oraciones.

La Gram√°tica Generativa se basa en el principio de que existen reglas subyacentes que gobiernan la estructura y la formaci√≥n de las oraciones en todos los idiomas. Estas reglas se representan a trav√©s de una notaci√≥n formal llamada "gram√°tica generativa-transformacional", que describe las estructuras y las transformaciones que ocurren en el proceso de generaci√≥n del lenguaje.

Una de las caracter√≠sticas fundamentales de la Gram√°tica Generativa es la noci√≥n de "competencia ling√º√≠stica". Seg√∫n Chomsky, cada hablante nativo posee una competencia innata para su idioma materno, que se refiere al conocimiento inconsciente de las reglas gramaticales que rigen el lenguaje. Por otro lado, la "actuaci√≥n ling√º√≠stica" se refiere al uso real y observable del lenguaje en la comunicaci√≥n diaria.

Chomsky tambi√©n introdujo la noci√≥n de "gram√°tica transformacional-generativa" como una forma de describir y explicar las transformaciones que ocurren en la generaci√≥n y comprensi√≥n de las oraciones. Estas transformaciones permiten a los hablantes modificar las estructuras de las oraciones para expresar diferentes significados o relaciones gramaticales.

### Arte Sonoro

Como referencias propias del arte sonoro, me gustar√≠a comenzar mencionando la estrategia principal de este trabajo: la desterritorializaci√≥n del sonido. Este recurso es un concepto que ha ganado relevancia en el √°mbito del arte sonoro. Se refiere al proceso mediante el cual el sonido se separa de su contexto habitual y permite que adquieran nuevas significaciones, liber√°ndose de las limitaciones geogr√°ficas y convenciones tradicionales que lo sit√∫an y los vuelven reconocibles en categor√≠as, como el lenguaje o la m√∫sica.

En este sentido, el Arte Sonoro se convierte en un espacio de experimentaci√≥n y exploraci√≥n, donde se puede trabajar con cualquier tipo de sonido, ya sea generado por instrumentos convencionales, objetos cotidianos, grabaciones de campo, sonidos electr√≥nicos, generados por IA, o el habla. La desterritorializaci√≥n del sonido desaf√≠a las categor√≠as establecidas y permite la creaci√≥n de paisajes sonoros resignificados.

Al desterrar el sonido de su contexto original, el Arte Sonoro invita a la audiencia a cuestionar y reflexionar sobre su relaci√≥n con el sonido y su entorno. Se rompen las barreras entre lo natural y lo artificial, generando nuevos di√°logos y perspectivas sobre el papel del sonido en nuestras vidas. Tambi√©n representa una liberaci√≥n creativa y conceptual del sonido, permitiendo su recontextualizaci√≥n y reinterpretaci√≥n. A trav√©s de este proceso, se exploran nuevas formas de expresi√≥n y nos invitan a cuestionar nuestras percepciones y experiencias auditivas.

En la filosof√≠a de Gilles Deleuze y F√©lix Guattari, la desterritorializaci√≥n se refiere a un proceso en el que algo rompe o trasciende sus l√≠mites o restricciones habituales. En el contexto del sonido, la desterritorializaci√≥n implica la liberaci√≥n y desvinculaci√≥n de los sonidos de su contexto original, permitiendo nuevas posibilidades y significados.

En su obra "Mil mesetas: Capitalismo y esquizofrenia" [(Deleuze et al., 2020)](https://www.zotero.org/google-docs/?6pSoJ3), Deleuze y Guattari discuten la idea de la desterritorializaci√≥n en relaci√≥n con la m√∫sica y el sonido. Argumentan que la m√∫sica, a trav√©s de la desterritorializaci√≥n del sonido, puede abrir nuevos espacios y modos de expresi√≥n. Seg√∫n ellos, la m√∫sica puede "crear un nuevo agenciamiento del sonido, un desterritorializante que desaloje al sonido de su territorio habitual" (Deleuze y Guattari, 1980).

En el campo del arte sonoro, la desterritorializaci√≥n del sonido implica la exploraci√≥n de nuevas formas de producci√≥n y percepci√≥n del sonido, y la creaci√≥n de paisajes sonoros que desaf√≠an las convenciones tradicionales de la m√∫sica. Los artistas sonoros utilizan t√©cnicas y tecnolog√≠as para liberar el sonido de sus confines originales, permitiendo su transformaci√≥n y recontextualizaci√≥n.

**Archivo P.A.I.S.** de Nicol√°s Varchausky es un archivo de artista que explora las tensiones entre sonido y sentido en la voz hablada, sus perspectivas diversas de enunciaci√≥n y contextos de sentido. [(Varchausky, n.d.)](https://www.zotero.org/google-docs/?8iUfMQ). Est√° organizado en 4 colecciones: La Voz del Mercado, La Voz de las Instituciones, La Voz De Dios y La Voz del Arte. Similar a un dataset de grabaciones urbanas, pero diferenci√°ndose en su estructura y presentaci√≥n, este trabajo me resulta af√≠n dado que trata sobre la voz en su modo m√°s puro y primitivo, de su *musicalidad* dadas sus caracter√≠sticas de entonaci√≥n y enunciaci√≥n, variaci√≥n t√≠mbrica, tono. Su b√∫squeda yace en la intermodulaci√≥n entre sentido y sonido [(*Archivo P.A.I.S.*, n.d.)](https://www.zotero.org/google-docs/?WLYJ0t).

El trabajo de Nicol√°s despierta algunas preguntas: ¬øSe podr√° separar del todo el contenido sem√°ntico al generar nuevas voces sint√©ticas? ¬øQu√© pasa con la entonaci√≥n-intenci√≥n? ¬øSer√° inevitable comprender algo aunque no hayan palabras reales? ¬øComprender√°n lo mismo distintas personas a partir de una misma frase ficticia, supuestamente despojada de sentido?

**√âramos la humanidad** es un proyecto audiovisual de Mateo Amaral que utiliza distintos sistemas de inteligencia artificial y motores de videojuegos para imaginar un futuro posthumano. En este futuro, los Entes -seres h√≠bridos entre inteligencia artificial y vida org√°nica- habitan un extra√±o ciberespacio entre estructuras incompresibles y restos de la cultura humana.

La instalaci√≥n consiste en la superposici√≥n de dos ecosistemas, uno virtual y otro vegetal. Mientras que los escenarios y personajes digitales aparecen proyectados en las paredes, en el suelo hay mont√≠culos de tierra donde crecen plantas y del techo cuelgan ra√≠ces, lianas y l√≠quenes. En la intersecci√≥n, la luz de la proyecci√≥n se encuentra con los cuerpos vegetales produciendo dibujos flotantes. El ecosistema virtual es generado en tiempo real por un motor de¬† videojuegos que se juega a s√≠ mismo o una obra de teatro improvisada por criaturas fantasmales. El programa sintetiza entornos a partir de la combinaci√≥n de distintos elementos y luego puebla esos entornos con personajes dotados de inteligencia artificial.

Lo particular que me interes√≥ de la obra de Mateo es que sus entes-personajes hablan un idioma desconocido, pero a la vez familiar. Este es el punto al que deseo llegar. En su caso fue gracias a entrenar con pocas *√©pocas* sus modelos de generaci√≥n de voz, de manera que nunca llegaron a *aprender el idioma* del todo, sino que quedaron truncos, eternamente balbuceando en su universo cibern√©tico. El problema aqu√≠ es que no se puede *tunear* el idioma a asimilarse, sino que tom√≥ muestras de audio aleatorias para su modelo, de manera que es virtualmente imposible rastrear su origen.

Por √∫ltimo, me gustar√≠a hacer menci√≥n a una obra fundacional en el arte sonoro, que hasta he recreado en 2016 durante mi cursada de la materia Psicoac√∫stica dictada por Eleonora Rap√°n. Se trata de **"I am sitting in a room"** del compositor experimental Alvin Lucier (1969). En esta pieza, Lucier explora la relaci√≥n entre el sonido, el espacio y la tecnolog√≠a de grabaci√≥n. La relaci√≥n con mi trabajo es que esta pieza realiza la operaci√≥n opuesta a la planteada en mi experimento sonoro.

La obra comienza con una grabaci√≥n de la voz de Lucier diciendo: "I am sitting in a room different from the one you are in now. I am recording the sound of my speaking voice...". Esta grabaci√≥n inicial se reproduce y se vuelve a grabar en la habitaci√≥n una y otra vez. Con cada repetici√≥n, la resonancia y las caracter√≠sticas ac√∫sticas de la habitaci√≥n se realzan y amplifican, mientras que la voz original se va desvaneciendo y transformando en una textura sonora m√°s abstracta. A medida que la pieza avanza, los aspectos m√°s reconocibles de la voz se vuelven borrosos y se disuelven en el sonido del espacio mismo. Al contrario al objetivo sonoro de mi software, que ser√° partir de audio totalmente entendible a otro que no lo es, pero hace referencia a aquel que fue, y adem√°s en este caso eliminamos la componente del espacio ac√∫stico.

# El lenguaje y el sentido

## Linguistica

Los sonidos de las lenguas son el resultado de la combinaci√≥n de diferentes rasgos fon√©ticos, como el modo, el punto y la fuerza de articulaci√≥n, la duraci√≥n, la amplitud y la frecuencia, la coarticulaci√≥n, la asimilaci√≥n, la armon√≠a voc√°lica y la suprasegmentalidad. 

Cada lengua tiene un sistema sonoro propio, que se caracteriza por tener ciertos sonidos y no otros, y por organizarlos de ciertas formas y no otras. Por lo tanto, podemos identificar cada idioma sin saber hablarlo a partir de escuchar sus sonidos si prestamos atenci√≥n a algunas caracter√≠sticas que hacen que reconozcamos un idioma de otro. 

La¬†**fon√©tica**¬†y la¬†**fonolog√≠a**¬†son dos disciplinas de la ling√º√≠stica que se dedican al estudio de los sonidos de la lengua.¬†La fon√©tica estudia el inventario de los sonidos de una lengua con arreglo a las diferencias articulatorias perceptibles; por ejemplo, distingue entre la ‚Äúb‚Äù de rombo y la ‚Äúb‚Äù de robo¬†[**1**](https://bing.com/search?q=Fon%C3%A9tica+y+Fonolog%C3%ADa). La fon√©tica trata del an√°lisis de los sonidos del habla y c√≥mo se producen esos sonidos.¬†[Un Sonido es el fen√≥meno producido al vibrar las cuerdas vocales del aparato fonador¬†**2**](https://www.gramaticas.net/2014/07/fonetica-y-fonologia.html).¬†[Los sonidos pueden ser clasificados seg√∫n su punto de articulaci√≥n (bilabiales, labiodentales, interdentales, etc.), modo de articulaci√≥n (oclusivo, fricativo, africado, etc.), actuaci√≥n de las cuerdas voc√°licas (sordo o sonoro) y camino del aire (nasal u oral)¬†**2**](https://www.gramaticas.net/2014/07/fonetica-y-fonologia.html).

[La fonolog√≠a, en cambio, estudia los elementos f√≥nicos, o unidades, que componen una lengua desde el punto de vista de su funci√≥n en el sistema de comunicaci√≥n ling√º√≠stico](https://www.diccionariodedudas.com/fonetica-y-fonologia/)¬†[**1](https://bing.com/search?q=Fon%C3%A9tica+y+Fonolog%C3%ADa)[3](https://www.diccionariodedudas.com/fonetica-y-fonologia/)**. La fonolog√≠a trata de la organizaci√≥n de los sonidos del habla para formar significados.¬†[Un fonema es la abstracci√≥n (imagen mental) de los sonidos del habla humana¬†**2**](https://www.gramaticas.net/2014/07/fonetica-y-fonologia.html).¬†[Los fonemas pueden ser voc√°licos o conson√°nticos y se clasifican seg√∫n su punto y modo de articulaci√≥n¬†**2**](https://www.gramaticas.net/2014/07/fonetica-y-fonologia.html).

Un ejemplo en espa√±ol ser√≠a el sonido de la letra ‚Äúc‚Äù que puede tener un sonido suave como en ‚Äúcereza‚Äù o un sonido fuerte como en ‚Äúcasa‚Äù. En ingl√©s, un ejemplo ser√≠a el sonido de la letra ‚Äút‚Äù que puede tener un sonido suave como en ‚Äúwater‚Äù o un sonido fuerte como en ‚Äútop‚Äù.

Los humanos tienen la capacidad innata de reconocer diferentes idiomas debido a la capacidad del cerebro para procesar y distinguir entre diferentes patrones de sonidos.

<aside>
üí° Para recopilar esta informaci√≥n, adem√°s de Google, Bing Chat y ChatGPT, utilic√© un libro llamado "The Sounds of the World's Languages" por Peter Ladefoged y Ian Maddieson (2018), pero me di cuenta que esta era una tarea te√≥rica muy ambiciosa para el alcance de este proyecto, que pretende comprender caracter√≠sticas del lenguaje en base a la experiencia de escucha. Sin embargo, comprender estos conceptos b√°sicos colabor√≥ luego a poder trabajar con el lenguaje como material concreto, como concepto modulador de los sonidos a generar.

</aside>

## Torre de Babel

![**Pieter Bruegel el Viejo - La Torre de Babel (Rotterdam)**](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/Untitled.png)

**Pieter Bruegel el Viejo - La Torre de Babel (Rotterdam)**

// COMPLETAR 

## Sinsentido

L√≥gica del Sentido

sound of nonsense, 

alicia en el pais de las maravillas

A lo largo de este trabajo tomo el sinsentido como un recurso est√©tico que utilizo en distintos niveles de mi investigaci√≥n. Me refiero al sentido propio de esta investigaci√≥n, de mi carrera, del constante accionar en cada paso de cada experimento. Entrando a la materialidad: el sentido sonoro por s√≠ mismo, el nuevo sentido que genera el sinsentido, un relato que no tiene sentido pero puede ser interpretado y genera im√°genes que tienen sentido en un mundo cuyas reglas del sentido son otras.

El prefijo negativo de *sin-sentido* propone una falta, pero como se plantea en "The Sound of Nonsense‚Äù tambi√©n es *meta-sentido* y es que esto refiere a una nueva estrategia, una decisi√≥n arbitraria, diferente [(Elliott & Bull, 2017)](https://www.notion.so/Un-recorrido-tecno-sonoro-hacia-el-habla-as-mica-31eaddcc2bb841bcaa124ed890ed8ca7?pvs=21). No es lo opuesto al sentido, es otra cosa. Esto va en l√≠nea con el trabajo de escritores del sinsentido, como Lewis Carrol, conocido por su famosa obra ‚ÄúAlicia en el pa√≠s de las maravillas‚Äù. Me atrevo a mencionar al movimiento del *Realismo M√°gico*, que conoc√≠ gracias a Gabriel Garc√≠a M√°rquez, en donde las reglas del juego parecen cambiar por completo a nuestro universo pero all√≠ son parte de la cotidianeidad. Me gustar√≠a decir que es nuestra versi√≥n latinoamericana del sinsentido.

Richard Elliot no logra una definici√≥n concreta del sinsentido por lo que establece cinco pautas de a qu√© se refiere con sinsentido en su texto. Me quedo con dos de ellas, y el entramado entre todas:

> Destaco las que me interesan:
> 
> - **Aquello que introduce una l√≥gica alterada incluso cuando se utiliza un lenguaje *norma*. Ser√≠a el reino de [Lewis] Carroll y sus seguidores o de la comedia Goonesca.**
> - Lo que se mantiene dentro de un r√©gimen sint√°ctico ¬´normal¬ª pero introduce fallos y otras estrategias disruptivas y juega con la l√≥gica de la sem√°ntica. Los escritores y m√∫sicos modernistas y posmodernistas estar√≠an entre los ejemplos de este tipo.
> - **Lo que surge de alterar la sintaxis para crear magia, confusi√≥n o verdad. Incluir√≠a recortes, permutaciones y otras estrategias que juegan con la l√≥gica de la sintaxis.**
> - Lo que roza o se solapa con lo absurdo. Esto incluir√≠a el drama Beckettiano y la canci√≥n Dylanesca.
> - Lo que utiliza c√≥digos o t√©rminos que s√≥lo entienden los especialistas o los iniciados. Para los no especialistas o ajenos al tema, s√≥lo se trata de ¬´tonter√≠as¬ª, lo que com√∫nmente se conoce como jerga. Esto incluye el lenguaje de moda, como el jazz jive, la jerga del hip-hop, los t√©rminos subculturales, ciertos discursos acad√©micos y el Shipping Forecast de la BBC.
> 
> Entre ellos se encuentra el momento del sinsentido, ese espacio de desconcierto entre dos o m√°s modos de significado en el que la b√∫squeda de sentido se convierte en un intercambio de c√≥digos.
> 

Quiz√°s se vea borroso el borde entre el sinsentido y la fantas√≠a, pero la fantas√≠a adhiere a un sistema de reglas y l√≥gicas que se respetan a lo largo de su desarrollo. El sinsentido, por otro lado, desaf√≠a constantemente cualquier atisbo de regla que se establezca. Est√° m√°s cerca de un mecanismo que permite el lenguaje: poder decir cualquier cosa, armar estructuras de palabras que no suelen ir juntas, casi como si la materialidad de un juego de encastrar piezas sean palabras, o incluso mejor: letras. 

<aside>
ü§î Es aleifuv una emuuef forma dkljelkeaji orden ojfodj palabras qe mve letras unoe j d ap.

</aside>

Y no pasa nada. Como si fuera simplemente *otra* *forma* de armar una oraci√≥n, s√≥lo porque se puede.

imagen AI con los keyframes hechos en chatgpt

surrealismo 

caosmosis: tipo de orden que surge de la caotizaci√≥n de las cosas.

del caos pueden emerger un conjunto de relaciones que reordenan el orden

lenguaje: tiene una relaci√≥n con el sentido. puede decir lo que pasa. crea sentido, por qu√© lo crea? el sentido es lo que verdaderamente pasa.

# Desarrollo T√©cnico

## Camino y Objetivos

Como mencion√© anteriormente, este trabajo surge de la idea de generar un software de habla que *no diga nada.* Para eso necesitaba realizar un desarrollo que logre en **tiempo real** generar sonidos, similar a un instrumento, pero de habla. El puntapi√© para el comienzo del desarrollo fue la b√∫squeda de modelos de machine learning que puedan generalizar la voz y devolvernos sonidos, similar a lo que hace un sistema TTS, con la diferencia de que yo no quer√≠a darle contenido sem√°ntico, y tampoco deb√≠a *hablar correctamente,* es decir, hacer sentido en su dicci√≥n. Solamente buscaba *que suene como si hablara*, pero que no haya palabras reconocibles por dilucidar en lo que se dice.

## Colaboraci√≥n con LLMs

Hoy en d√≠a contamos con una herramienta que fue fundamental para el avance de la investigaci√≥n, tanto para realizar preguntas generales gu√≠a, b√∫squeda de referencias, y sobretodo especificidades dentro del desarrollo m√°s duro. Se trata de los modelos grandes de lenguaje, o por sus siglas en ingl√©s, LLMs. Me parece de suma relevancia mencionar esta herramienta dado que este es un desarrollo principalmente pr√°ctico cuyo concepto subyacente principal es el lenguaje.

Los modelos de lenguaje basados en transformadores, como los LLMs (Large Language Models), son parte de los **modelos fundacionales** en inteligencia artificial. Estos modelos revolucionaron el procesamiento del lenguaje natural al introducir arquitecturas de transformadores, como el modelo GPT (Generative Pre-trained Transformer). Los LLMs son pre-entrenados en grandes cantidades de datos textuales, lo que les permite una comprensi√≥n profunda del lenguaje. Posteriormente, se pueden ajustar para tareas espec√≠ficas, como traducci√≥n, resumen de texto o generaci√≥n de texto y respuesta a preguntas. Su capacidad para entender y generar lenguaje humano ha impulsado avances significativos en aplicaciones de procesamiento de lenguaje natural, y su adaptabilidad los convierte en componentes clave en una amplia gama de aplicaciones y servicios basados en texto (Vaswani et al., 2023).

Principalmente utilic√© la versi√≥n gratuita de [ChatGPT](https://chat.openai.com/), desarrollado por [OpenAI](https://openai.com/), que en este momento es el modelo `gpt-3.5-turbo`. Est√° entrenado con informaci√≥n recopilada de todo internet hasta septiembre de 2021. Comprend√≠ que si se le da contexto y se le define un rol responde de manera muy precisa. Sobretodo en tareas peque√±as y espec√≠ficas, pero tambi√©n como puntapi√© inicial hacia las sub-investigaciones que fui llevando a cabo a lo largo del proceso.

Dada la limitaci√≥n del modelo gratuito de OpenAI, tambi√©n recurr√≠ a otros LLMs como [Bing Chat](https://www.bing.com/), de [Microsoft](https://www.microsoft.com/), que cuenta con la habilidad de buscar en internet, de hecho, es un buscador similar a [Google](https://www.google.com/), pero con *quien* uno puede establecer una conversaci√≥n donde afinar sus resultados. El gigante de internet Google tambi√©n cuenta con [Bard](https://bard.google.com/chat), pero de momento se encuentra en una etapa experimental, muy revisada por humanos y con poco soporte en idiomas que no sean ingl√©s. Por √∫ltimo prob√© [LLaVA](https://llava.hliu.cc/): Large Language and Vision Assistant, que si bien promete y es de c√≥digo abierto, no cuenta con conversaciones que pueda almacenar en hilos de contexto como s√≠ lo hace ChatGPT, teniendo que iniciar nuevas conversaciones en cada sesi√≥n de uso.

## Autoencoder

Leandro Garber me recomend√≥ que un buen experimento inicial es entrenar un modelo con la arquitectura **Autoencoder**, en particular uno desarrollado por Pablo Riera **// ADD REFE paper? //.**  Al ser alimentado con audio de idiomas hablados, tuvimos la hip√≥tesis de que este podr√≠a generalizar lo suficiente sus caracter√≠sticas como para luego generar nuevos resultados sonoros sin necesidad de contenido sem√°ntico, qued√°ndose s√≥lo con la prosodia de la lengua.

### Introducci√≥n al modelo

Un Autoencoder es una arquitectura de redes neuronales artificiales utilizado en tareas de procesamiento de datos, como la reducci√≥n de dimensionalidad y la reconstrucci√≥n de informaci√≥n. Se puede pensar como una especie de funci√≥n que toma datos de entrada, los reduce a una forma m√°s compacta en un espacio de representaci√≥n interno y luego intenta reconstruir los datos originales a partir de esta representaci√≥n interna. La idea principal detr√°s de estos es aprender una representaci√≥n eficiente de los datos de entrada.

Consiste en las siguientes partes:

![Untitled](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/Untitled%201.png)

1. **Capa de Codificaci√≥n (Encoder)**: En la primera parte del modelo, llamada el codificador (encoder), los datos de entrada se transforman en una representaci√≥n interna m√°s compacta. Esta capa reduce la dimensionalidad de los datos, capturando las caracter√≠sticas m√°s importantes.
2. **Espacio Latente (Latent Space)**: Es la parte del medio, la representaci√≥n interna o intermedia de los datos que se encuentra entre el codificador y el decodificador. En este espacio latente, las caracter√≠sticas m√°s importantes de los datos de entrada se capturan de una manera m√°s compacta y abstra√≠da. Es decir, en lugar de describir cada detalle de los datos, el espacio latente representa caracter√≠sticas m√°s significativas y abstractas. Un espacio latente bien entrenado permite operaciones que permiten explorar y manipular las caracter√≠sticas subyacentes de los datos de entrada de manera m√°s eficiente.
3. **Capa de Decodificaci√≥n (Decoder)**: El decodificador (decoder) es la representaci√≥n interna que se utiliza para intentar reconstruir los datos originales. El objetivo es que la reconstrucci√≥n sea lo m√°s fiel posible a las generalidades de los datos de entrada.

Para entrenar el Autoencoder, se utiliza una **funci√≥n de p√©rdida** que mide la diferencia entre los datos de entrada y la reconstrucci√≥n. El modelo ajusta sus par√°metros para minimizar esta p√©rdida, lo que significa que aprender√° a crear representaciones internas √∫tiles.

El Autoencoder es √∫til en varias aplicaciones como la eliminaci√≥n de ruido en datos, la generaci√≥n de datos similares a los de entrada y la detecci√≥n de anomal√≠as. Al comprimir la informaci√≥n en una representaci√≥n interna puede ser √∫til para simplificar datos complejos y hacer que sea m√°s f√°cil trabajar con ellos o identificar patrones importantes.

La exploraci√≥n del espacio latente en un modelo de Autoencoder se asemeja a recorrer un intrincado sistema de *caminos* en busca de informaci√≥n relevante. Los datos de entrada son como un gran mapa con numerosos senderos, y el Autoencoder act√∫a como un cart√≥grafo que reduce la complejidad del mapa, resaltando rutas esenciales en un espacio latente. Al seguir estos caminos, se puede navegar de manera eficiente a trav√©s de las caracter√≠sticas m√°s importantes de los datos, como si fueran flujos de informaci√≥n, permitiendo una comprensi√≥n m√°s profunda y la capacidad de generar nuevos datos relacionados a los que entraron.

### Dataset

Entrenar una red neuronal implica ense√±ar al modelo a realizar una tarea espec√≠fica mediante la exposici√≥n a datos de entrada y salidas correspondientes. Durante el proceso de entrenamiento, los pesos y las conexiones entre las neuronas de la red se ajustan iterativamente para minimizar la diferencia entre las predicciones del modelo y las respuestas reales en el conjunto de datos. Este ajuste se realiza utilizando algoritmos de optimizaci√≥n que buscan optimizar la funci√≥n de p√©rdida, que mide la discrepancia entre las predicciones del modelo y los resultados reales.

Un dataset, o conjunto de datos, es una colecci√≥n de ejemplos de entrada y sus correspondientes salidas que se utiliza para entrenar, validar o probar un modelo de machine learning. En el contexto de entrenar una red neuronal sirve como la fuente de informaci√≥n que el modelo utiliza para aprender patrones y realizar predicciones. Este conjunto de datos suele dividirse en conjuntos de entrenamiento, validaci√≥n y prueba. El conjunto de entrenamiento se utiliza para ajustar los par√°metros del modelo, el conjunto de validaci√≥n para afinar hiperpar√°metros ‚Äìvalores internos del modelo‚Äì y evitar el sobreajuste, y finalmente, el conjunto de prueba para evaluar el rendimiento general del modelo en datos no vistos durante el entrenamiento. La calidad y representatividad del dataset son cruciales para el √©xito del entrenamiento y el rendimiento del modelo en situaciones del mundo real. 

El primer Dataset con el que trabaj√© fue [common_language](https://huggingface.co/datasets/common_language). Est√° compuesto por grabaciones de voz de idiomas seleccionados cuidadosamente de la base de datos [Mozilla CommonVoice](https://commonvoice.mozilla.org/). La duraci√≥n total de las grabaciones de audio es de 45.1 horas (1 hora de material para cada idioma). Est√° destinado a entrenar sistemas de identificaci√≥n de idiomas.

A continuaci√≥n algunos ejemplos de los archivos incluidos en el dataset. Son archivos `.wav`, es decir que no tienen p√©rdida de calidad:

[common_voice_fr_19598591.wav](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/common_voice_fr_19598591.wav)

[common_voice_fr_19598240.wav](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/common_voice_fr_19598240.wav)

[Ejemplos de la carpeta de franc√©s.](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/common_voice_fr_19140964.wav)

Ejemplos de la carpeta de franc√©s.

N√≥tese que los fragmentos est√°n dichos por diferentes personas, con distintas calidades y condiciones de grabaci√≥n. Esto se debe a que es necesaria esta variabilidad para enriquecer el modelo, que logre generalizar el habla y la voz humana, pero a ninguna voz en particular.

Para entrenar el Autoencoder deb√≠a alimentarlo con un s√≥lo archivo de audio. El dataset cuenta con una carpeta por idioma con peque√±os fragmentos, por lo que con el siguiente comando en la terminal de la computadora logr√© concatenar inmediatamente todos los archivos de una carpeta en un archivo de audio √∫nico:

```bash
# Los archivos est√°n nombrados de la siguiente forma:
# common_voice_<lang>_<n√∫mero>.wav
# Por ejemplo: common_voice_fr_19140964.wav
$ sox common_* frances.wav
# Obtengo `frances.wav` como resultado.
```

`sox` es una utilidad para manipular audio desde la consola, funciona en macOS, Linux y Windows. De esa forma evit√© el trabajo manual de concatenar todos los archivos.

[Archivo de audio mp3 61.8kbps VBR para fines demostrativos, pero se utiliz√≥ el wav para entrenar el modelo.](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/french.mp3)

Archivo de audio mp3 61.8kbps VBR para fines demostrativos, pero se utiliz√≥ el wav para entrenar el modelo.

Lo mismo pero para el Espa√±ol:

[common_voice_es_18681301.wav](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/common_voice_es_18681301.wav)

[common_voice_es_19125377.wav](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/common_voice_es_19125377.wav)

[common_voice_es_19602100.wav](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/common_voice_es_19602100.wav)

Y su versi√≥n concatenada:

[Archivo convertido a mp3 con ffmpeg a 61.7kbps VBR.](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/spanish.mp3)

Archivo convertido a mp3 con ffmpeg a 61.7kbps VBR.

Al solicitarle [inferencia](https://www.notion.so/Un-recorrido-tecno-sonoro-hacia-el-habla-as-mica-31eaddcc2bb841bcaa124ed890ed8ca7?pvs=21) al modelo para generar nuevos sonidos a partir del entrenamiento con los mostrados anteriormente, los resultados no eran muy favorables para mi investigaci√≥n por su alto nivel de ruido, por lo que decid√≠ buscar videos en YouTube de Podcasts o conversaciones. Encontr√© voces limpias de gente con buena calidad de grabaci√≥n, sent√≠ que con estos el modelo podr√≠a generalizar voces m√°s n√≠tidas.

- Podcast en franc√©s. mp3 55.7kbps VBR para fines demostrativos:
    
    [Audio extra√≠do de [https://www.youtube.com/watch?v=N55d9mPm_HA](https://www.youtube.com/watch?v=N55d9mPm_HA)](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/french-clean.mp3)
    
    Audio extra√≠do de [https://www.youtube.com/watch?v=N55d9mPm_HA](https://www.youtube.com/watch?v=N55d9mPm_HA)
    
- Japon√©s:
    
    [Audio extra√≠do de [https://www.youtube.com/watch?v=trySjnuJfJY](https://www.youtube.com/watch?v=trySjnuJfJY)](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/japanese-clean.mp3)
    
    Audio extra√≠do de [https://www.youtube.com/watch?v=trySjnuJfJY](https://www.youtube.com/watch?v=trySjnuJfJY)
    

### Entrenamiento

Para entrenar el modelo prepar√© un Colab con el c√≥digo a ejecutar y un entorno donde ejecutarlo, que pod√≠a ser la nube (Google) o mi computadora. Luego de realizar unos *benchmarks* -pruebas de rendimiento- observ√© que mi computadora ser√≠a un poco m√°s r√°pida que los recursos que ofrece el *free tier* de Google Colab. As√≠ que instal√© Anaconda para montar un entorno con la versi√≥n requerida de Python y todas las dependencias requeridas para poder ejecutar mi propio Colab, incluso la configuraci√≥n de seguridad y redes para poder acceder remotamente desde cualquier lado, como si tuviera mi propia nube en mi casa, manteniendo la soberan√≠a de mi sistema computacional.

Realic√© el entrenamiento con los archivos de audio concatenados que arm√© en la etapa del dataset. Esto result√≥ en modelos *uni-lengua* ya que s√≥lo podr√≠an generalizar habla en una sola lengua. Por esto fue que el siguiente paso fue realizar un proceso de entrenamiento con dos lenguas que, como explicar√© [m√°s adelante](https://www.notion.so/Un-recorrido-tecno-sonoro-hacia-el-habla-as-mica-31eaddcc2bb841bcaa124ed890ed8ca7?pvs=21), podr√≠a generar una cruza interesante. Concaten√© una conversaci√≥n en Franc√©s con otra en Japon√©s como dataset para luego generar un modelo *multi-lengua* o en este primer caso biling√ºe.

### Espacio Latente

La idea es explorar este espacio para encontrar nuevos [*caminos*](https://www.notion.so/Un-recorrido-tecno-sonoro-hacia-el-habla-as-mica-31eaddcc2bb841bcaa124ed890ed8ca7?pvs=21) que extraigan las caracter√≠sticas principales del sonido con el que fue entrenado el modelo. Con caminos me refiero a datos en serie de tantas dimensiones como tenga el espacio latente ‚Äìn√∫meros‚Äì que representan esta exploraci√≥n, y al ser decodificados generan nueva informaci√≥n con las caracter√≠sticas principales de sus datos de entrada.

En principio, junto con Leandro Garber, pensamos en enviar datos aleatorios de caminos posibles para comenzar a ver qu√© hab√≠a en ese espacio latente. Para eso armamos un peque√±o patch en Pure Data para enviar los valores en tiempo real al modelo y que este genere audio a partir de los datos de entrada.

### Escuchar el modelo en tiempo real

El [**2ASMRSynth**](https://github.com/pabloriera/2ASMRS) tambi√©n programado por Pablo Riera es un sintetizador basado en [JUCE](https://juce.com/) para generar inferencia sonora en tiempo real a partir de un modelo de tipo [`torchscript`](https://pytorch.org/docs/stable/jit.html). En esencia, uno carga el modelo en el sintetizador y puede mover los par√°metros dentro del espacio latente y escucharlo en tiempo real, como un instrumento. La sonoridad de este instrumento depender√° del modelo cargado.

![Captura de pantalla del 2ASMRSynth corriendo en Linux Mint.](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/Untitled%202.png)

Captura de pantalla del 2ASMRSynth corriendo en Linux Mint.

Cada *fader* del instrumento representa una de las dimensiones del espacio latente, y al moverlos le estamos asignando valores a cada una. La parte del decodificador del modelo genera la salida de audio. Mover estos valores manualmente, por ejemplo con el mouse, no resulta interesante ya que se debe orquestar un movimiento coordinado de todos los par√°metros para que la salida haga *sentido*. Por supuesto que cuenta con la posibilidad de ser controlado mediante el protocolo MIDI.

Pablo desarroll√≥ y prob√≥ este instrumento en una distribuci√≥n de Linux basada en Debian. Yo, en cambio, intent√© compilar el software en distintas plataformas. Primero en mi computadora de buenos recursos que corre Windows, me parec√≠a que tambi√©n aportar√≠a al mundo open source si luego pod√≠a compartir mi camino haci√©ndolo andar en una nueva plataforma. All√≠ me encontr√© con problemas propios de arquitecturas y matem√°ticas de bajo nivel que se escapan del alcance de esta investigaci√≥n, por lo que abandon√© ese camino. Continu√© con WSL (una forma de ejecutar Linux adentro de Windows), pero las capacidades de routeo interno MIDI que necesito no est√°n desarrolladas al momento de este trabajo, por lo que tambi√©n tuve que dejar este camino. Luego prob√© Raspbian (Debian) en Raspberry Pi, pensando que ser√≠a √∫til esta plataforma para montajes compactos, simples y econ√≥micos en cuanto a recursos, pero la arquitectura ARM de los procesadores de RPi no est√°n muy bien soportados en JUCE en este momento, y el procesador y la memoria RAM no son √≥ptimos para este trabajo; otro camino trunco.

Finalmente instal√© una m√°quina virtual en mi computadora con Windows, prob√© varios softwares de virtualizaci√≥n, primero VirtualBox por ser open source, pero que no utiliza los recursos de la m√°quina de forma √≥ptima y por ende corr√≠a lento. Luego VMWare, que corre mejor, a la que le instal√© Linux Mint, y todo funcion√≥ de maravilla. El archivo de audio y video de esta investigaci√≥n est√° grabada con ese entorno virtual.

Con el modelo [multi-lengua](https://www.notion.so/Un-recorrido-tecno-sonoro-hacia-el-habla-as-mica-31eaddcc2bb841bcaa124ed890ed8ca7?pvs=21) la idea fue alimentar las dimensiones del espacio latente con los datos de entrenamiento de un modelo entrenado en Franc√©s anteriormente. La pregunta era ¬øqu√© pasar√≠a si a un modelo biling√ºe le hago *hablar* uno de los idiomas que conoce?

[JF-MODEL_japanese-fantasmagorico.mp3](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/JF-MODEL_japanese-fantasmagorico.mp3)

JF-MODEL_japanese-fantasmagorico.mp3

Este audio es el resultado de alimentar el modelo biling√ºe con los datos de entrenamiento que representan los movimientos dentro del espacio latente para segmento de habla en Franc√©s.

- el problema de la fase

- Intentos de explorar otros modelos a trav√©s de Max/MSP

El sintetizador de Pablo Riera es una excelente idea y herramienta para lograr inferencia en tiempo real de un modelo sonoro, pero no estaba alcanzando los resultados que quer√≠a, por lo que investigu√© alternativas en otros entornos donde tuviera m√°s control de las partes involucradas, sin tiempos de compilaci√≥n ni plataformas espec√≠ficas, procesamiento y c√≥digo en tiempo real: entra Miller Puckette. Busqu√© *cajitas* desarrolladas en Max/MSP o PureData.

### Observaciones

Primeras pruebas

Dado este audio:

[Audio original alimentado a la red.](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/common_voice_es_19609042.wav)

Audio original alimentado a la red.

Se reconstruy√≥ el siguiente audio directamente desde el c√≥digo. Para eso se insertan en el espacio latente del modelo los mismos valores que devuelve el entrenamiento. Resultando en:

[Audio reconstruido.](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/reconstructed.mp3)

Audio reconstruido.

![Espectrograma del resultado reconstruido.](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/predicted_spectrogram.png)

Espectrograma del resultado reconstruido.

![Camino del espacio latente de 4 dimensiones que genera el audio reconstruido.](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/Z_latent_score.png)

Camino del espacio latente de 4 dimensiones que genera el audio reconstruido.

Las siguientes pruebas consisten en cargar los modelos entrenados en el sintetizador *2ASMRSynth* y luego alimentar los par√°metros del modelo (n√≥tese el err√°tico movimiento de los faders) con una lista de cada valor en el tiempo. En el siguiente ejemplo son 8 listas, una para cada dimensi√≥n del espacio latente representada en los faders.

- Completar.

[French 8 dim](https://drive.google.com/file/d/1xKMgiRHJ8iejAhcU1RUt8BafiqYWJBiq/view?usp=drive_link)

French 8 dim

Para lograr esto se desarroll√≥ un patch de Pure Data con el que se cargan los valores num√©ricos de un archivo de texto (generado durante la etapa de entrenamiento) y se *mapean* o traducen a controles MIDI (rango de 0-127).

[**French4dim_drunkYlatent**](https://drive.google.com/file/d/1s8pfhotzjgofhLQ3mdZndm4c3Y6cGPVc/view?usp=sharing)

**French4dim_drunkYlatent**

- explicar como al alimentarlo con los datos extraidos del entrenamiento, s√≠ suena como lo que se le dio de entrada.

Esta herramienta que posibilita la ejecuci√≥n en tiempo real del modelo trae un problema inherente de su m√©todo de control: el protocolo MIDI est√° limitado a valores de 0-127. Esta resoluci√≥n no es lo suficientemente precisa para *expresar* la voz en cada una de sus variables internas del espacio latente. Como resultado sonoro se genera un sonido fantasmag√≥rico poco definido.

# Crisis

Luego de haber aprendido lo suficiente acerca del workflow y las tecnicalidades de la tecnolog√≠a de inteligencia artificial con la que encar√© el proyecto (el Autoencoder de Riera), llegu√© a resultados que no eran exactamente los que esperaba. Por un momento me detuve a contemplar mi fracaso. Empec√© a dudar de si lo que hab√≠a pensado ‚Äìcrear un sinsentido a partir de un modelo l√≥gico‚Äì ten√≠a sentido, pero mi instinto me obligaba a probarlo para poder llegar a esta conclusi√≥n. 

Inmediatamente, as√≠ como fabul√© este desarrollo y sus resultados, comenc√© a hilar un plan B m√°s elemental, uno que quiz√°s tendr√≠a que haber sido la primera prueba, pero mi predicci√≥n acerca del resultado sonoro era que no iba a ser muy preciso, o escalable, o lo que en mi imaginario buscaba. Esta alternativa consiste en separar audios de habla en fonemas. Un **fonema** es la unidad b√°sica de sonido en un idioma o sistema de comunicaci√≥n. Representa un sonido distintivo que puede cambiar el significado de una palabra en ese idioma. Los fonemas son elementos abstractos que forman parte del sistema fonol√≥gico de una lengua y se utilizan para distinguir palabras y su significado. Pueden variar de un idioma a otro y a menudo se representan con s√≠mbolos fon√©ticos en la ling√º√≠stica para describir c√≥mo se pronuncian en el habla.

// PARRAFO CON LA NUEVA IDEA EXPLICANDO AUDIOSTELLAR

Ser√≠a interesante poder 

Estos recortes vocales, audios segmentados en fonemas, o peque√±as porciones, ser√≠an importados en AudioStellar, lo cual brinda dos resultados/piezas que me interesa observar. Por un lado, un mapa de sonidos que quiz√°s est√©n re-ordenados por sus cualidades sonoras y no por grupos de idiomas, trazando una cartograf√≠a diferente en el espacio sonoro que en el geogr√°fico. Adem√°s y principalmente, la capacidad de recorrer estos fonemas de las diversas formas que ofrece AudioStellar, haciendo que estos suenen de manera concatenada, por lo que formar√≠an habla. 

La distribuci√≥n de los fonemas en el mapa 2D de AudioStellar es lo que define esta experiencia. Hete aqu√≠ el *trade-off*: una cartograf√≠a diferente a la geopol√≠tica de las lenguas versus la interpolaci√≥n de idiomas al recorrer los bordes entre grupos de fonemas. Paso a explicar:

- Si los fonemas se agrupan geopol√≠ticamente, ie. por idioma, ser√° posible recorrer un idioma espec√≠fico a trav√©s de sus fonemas, sin hacer sentido, pero utilizando contenido -sonidos- propios de un s√≥lo idioma. Esto nos da la ventaja de tambi√©n poder escuchar qu√© hay en los bordes de cada grupo de fonemas e interpolar entre idiomas cercanos.
- Si se agrupan por sus caracter√≠sticas sonoras, se podr√°n contemplar fonemas similares de los distintos idiomas, se barren las barreras geopol√≠ticas, trazando un mapa donde el sonido hace nuestro sentido (ref sound of nonsense), es nuestra fuente de verdad y no vamos a interpolar entre idiomas sino entre sonidos de diferentes or√≠genes, agrupados por sus cualidades similares. As√≠ encontramos una nueva [*Cartograf√≠a Mundial del Habla*](https://www.notion.so/Un-recorrido-tecno-sonoro-hacia-el-habla-as-mica-31eaddcc2bb841bcaa124ed890ed8ca7?pvs=21).

## Separaci√≥n en fonemas

La tarea de separaci√≥n en fonemas de los archivos de audio con los que contaba podr√≠a hacerse de varias maneras. En primer lugar, manualmente. Esto tardar√≠a demasiado tiempo y esfuerzo, adem√°s de que quien realice la tarea deber√≠a conocer muy en detalle el idioma y su construcci√≥n fon√©tica. Siguiendo el modus operandi habitual actual, la primera intuici√≥n fue consultarle a ChatGPT acerca del estado de la cuesti√≥n frente a este objetivo. Existen m√∫ltiples herramientas de software dedicado en materia ling√º√≠stica. Entre los que fueron sugeridos se encuentran:

1. **Herramientas de Fon√©tica**:
    - **Praat** es un software de fon√©tica ampliamente utilizado que permite realizar diversos an√°lisis fon√©ticos. Es √∫til para anotar los l√≠mites de los fonemas y extraer informaci√≥n detallada sobre los mismos.
    - **CMU Sphinx (Pocketsphinx)** es un sistema de reconocimiento de voz de c√≥digo abierto muy utilizado que incluye herramientas de fon√©tica. Permite convertir audio en secuencias de fonemas y cuenta con envoltorios de Python disponibles.
    - **VOSK** es una relativamente nueva herramienta de ASR (Automatic Speech Recognition) conocida por su velocidad y eficiencia. Utiliza modelos de deep learning y est√° dise√±ada para aplicaciones modernas de ASR. Una ventaja de Vosk es que sus modelos tienen un tama√±o en general peque√±o, lo que la hace adecuada para aplicaciones con espacio de almacenamiento limitado. Adem√°s, Vosk admite m√∫ltiples idiomas y ofrece modelos pre-entrenados para varios de ellos. Es muy valorada por su facilidad de uso y proporciona integraciones con Python, lo que la hace accesible para desarrolladores.
    - **eSpeak** es un sintetizador de voz compacto y de c√≥digo abierto que se puede utilizar para la fonetizaci√≥n del habla. Proporciona anotaciones fon√©ticas.
    - **Phonetisaurus** es un fonetizador basado en transductores finitos (FST) que puede generar secuencias de fonemas a partir de texto.
2. **Sistemas de Reconocimiento Autom√°tico del Habla (ASR)**:
    
    Automatic Speech Recognition es una tecnolog√≠a que permite a las computadoras implementar sistemas de procesamiento del lenguaje para convertir el habla humana en texto escrito de manera autom√°tica. El proceso implica el an√°lisis de las se√±ales de audio para identificar y transcribir las palabras y frases habladas en un formato legible por m√°quinas. Para lograr esto, utilizan algoritmos y modelos de lenguaje entrenados previamente con grandes conjuntos de datos de audio y texto transcritos emparejados.
    
    Los sistemas ASR como Kaldi, Mozilla DeepSpeech y el ASR de Google se pueden utilizar para transcribir audio en fonemas. Estos sistemas son m√°s complejos pero ofrecen transcripciones detalladas a nivel de fonema.
    
3. **Software de Anotaci√≥n de Fonemas**:
    
    Existe software como Audacity, una herramienta gratuita y de c√≥digo abierto para la edici√≥n de audio, que se puede utilizar para anotar manualmente los tiempos de los l√≠mites de los fonemas. Este proceso puede ser laborioso, pero brinda un control preciso sobre las anotaciones. Otro encontrado fue Sonic Visualizer, de similares caracter√≠sticas.
    
4. **Servicios de Transcripci√≥n Fon√©tica**:
    
    Si no es factible realizar la anotaci√≥n manual, se puede considerar el uso de servicios de transcripci√≥n o contratar ling√ºistas que puedan transcribir los fonemas.
    

Eleg√≠ **VOSK**, dado que ofrec√≠a una metodolog√≠a de trabajo m√°s amigable y r√°pida, de f√°cil instalaci√≥n y barato en cuanto a recursos computacionales, mayor documentaci√≥n y comunidad, m√°s modelos entrenados disponibles para su descarga y uso. As√≠ como una persona debe conocer el idioma para trabajarlo, el software tambi√©n debe contar con modelos entrenados para realizar tareas anal√≠ticas sobre el mismo de manera coherente. 

VOSK es una kit de herramientas para el reconocimiento de voces. Tiene una interfaz a trav√©s de Python, por lo que hac√≠a muy conveniente el desarrollo de scripts para automatizar el proceso de fonetizaci√≥n de los archivos de audio de larga duraci√≥n de habla en cada lengua recopilados. Comenc√© iterando **prompts** ‚Äìconversando‚Äì con ChatGPT para el desarrollo del script en cuesti√≥n. Este deber√≠a tomar un archivo de audio en cualquier formato como entrada y devolver en una carpeta todos los segmentos de fonemas separados cada uno en un nuevo archivo de audio corto ‚Äìde la duraci√≥n del fonema‚Äì. Este proceso de iteraci√≥n de desarrollo fue relativamente r√°pido pero a la vez curioso.

La conversaci√≥n pas√≥ por varias etapas, fue larga pero r√°pida y eficaz ya que primero necesitaba entender las diferencias entre dichas herramientas y luego ir generando el c√≥digo del programa. Esta *charla* fue en ingl√©s ya que el mayor caudal de informaci√≥n de entrenamiento para este LLM est√° en ingl√©s, por ende, las probabilidades de que los resultados sean m√°s precisos aumenta utilizando este idioma. Paso a narrar una versi√≥n resumida de esta conversaci√≥n donde surge el resultado del programa que utilic√© finalmente:

1. Ped√≠ que genere el c√≥digo necesario para realizar mi objetivo:
    
    > generate a script that would chop audio into phonemes using a combination of VOSK and ffmpeg. Its input would be an audio file containing speech in any language and it should output multiple audio files in a folder, each containing one phoneme of the initial audio file.
    > 
2. En principio comprendi√≥ su tarea, pero el script retornaba timestamps ‚Äìmarcas de tiempo‚Äì en formato de texto, de manera que habr√≠a que hacer el trabajo de separarlos manualmente.
    
    > I need the output to be audio files, not text. Also modify the code so that I can pass the input audio_file and model_path as parameters.
    > 
    
    To segment an audio file into phoneme audio files using Vosk and allow for input parameters, you can modify the code as follows. This code will take the input audio file, Vosk model path, and an output directory as parameters and save phoneme segments as separate audio files:
    
    - <C√≥digo>
        
        ```python
        import os
        import vosk
        import wave
        import shutil
        
        def segment_audio_to_phonemes(audio_path, model_path, output_dir):
            # Initialize the Vosk recognizer with the model
            model = vosk.Model(model_path)
            recognizer = vosk.KaldiRecognizer(model, 16000)
        
            # Open the audio file
            wf = wave.open(audio_path, 'rb')
        
            # Initialize variables
            results = []
            data = wf.readframes(4000)  # Read audio data in chunks
            segment_count = 0
        
            # Create the output directory if it doesn't exist
            os.makedirs(output_dir, exist_ok=True)
        
            while len(data) > 0:
                recognizer.AcceptWaveform(data)
                data = wf.readframes(4000)
        
                if recognizer.FinalResult():
                    # Get the final recognition result
                    result = recognizer.Result()
                    results.append(result)
        
                    # Save the phoneme segment as a separate audio file
                    phoneme_audio = recognizer.GetWaveform()
                    output_path = os.path.join(output_dir, f'segment_{segment_count}.wav')
                    with wave.open(output_path, 'wb') as outwf:
                        outwf.setnchannels(1)
                        outwf.setsampwidth(2)
                        outwf.setframerate(16000)
                        outwf.writeframes(phoneme_audio)
        
                    segment_count += 1
        
            return results
        
        def main(audio_file, model_path, output_dir):
            phoneme_results = segment_audio_to_phonemes(audio_file, model_path, output_dir)
        
            for i, result in enumerate(phoneme_results):
                print(f"Segment {i + 1}: {result}")
        
        if __name__ == "__main__":
            # Provide the input audio file, Vosk model path, and output directory as command-line arguments
            input_audio_file = "your_audio.wav"  # Replace with your audio file
            vosk_model_path = "path/to/your/model"  # Replace with the path to the Vosk model
            output_directory = "output_phoneme_segments"  # Replace with your desired output directory
        
            # Clear the output directory if it already exists
            if os.path.exists(output_directory):
                shutil.rmtree(output_directory)
        
            main(input_audio_file, vosk_model_path, output_directory)
        
        ```
        
    
    Make sure to provide the appropriate values for `input_audio_file`, `vosk_model_path`, and `output_directory` when running the script. The script will create separate audio files for each phoneme segment in the specified output directory.
    
3. No era exactamente lo que necesitaba, y el c√≥digo daba errores de funciones inexistentes, como si hubiera inventado partes de una librer√≠a que no existe. Acept√≥ su error y corrigi√≥ el c√≥digo, acerc√°ndose bastante al resultado, pero a√∫n con problemas que le hice saber, incluso d√°ndole la pista del *sample rate* donde yo pensaba que radicaba el problema:
    
    > two problems I have found:
    - the segment text of all results is always empty.
    - the chopped audio files seem to be in slow motion, I guess it was because of the sample rate being wrong, but maybe there's some other reason since we're now using pydub to automatically detect it. 
    can you fix these?
    also add a print for the sample rate to be able to check if it was detected correctly.
    > 
    
    I apologize for the issues. Let's address them and add a print statement to display the detected sample rate. Also, it's possible that the Vosk model used for segmentation may not provide phoneme-level output. You may need to use a different model that provides more granular results. Here's the updated script:
    
    - <C√≥digo>
        
        ```python
        import os
        import vosk
        import wave
        import shutil
        import argparse
        from pydub import AudioSegment
        
        def get_sample_rate(audio_path):
            # Use pydub to load the audio file and obtain its sample rate
            audio = AudioSegment.from_file(audio_path)
            return audio.frame_rate
        
        def segment_audio_to_phonemes(audio_path, model_path, output_dir):
            # Get the sample rate of the input audio file
            sample_rate = get_sample_rate(audio_path)
            print(f"Detected sample rate: {sample_rate}")
        
            # Initialize the Vosk recognizer with the model and the detected sample rate
            model = vosk.Model(model_path)
            recognizer = vosk.KaldiRecognizer(model, sample_rate)
        
            # Open the audio file
            wf = wave.open(audio_path, 'rb')
        
            # Initialize variables
            results = []
            data = wf.readframes(4000)  # Read audio data in chunks
            segment_count = 0
        
            # Create the output directory if it doesn't exist
            os.makedirs(output_dir, exist_ok=True)
        
            phoneme_segments = {}  # Store phoneme boundaries and audio data
            phoneme_start = 0  # Start time of the current phoneme segment in milliseconds
        
            while len(data) > 0:
                recognizer.AcceptWaveform(data)
                data = wf.readframes(4000)
        
                if recognizer.FinalResult():
                    # Get the final recognition result, which includes phonemes
                    result = recognizer.Result()
                    results.append(result)
        
                    # Extract the end time of the phoneme from the audio frame position
                    phoneme_end = wf.tell() / sample_rate * 1000  # Calculate the end time of the phoneme in milliseconds
                    phoneme_duration = phoneme_end - phoneme_start
        
                    # Store the phoneme boundaries and audio data
                    phoneme_segments[segment_count] = {
                        "start": phoneme_start,
                        "end": phoneme_end,
                        "duration": phoneme_duration,
                    }
        
                    phoneme_start = phoneme_end
                    segment_count += 1
        
            # Save the phoneme segments as separate audio files
            wf = wave.open(audio_path, 'rb')  # Reopen the audio file
            for segment_number, boundaries in phoneme_segments.items():
                start_frame = int(boundaries["start"] / 1000 * sample_rate)
                end_frame = int(boundaries["end"] / 1000 * sample_rate)
        
                # Extract the audio data for the phoneme segment
                wf.setpos(start_frame)
                audio_data = wf.readframes(end_frame - start_frame)
        
                # Save the phoneme segment as an audio file
                output_path = os.path.join(output_dir, f'segment_{segment_number}.wav')
                with wave.open(output_path, 'wb') as outwf:
                    outwf.setnchannels(1)
                    outwf.setsampwidth(2)
                    outwf.setframerate(sample_rate)
                    outwf.writeframes(audio_data)
        
            return results
        
        def main(args):
            phoneme_results = segment_audio_to_phonemes(args.input_audio_file, args.vosk_model_path, args.output_directory)
        
            for i, result in enumerate(phoneme_results):
                print(f"Segment {i + 1}: {result}")
        
        if __name__ == "__main__":
            parser = argparse.ArgumentParser(description="Segment audio into phoneme audio files using Vosk.")
            parser.add_argument("input_audio_file", type=str, help="Path to the input audio file.")
            parser.add_argument("vosk_model_path", type=str, help="Path to the Vosk model.")
            parser.add_argument("output_directory", type=str, help="Directory to save phoneme audio segments.")
        
            args = parser.parse_args()
            main(args)
        
        ```
        
    
    This updated script should display the detected sample rate, and it retains the functionality to segment the audio into phonemes and save them as audio files. If you encounter issues with the phoneme segment text being empty, you may need to investigate whether the chosen Vosk model provides phoneme-level recognition results.
    
4. Supuestamente deb√≠a generarme, adem√°s de los segmentos de audio, texto de los fonemas separados, pero esto nunca lo logr√≥, as√≠ que descart√© ese resultado ya que s√≥lo me interesaban los resultados sonoros para el experimento. El problema fue que tampoco solucion√≥ los resultados de audio que parec√≠an estar en *slow motion.* 
    
    > As I mentioned, the files that the script outputs seem to be pitched down, or slow motion. I need the output wav files to playback at the same pitch as the input file. It seems like the playback speed is wrong.
    > 
    
    Esta vez no hizo nada m√°s que agregar un comentario en el c√≥digo. Un c√≥digo comentado no es computado por el int√©rprete de Python, es s√≥lo una aclaraci√≥n para el programador humano.
    
    `# Save the phoneme segment as an audio file with the same sample rate`
    
    > you didn't change any code, just a comment, that doesn't change the result. Please correct yourself.
    > 
    
    Esta vez no cambi√≥ **nada**.
    
    > you are still not addressing the issue
    > 
    
    I apologize for any confusion. To ensure that the output audio files maintain the same pitch and playback speed as the input audio, you should also copy the input audio's sample width and number of channels when saving the phoneme segments. Here's the corrected code:
    
    - <C√≥digo>
        
        ```python
        # VOSK_phoneme_slicer.py
        
        import os
        import vosk
        import wave
        import shutil
        import argparse
        from pydub import AudioSegment
        
        def get_audio_properties(audio_path):
            # Use pydub to load the audio file and obtain its properties
            audio = AudioSegment.from_file(audio_path)
            return audio.frame_rate, audio.sample_width, audio.channels
        
        def segment_audio_to_phonemes(audio_path, model_path, output_dir):
            # Get the properties of the input audio file
            sample_rate, sample_width, channels = get_audio_properties(audio_path)
            print(f"Detected sample rate: {sample_rate}")
        
            # Initialize the Vosk recognizer with the model and the detected sample rate
            model = vosk.Model(model_path)
            recognizer = vosk.KaldiRecognizer(model, sample_rate)
        
            # Open the audio file
            wf = wave.open(audio_path, 'rb')
        
            # Initialize variables
            results = []
            data = wf.readframes(4000)  # Read audio data in chunks
            segment_count = 0
        
            # Create the output directory if it doesn't exist
            os.makedirs(output_dir, exist_ok=True)
        
            phoneme_segments = {}  # Store phoneme boundaries and audio data
            phoneme_start = 0  # Start time of the current phoneme segment in milliseconds
        
            while len(data) > 0:
                recognizer.AcceptWaveform(data)
                data = wf.readframes(4000)
        
                if recognizer.FinalResult():
                    # Get the final recognition result, which includes phonemes
                    result = recognizer.Result()
                    results.append(result)
        
                    # Extract the end time of the phoneme from the audio frame position
                    phoneme_end = wf.tell() / sample_rate * 1000  # Calculate the end time of the phoneme in milliseconds
                    phoneme_duration = phoneme_end - phoneme_start
        
                    # Store the phoneme boundaries and audio data
                    phoneme_segments[segment_count] = {
                        "start": phoneme_start,
                        "end": phoneme_end,
                        "duration": phoneme_duration,
                    }
        
                    phoneme_start = phoneme_end
                    segment_count += 1
        
            # Save the phoneme segments as separate audio files
            wf = wave.open(audio_path, 'rb')  # Reopen the audio file
            for segment_number, boundaries in phoneme_segments.items():
                start_frame = int(boundaries["start"] / 1000 * sample_rate)
                end_frame = int(boundaries["end"] / 1000 * sample_rate)
        
                # Extract the audio data for the phoneme segment
                wf.setpos(start_frame)
                audio_data = wf.readframes(end_frame - start_frame)
        
                # Save the phoneme segment as an audio file with the same properties as the input audio
                output_path = os.path.join(output_dir, f'segment_{segment_number}.wav')
                with wave.open(output_path, 'wb') as outwf:
                    outwf.setnchannels(channels)
                    outwf.setsampwidth(sample_width)
                    outwf.setframerate(sample_rate)
                    outwf.writeframes(audio_data)
        
            return results
        
        def main(args):
            phoneme_results = segment_audio_to_phonemes(args.input_audio_file, args.vosk_model_path, args.output_directory)
        
            for i, result in enumerate(phoneme_results):
                print(f"Segment {i + 1}: {result}")
        
        if __name__ == "__main__":
            parser = argparse.ArgumentParser(description="Segment audio into phoneme audio files using Vosk.")
            parser.add_argument("input_audio_file", type=str, help="Path to the input audio file.")
            parser.add_argument("vosk_model_path", type=str, help="Path to the Vosk model.")
            parser.add_argument("output_directory", type=str, help="Directory to save phoneme audio segments.")
        
            args = parser.parse_args()
            main(args)
        
        ```
        
    
    This corrected code ensures that the output audio files maintain the same sample width and number of channels as the input audio, preserving the pitch and playback speed. Please try running this updated code, and let me know if it resolves the issue.
    

En resumen, la conversaci√≥n con ChatGPT para generar el c√≥digo, como si fuera un empleado *ad-honorem* de la investigaci√≥n, pas√≥ por varias etapas. En principio, al no lograr el objetivo requerido, se fue corrigiendo el pedido para alinearlo con este. Luego, la aparici√≥n de errores propios del c√≥digo, como m√©todos ‚Äìfunciones‚Äì que no existen o se llaman de otra forma, y por √∫ltimo un problema sobre los audios generados que no eran generados con el sample rate correcto. 

Finalmente, corr√≠ el √∫ltimo [script sugerido](https://www.notion.so/Un-recorrido-tecno-sonoro-hacia-el-habla-as-mica-31eaddcc2bb841bcaa124ed890ed8ca7?pvs=21) sobre uno de los audios de mi dataset para realizar la separaci√≥n de los archivos de audio. VOSK necesita el archivo del modelo espec√≠fico para comprender el idioma con el que va a trabajar, por suerte cuentan con varios idiomas ya entrenados y [disponibles](https://alphacephei.com/vosk/models) para su descarga y uso. A continuaci√≥n algunos de los resultados:

- El audio original a recortar fue de la [conversaci√≥n en franc√©s](https://www.notion.so/Un-recorrido-tecno-sonoro-hacia-el-habla-as-mica-31eaddcc2bb841bcaa124ed890ed8ca7?pvs=21) descargada de YouTube referido anteriormente.
- Algunos recortes:

[segment_0.wav](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/segment_0.wav)

[segment_2.wav](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/segment_2.wav)

[segment_1.wav](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/segment_1.wav)

[segment_3.wav](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/segment_3.wav)

Al analizar los resultados, los segmentos de fonemas tienen la misma corta duraci√≥n, lo cual me dio sospechas, sin embargo parecen ser unidades b√°sicas de la lengua en cuesti√≥n. Imagin√© de inmediato que deb√≠a conseguir el modelo para todos los idiomas que quisiera integrar en mi sistema, y si no estaban disponibles habr√≠a que generarlos mediante entrenamiento. Comenc√© a pensar en otras alternativas para realizar esta separaci√≥n de forma m√°s homog√©nea en los idiomas, dado que adem√°s, no los conozco a todos como para juzgar si los resultados generados ser√≠an correctos.

# Conversaciones con colegas

Luego de las √∫ltimas pruebas y experimentaciones sobre la separaci√≥n de fonemas, surgi√≥ la necesidad de extender la mano hacia colegas que trabajaron con tecnolog√≠as aleda√±as en busca de estrategias alternativas para acercarse al resultado sonoro deseado en el principio de esta investigaci√≥n. **ArchiVoz** es un proyecto derivado de [**Intercambios Transorg√°nicos**](https://intercambiostransorganicos.org/), grupo de investigaci√≥n radicado en UNTREF dirigido por [**Gala Luc√≠a Gonz√°lez Barrios**](https://gala.bio/), del cual fui parte entre 2014 y 2018, quedando vinculado en forma de consultor externo para necesidades t√©cnicas puntuales. Se puede encontrar m√°s informaci√≥n en el Trabajo Final de Grado de Gala //AGREGAR REFE ZOTERO//.
Gala dio el puntapi√© inicial para hablar con dos investigadores de su equipo con experiencia en tecnolog√≠as basadas en inteligencia artificial para la generaci√≥n de audio, espec√≠ficamente, de habla generado por sistemas TTS ‚Äìtexto a habla‚Äì.

## ArchiVoz

### **Mat√≠as Di Bernardo**

Mat√≠as Di Bernardo es miembro activo de [ArchiVoz](https://intercambiostransorganicos.org/archivoz/) involucrado en el desarrollo t√©cnico con tecnolog√≠as de inteligencia artificial para la generaci√≥n de voces. Espec√≠ficamente, trabaj√≥ en tomar un modelo TTS llamado [Tacotron](https://pytorch.org/hub/nvidia_deeplearningexamples_tacotron2/) y realizar [fine-tuning](https://www.notion.so/Un-recorrido-tecno-sonoro-hacia-el-habla-as-mica-31eaddcc2bb841bcaa124ed890ed8ca7?pvs=21) para extrapolar un modelo que originalmente habla ingl√©s, al idioma espa√±ol.
Teniendo en cuenta que existen referencias *perform√°ticas* de personas imitando idiomas eliminando el contenido sem√°ntico //\{REFE AL VIDEO} //, una de las ideas fue combinar estos *semi-idiomas* inventados con su par existente para formar un dataset con el que alimentar un modelo. La hip√≥tesis es que esto funcionar√≠a como una suerte de fine-tuning del modelo a la inversa, es decir, que en lugar de ir en direcci√≥n a ajustar la precisi√≥n de alg√∫n acento, idioma, g√©nero de esa voz, se busca deformar su sem√°ntica, y preservar ‚Äìo no‚Äì el resto de las caracter√≠sticas.

La cuesti√≥n principal que se rescat√≥ del encuentro fue que el problema de recorrer un espacio latente de manera aleatoria, o incluso arbitraria, para generar inferencia sobre el modelo y resulte en sonidos similares a la voz era muy baja. Esto se debe a que los modelos de inteligencia artificial funcionan como una caja negra. Encontrar los caminos dentro de una red neuronal que generen las piezas sonoras deseadas es como encontrar una aguja en un pajar. Quiz√°s entrenando el modelo lo suficiente de manera que logre generalizar la voz humana al punto que sin importar qu√© camino neuronal se tome siempre encontremos semejanza con el habla es una pregunta de investigaci√≥n dentro del campo de la inteligencia artificial que requiere mucho m√°s tiempo, recursos y trabajo que excede esta instancia. Esto es una motivaci√≥n para continuar, no para detenerse. Los modelos que logran estos espacios latentes ricos de referencias son masivos. Por ejemplo, DALL-E o Stable Diffusion // **REFES** //, en momentos primitivos del proceso de inferencia ya logran dibujar bordes de objetos, es decir, diferenciar figura-fondo.
**//REFE IMAGEN BLANCO Y NEGRO figura fondo// // O SAMPLING STEPS //**

El desarrollo de un modelo que logre semejante nivel de riqueza en su espacio latente es tarea de grandes equipos y/o empresas que reciben un alto caudal de inversi√≥n. Por suerte existen modelos open-source de este calibre que podr√≠an ajustarse a las necesidades de este proyecto, pero para especular estos resultados debemos realizar las pruebas correspondientes, y entrenar para mover estos modelos tambi√©n es computacionalmente caro si buscamos un cambio radical en sus resultados.

### **Sebast√≠an Sosa Welford**

Por otro lado, Sebasti√°n Sosa Welford que ya no es parte activa de Intercambios Transorg√°nicos, al momento de este escrito se desarrolla como asistente de investigadores de CONICET realizando tareas t√©cnicas sobre el trabajo con voces. Sebasti√°n hizo hincapi√© en que el problema en cuesti√≥n no est√° allanado en el campo de estudio de la generaci√≥n de voces por computadora ya que lo que la ciencia que impulsa esta tecnolog√≠a busca resolver otro problema, que en esta √©poca ya est√° bastante bien logrado. Se trata de generar voces que hablen a la perfecci√≥n cada idioma, inclusive, muchos idiomas con la misma voz. Un ser artificial pol√≠glota, que conoce de emociones y sus entonaciones, acentos de distintas regiones, y hasta puede *encarnar* el g√©nero que se le solicite.

Un concepto propuesto por Sosa cuando escuch√≥ la descripci√≥n del proyecto es el de la "prosodia macro-din√°mica del lenguaje". Este t√©rmino enmarcado dentro de la ling√º√≠stica resulta muy apropiado para la descripci√≥n de la forma, la cadencia, el ritmo y dem√°s aspectos formales del habla que dejan de lado la sem√°ntica.

Una de las tareas de Sebasti√°n en su trabajo fue realizar la anotaci√≥n de timestamps ‚Äìmarcas de tiempo‚Äì que indiquen en qu√© momento comienza y termina una s√≠laba. Para ello, utiliz√≥ Praat, aquel software especializado en el an√°lisis de habla a trav√©s de la fon√©tica desarrollado en la Universidad de Amsterdam, sumado a un script en Python que llama a las funciones de dicha aplicaci√≥n para realizar las operaciones necesarias para la separaci√≥n en estas unidades del lenguaje. Este punto en com√∫n sum√≥ otro enfoque distinto de c√≥mo realizar una tarea que ya se hab√≠a realizado anteriormente, aunque la separaci√≥n fue con fonemas, tanto con otra herramienta para el trabajo con voces ‚ÄìVOSK‚Äì como con AudioStellar, que s√≥lo detectaba transientes en el audio como l√≠mites de separaci√≥n.

El joven investigador advirti√≥ que la separaci√≥n de un audio de lenguaje hablado en unidades m√°s b√°sicas como s√≠labas o fonemas es una tarea cuasi-subjetiva por el hecho de que depende de cada idioma y de cada hablante. Adem√°s, la precisi√≥n de d√≥nde comienzan y terminan estas unidades puede ubicarse infinitamente antes o despu√©s de los que propone cualquier software de an√°lisis ling√º√≠stico, inclusive si esta tarea se realiza manualmente por un actor humano.
Dentro de los sistemas TTS, para preparar un modelo debe ser entrenado con un dataset que contenga texto y su componente sonora, en otras palabras, una voz que habla y el texto de lo que dice de manera que la informaci√≥n est√° emparejada. Una idea que surge en esta conversaci√≥n es que estas dos versiones de la *misma* informaci√≥n **no** coincidan. Si se entrena un modelo con informaci√≥n "incorrecta" suponemos que vamos a tener un sistema impredecible, que al pedirle que diga algo, dir√° otra cosa. Desde luego que es de suma importancia este *des-emparejamiento*, c√≥mo se logre, qu√© texto corresponder√° con qu√© sonido de habla, tan importante como si se quisiera hacer un TTS funcional en cualquier idioma. Pero tambi√©n es importante tener en cuenta, a prop√≥sito de este proyecto, que la sonoridad de esa voz ser√≠a similar a la que le fue dada. ¬øAprender√° siempre igual? ¬øTendr√°n siempre coincidencias err√≥neas al ponerlo a prueba? Quiz√°s dependa qu√© nivel de aprendizaje se alcance, con cu√°nta precisi√≥n pueda representar estas nuevas relaciones entre texto y sonido.

Ambos investigadores hab√≠an centrado su trabajo en las tecnolog√≠as de text-a-habla ‚ÄìTTS‚Äì que hab√≠an sido descartadas en un principio por la necesidad de inserci√≥n de texto. Esta operaci√≥n tiene una relaci√≥n muy directa con el sentido, ya que las voces generadas hablar√≠an lo que se les pida... Entra Mateo Amaral.

## **Mateo Amaral**

[Mateo Amaral](https://mateoamaral.com/) fue el √∫ltimo convocado en estos encuentros. Es artista visual con √©nfasis en nuevos medios, colabora con su hermano para el desarrollo t√©cnico de la generaci√≥n de mundos virtuales generativos, con personajes que hablan en una extra√±a lengua con similitudes a varias de las que conocemos, sobretodo ingl√©s.

Su principal fuente de sonidos de voces son las tecnolog√≠as TTS, utilizadas de forma novedosa. Utiliz√≥ servicios como [Uberduck](https://www.uberduck.ai/) y [Musicfy](https://musicfy.lol/). La estrategia fue alimentar estos sistemas con *cualquier cosa*. En lugar de insertar texto legible, reconocible, y en un idioma en particular, lo √∫nico respetado fue nuestro teclado occidental como medio a escribir un sinsentido de caracteres para que el sistema los vocalice.

Por otro lado tambi√©n intentaron entrenar un modelo de Tacotron, pero incluso comprando cr√©ditos computacionales en la nube de Google para poder prepararlo r√°pidamente con mayor poder de c√≥mputo, llegaron lejos de sus resultados esperados. Se sintieron abrumados por la tecnolog√≠a y la falta de documentaci√≥n de aquel entonces en esta materia y prefirieron dedicar sus energ√≠as a la generaci√≥n de entornos 3D y personajes autogenerativos, incluyendo sus comportamientos en los escenarios que propone Mateo.

<aside>
‚ùó Una aclaraci√≥n importante es que, como se ya se ha visto, los objetivos planteados se basan en predicciones de sus resultados. Estas hip√≥tesis vienen del conocimiento de estas tecnolog√≠as y sus posibilidades, pero dada la naturaleza de estos sistemas, sus resultados pueden ser impredecibles. Conocer las internas de un modelo, en esencia matem√°tico, puede ser una tarea insondable para un humano. La predictibilidad de las inferencias generadas por modelos de inteligencia artificial depende en gran medida de varios factores, como el tipo y tama√±o del modelo, la calidad de los datos de entrenamiento y la naturaleza de la tarea en cuesti√≥n. En este caso, la generaci√≥n de audio semejante a una voz ‚Äìque puede ser de cualquier forma‚Äì es de naturaleza muy poco predecible, a diferencia de, por ejemplo, la traducci√≥n autom√°tica de un texto, que sabiendo el idioma, podemos adelantarnos al resultado.

</aside>

---

# Desarrollo T√©cnico pt. 2: Text-to-Speech

## Tacotron y FastSpeech

- allanar el camino con ambas tecnolog√≠as
- pruebas con collab tacotron

- buscar herramientas ya logradas online: ttsmaker
P√°rrafo explicando nueva forma de llegar:

Al llegar a [TTSmaker.com](http://TTSmaker.com) y gracias a la charla con Mateo, la idea de no involucrar texto me pareci√≥ caduca. Haciendo pruebas con todas las herramientas online que encontr√©, ide√© un flujo que me pareci√≥ interesante en cuanto a proceso y sus resultados. Escribir sus instrucciones como *performance inform√°tica* es un nuevo devenir de este proceso que describo a continuaci√≥n. Invito y celebro su interpretaci√≥n y modificaci√≥n.

## Instrucciones para un experimento TTS *(Experimento #31)*

1. [https://www.randomtextgenerator.com/](https://www.randomtextgenerator.com/) en esta pagina hacer texto en idioma
    1. Si no est√° el idioma, translate.google.com
    2. Revisar el texto generado utilizando un traductor como el mencionado, para ver su contenido. En lo posible que no repita tantas palabras, ni que haga sentido. Si hay coherencia en las oraciones pero el contenido del mensaje es surreal, vale para los prop√≥sitos de esta investigaci√≥n.
2. Pasar por [https://thinkzone.wlonk.com/Gibber/GibGen.htm](https://thinkzone.wlonk.com/Gibber/GibGen.htm)
    1. Revisar Gibberish Level. Probar en 3. Si el texto no parece muy coherente, o est√° muy mezclado (juzgado a partir de una noci√≥n del idioma) o el resultado sonoro ‚Äúno coincide tanto con el imaginario de esa lengua‚Äù subir un punto.
3. Generar habla con [https://ttsmaker.com/](https://ttsmaker.com/)
    1. Probar distintas voces
    2. Intentar que el audio resultante est√© entre 30 y 40 segundos
4. La estructura de carpetas para la organizaci√≥n de sonidos resultantes podr√≠a ser la siguiente
    
    ![Untitled](Un%20recorrido%20tecno-sonoro%20hacia%20el%20habla%20ase%CC%81mica%2031eaddcc2bb841bcaa124ed890ed8ca7/Untitled%203.png)
    
    O bien generalizando:
    
    ```bash
    + <Carpeta de un idioma>
    	- Textos.txt # Documento de texto que contiene el texto que es reproducido por el TTS.
    	- ttsmaker-file-<fecha> <n√∫mero>.wav # Donde n√∫mero corresponde al p√°rrafo dentro del archivo de texto.
    + <Otra carpeta de otro idioma>
    	- Textos.txt
    	- ttsmaker-file-<fecha> <n√∫mero>.wav
    ...
    # Tantos como idiomas se deseen agregar.
    ```
    
5. Un ejemplo del contenido de los archivos de texto es:
    
    ```
    1
    De se so soubemos matada pentadessem. Alimindo comedefez os mas obederraderetado se muda no te se. Cum ques emponge janter. Cartessa esta notimido ho qua te conros te confuncido filigas tu vidos as be. Aponge as manda noivem em te ha olvempesfacominar mente mando. Carra endos ha. Alimpros has manar as em. Apos manteis avellam expleto se cape. Sei irrana dosa pitadadaveo nosa noivo. De mausoube. Papantao. Bar larra cape. La pendivido do imeta seguessemo procebo erra em erradomina neliguietticahindo 
    
    2
    Ja sr litterato tolica patuscadas tratado por sujeitava. Vintens nervoso um fio. Entribue aos mal. Caminhar arreligioso vao rez inda vao religioso vao rez acces levantado ser que estincoes le fez accento tem nos affligioso dor sujeitae. Esmeros tento causara identes. Fazendo pe. Caminhava rajadas um oh fechava. Nos affligir symbolica documentes. Fazenda vao esmeros teu rez accesso um oh fechava. Retento por evitae. Esmerossas trando paiol vereram tal esta vae evidento companto tolica documento t
    ```
    
6. Recortar archivos de audio con AudioStellar o [`VOSK-phoneme-slicer.py`](https://www.notion.so/Un-recorrido-tecno-sonoro-hacia-el-habla-as-mica-31eaddcc2bb841bcaa124ed890ed8ca7?pvs=21) para generar segmentos peque√±os, uno por cada fonema.
**NOTA**: En este caso, utilic√© exclusivamente **AudioStellar** para esta tarea dado que este separa a partir de transientes. Esta estrategia √∫nica para todos los idiomas permite cierta homogeneidad de c√≥mo los analizamos, sin considerar reglas espec√≠ficas de cada uno. Caso contrario, se deber√≠a conseguir un modelo entrenado por cada lengua ‚Äìo desarrollarlo‚Äì para realizar una separaci√≥n de fonemas espec√≠fica debido a que no todos tienen las mismas reglas fon√©ticas. Esto complejiza y alarga bastante la tarea y no aporta considerablemente al resultado final sonoro.
Arm√© un nuevo script para utilizar los m√©todos que AudioStellar utiliza en su c√≥digo para realizar los recortes. Para esto Leandro me indic√≥ en qu√© parte del c√≥digo de ASt se encuentran estas funcionalidades y tom√© lo que necesitaba para armar el siguiente script:
    - `slicer_onset.py`
        
        ```python
        # SEPARATE AUDIOS INTO SEGMENTS FOR EACH LANGUAGE FILE.
        # TFG AAEE jaunmatrin
        
        # REQUIRES LIBROSA 0.9.1 NOT HIGHER
        # Python 3.10
        
        import os
        import librosa
        import numpy as np
        import scipy.io.wavfile
        import os
        
        dir = "./ElevenLabs-JM-test"
        
        supportedFileTypes = ["wav", "mp3"]
        
        def findOnsetsAndCut(window_max, window_avg, delta, backtrack):
            global onset_frames, onset_times, times
        
            hop_length = 512
            o_env = librosa.onset.onset_strength(y=segmento, sr=sr)
            times = librosa.times_like(o_env, sr=sr)
        
            window_max_final = window_max * sr // hop_length
            window_avg_final = window_avg * sr // hop_length
        
            onset_frames = librosa.onset.onset_detect(
                onset_envelope=o_env,
                sr=sr,
                hop_length=hop_length,
                backtrack=backtrack,
                delta=delta,
                # wait=0,
                pre_avg=window_avg_final,
                post_avg=window_avg_final + 1,
                pre_max=window_max_final,
                post_max=window_max_final + 1,
            )
        
            onset_times = librosa.frames_to_time(onset_frames, sr)
        
        def loadFile(filename):
            global x, sr, segmento
        
            target = filename
            if not os.path.isfile(target):
                return
            else:
                x, sr = librosa.load(target, sr=None)
        
            segmento = x
        
        def fadeNormalizeSave(
            folder,
            fadeSamples=1000,
            normalize=True,
        ):
            segmentoFade = np.copy(segmento)
        
            for f in range(1, len(onset_frames)):
                sampleFinish = librosa.core.frames_to_samples(onset_frames[f])
        
                j = fadeSamples
                for i in np.arange(sampleFinish - fadeSamples, sampleFinish + 1):
                    segmentoFade[i] *= j / fadeSamples
                    j -= 1
        
            filePath = os.path.dirname(audioFilePath)
            if not os.path.exists(folder):
                os.mkdir(folder)
        
            for i in range(len(onset_frames) - 1):
                timeF = librosa.core.frames_to_samples(onset_frames[i])
                timeT = librosa.core.frames_to_samples(onset_frames[i + 1])
                filename, ext = os.path.splitext(audioFile)
                archivoAGuardar = f"{filename}_{i}.wav"
                segmentoAGuardar = segmentoFade[timeF:timeT]
        
                if normalize:
                    segmentoAGuardar = np.array(
                        (segmentoAGuardar / np.max(np.abs(segmentoAGuardar)))
                    )
        
                scipy.io.wavfile.write(archivoAGuardar, sr, segmentoAGuardar)
        
        def process_folder(input_folder):
            global audioFilePath, audioFile
            for root, dirs, files in os.walk(input_folder):
                for file in files:
                    if os.path.splitext(file)[1][1:].lower() in supportedFileTypes:
                        audio_file = os.path.join(root, file)
                        audioFile = audio_file
                        audioFilePath = audio_file
                        process_audio_file(audio_file)
        
        def process_audio_file(audio_file):
            loadFile(audio_file)
            if segmento.size > 0:
                findOnsetsAndCut(window_max, window_avg, delta, backtrack)
                if onset_frames.any():
                    folder = os.path.dirname(audio_file)
                    print("folder: ", folder)
                    folder_name = os.path.basename(folder)
                    print("folder_name: ", folder_name)
                    # output_folder = os.path.join(folder, folder_name + "_slices")
                    output_folder = folder
                    print("output_folder: ", output_folder)
                    os.makedirs(output_folder, exist_ok=True)
                    fadeNormalizeSave(output_folder, fade, normalize)
        
        if __name__ == "__main__":
            print("Onset Detection Slicer")
            input_folder = dir
            window_max = 0.10
            window_avg = 0.05
            delta = 0.10
            backtrack = True
            fade = 1000
            normalize = True
        
            print("processing folder: ", input_folder)
            process_folder(input_folder)
        
        ```
        
    
    De esta forma, con la estructura de carpetas del paso anterior, se generar√≠an las *tajadas* de cada idioma en lote, de forma program√°tica, en cuesti√≥n de minutos. S√≥lo se debe especificar los siguientes par√°metros en el script, que son los mismos que ofrece la interfaz de AudioStellar:
    
    - Ver par√°metros:
    Para mayor informaci√≥n, revisar la documentaci√≥n de `librosa` [aqu√≠](https://librosa.org/doc/main/generated/librosa.onset.onset_detect.html#librosa.onset.onset_detect).
        
        ```python
        input_folder = dir # Directorio contenedor de las carpetas de idiomas
        window_max = 0.10
        window_avg = 0.05
        delta = 0.10
        backtrack = True
        fade = 1000
        normalize = True
        ```
        
7. Crear mapa en AudioStellar utilizando **MFCC**, algoritmo que ignora caracter√≠sticas de pitch ‚Äìideal para voces de diferentes personas‚Äì pero conserva caracter√≠sticas t√≠mbricas ‚Äìprosodia‚Äì de manera que agrupe los fonemas sin importar quien lo *diga*. MFCC es el algoritmo m√°s cercano a una subjetividad perceptiva del sonido.
8. Experimentar.

AGREGAR RECORRIDOS CON LAS FUNCIONALIDADES DE AST VIDEO

<aside>
üí° Este experimento es altamente escalable, es decir que cuanto m√°s largos, o m√°s archivos generados por el TTS se utilicen, habr√° mayor riqueza de fonemas, m√°s contenido por idioma. La cantidad de lenguas, y cu√°les, depender√° de la b√∫squeda sonora del experimento.

</aside>

### Consideraci√≥n sobre idiomas con ideogramas

El Chino, por ejemplo, est√° compuesto por ideogramas, por lo que mezclar sus caracteres no hace mucho *sentido*. Es decir que no podemos inventar, al menos con este tipo de herramientas, palabras que no existan a la hora de ser pronunciadas. Quiz√°s la oraci√≥n no tenga sentido pero las palabras que la componen s√≠, y esto es diferente a los otros casos dado el algoritmo utilizado en la herramienta que convierte texto a *gibberish* (ver c√≥digo fuente en el mismo [sitio web](https://thinkzone.wlonk.com/Gibber/GibGen.htm)).

Una estrategia para mezclar el texto y evitar sonidos repetidos es borrar caracteres aleatorios en el texto a reproducir, por lo que se sugiere generar uno largo al cual se le puedan eliminar fragmentos hasta lograr el efecto y la duraci√≥n deseada.

### Alternativa a paso 1 y 2

Cuando el generador de texto aleatorio no ofrece determinado idioma, y si se traduce textualmente no favorece a la sonoridad *conocida* conviene utilizar ChatGPT ‚Äìu otro modelo de lenguaje‚Äì para generar texto en el idioma deseado.

Por ejemplo, para turco esta fue la conversaci√≥n con ChatGPT:

- generate a random turkish text of 100 words
    
    > Certainly, here's a random 100-word gibberish text in Turkish:
    > 
    > 
    > "Gibberish metinleri, yabancƒ± dillerin seslerini taklit eden ilgin√ß bir yolculuƒüa davet ediyor. Zƒ±pƒ±r zƒ±pƒ±r harf sƒ±ralamalarƒ±, d√º≈ü√ºnce d√ºnyasƒ±nƒ± farklƒ± bir evrene ta≈üƒ±yor. Bu tuhaf dilin anlamƒ± yok, ama onun sesleri ve ritmi ≈üa≈üƒ±rtƒ±cƒ± derecede ger√ßek√ßi. Ku≈ülar zƒ±pƒ±rdƒ±yor, r√ºzgar zƒ±pƒ±rdƒ±yor ve g√ºne≈ü yava≈ü√ßa batƒ±yor. Bu yazƒ±larƒ± anlamasanƒ±z bile, zƒ±plamak istediƒüiniz hissine kapƒ±lacaksƒ±nƒ±z. Haydi, zƒ±playalƒ±m ve bu dilin sƒ±rlarƒ±nƒ± ke≈üfedelim. Zƒ±pƒ±r zƒ±pƒ±r zƒ±p, zƒ±pƒ±r zƒ±pƒ±r zƒ±p, zƒ±pƒ±r zƒ±pƒ±r zƒ±p!"
    > 
    > Bu yazƒ± tamamen anlamsƒ±zdƒ±r ve T√ºrk√ße ile ilgisi yoktur, ama ses ve harf sƒ±ralamalarƒ± T√ºrk√ßeyi taklit etmektedir.
    > 
- make it sound more turkish not so gibberish
    
    > Certainly, here's a 100-word text in Turkish that may not be gibberish but is still non-sensical:
    > 
    > 
    > "Bug√ºn g√ºzel bir g√ºn. G√ºne≈ü parlƒ±yor, ku≈ülar cƒ±vƒ±l cƒ±vƒ±l ≈üarkƒ±lar s√∂yl√ºyor. Etrafta ye≈üil tepeler ve mavi g√∂ky√ºz√º ile huzur dolu bir manzara var. ƒ∞nsanlar g√ºler y√ºzl√º, sokaklarda gezip dola≈üƒ±yorlar. Ama aynƒ± zamanda, kƒ±rmƒ±zƒ± armutlar d√∂k√ºl√ºyor aƒüa√ßlardan ve ≈üeftali √ßi√ßekleri kƒ±≈üƒ±n a√ßƒ±yor. Her ≈üey m√ºthi≈ü bir uyum i√ßinde, ama aslƒ±nda bu tamamen hayal √ºr√ºn√º bir d√ºnya. Biraz daha yakƒ±ndan bakƒ±nca, aniden ye≈üil elma yaƒümurunun yaƒüdƒ±ƒüƒ±nƒ± ve √ßikolata √ßi√ßeklerinin g√ºl√ºmsediƒüini g√∂receksiniz. Her ≈üey m√ºmk√ºn bu harika d√ºnyada."
    > 
    > This text is constructed to sound more like a coherent passage in Turkish, even though the content is still nonsensical.
    > 
- make another one, emphasizing how turkish it may sound when spoken aloud
    
    > Certainly, here's a 100-word text in Turkish that emphasizes the Turkish language's phonetic and rhythmic qualities when spoken aloud:
    > 
    > 
    > "G√ºn doƒüar, ku≈ülar kanat √ßƒ±rpar, r√ºzgar hafif√ße eserken, doƒüanƒ±n melodisi kulaklarƒ± √ßƒ±nlatacak kadar g√ºzeldir. Denizin dalgalarƒ± sahile vurur, √ßocuklar g√ºlerek oynar, ya≈üamƒ±n ritmi hƒ±zlƒ±ca akar. K√∂yde simit√ßiler simitlerini taze taze satar, √ßaycƒ±lar √ßaylarƒ±nƒ± demler ve √ßay bardaklarƒ±nƒ± birer birer dizerek servis eder. Sohbetler kahkahalarla dolup ta≈üar, t√ºm renkler birbirine karƒ±≈üƒ±r ve g√ºl√º≈üler y√ºrekleri ƒ±sƒ±tƒ±r. Her bir kelime, her bir hece, bu g√ºzel dili konu≈üurken sanki birer nota gibi √ßalar. T√ºrk√ße, kulaklara ho≈ü gelen bir ≈üarkƒ±dƒ±r."
    > 
    > This text aims to capture the phonetic and rhythmic qualities of the Turkish language when spoken aloud, while the content remains non-sensical.
    > 

Los √∫ltimos dos ejemplos son los que luego pas√© por TTSmaker y utilizados en el experimento de la cartograf√≠a con ASt.

[Exploraci√≥n manual con *units* de AudioStellar.](https://drive.google.com/file/d/1ni5xYik7aKoKLQKXgf6DeqWNqAbPl6hG/view?usp=drive_link)

Exploraci√≥n manual con *units* de AudioStellar.

## AudioStellar + PureData

Habiendo recorrido las nuevas [Cartograf√≠as Mundiales del Habla](https://www.notion.so/Un-recorrido-tecno-sonoro-hacia-el-habla-as-mica-31eaddcc2bb841bcaa124ed890ed8ca7?pvs=21) con las unidades provistas por AudioStellar, conversando con un amigo que fue parte de su equipo de desarrollo, [Tomas Ciccola](https://szgy.ahh.red/) me sugiere la idea de investigar nuevas formas de recorrer mi mapa mediante [OSC](https://www.notion.so/Un-recorrido-tecno-sonoro-hacia-el-habla-as-mica-31eaddcc2bb841bcaa124ed890ed8ca7?pvs=21) ya que dicho software provee una [API](https://www.notion.so/Un-recorrido-tecno-sonoro-hacia-el-habla-as-mica-31eaddcc2bb841bcaa124ed890ed8ca7?pvs=21) para ser [controlado con este protocolo](https://gitlab.com/ayrsd/audiostellar/-/blob/units/OSC_Documentation.md).

El mapa 2D de fragmentos de voces est√° categorizado en Clusters: conjunto de sonidos que pueden ser agrupados tanto por cercan√≠a (ajustable con ciertos par√°metros para definir sus caracter√≠sticas) o por la carpeta donde estos archivos residen. Dado que ten√≠a una carpeta por idioma, mis clusters estaban agrupados por idioma üëç. A su vez, est√°n desparramados por todo el mapa ya que la distribuci√≥n de cada punto -sonido- estaba dada por su *parecido t√≠mbrico* para determinar la cercan√≠a entre s√≠. Esto permit√≠a que tenga la posibilidad de recorrerlos con las unidades de AudioStellar de manera espacial, como hice antes, o bien, llamar por cluster mediante OSC.

```yaml
/play/cluster [clusterName] [[index]] [[volume]]
# Play a sound from a cluster named clusterName. If index is not present AudioStellar will choose a random one; note that the index will cycle through the number of sounds in the cluster. Volume is optional and is between [0,1].
```

Comenc√© a bocetar una aplicaci√≥n utilizando [Plug Data](https://plugdata.org/), una versi√≥n moderna de [Pure Data](https://puredata.info/) implementada con [JUCE](https://www.notion.so/Un-recorrido-tecno-sonoro-hacia-el-habla-as-mica-31eaddcc2bb841bcaa124ed890ed8ca7?pvs=21). La idea era enviar estos mensajes hacia AudioStellar para poder reproducir los sonidos de cada idioma con determinada frecuencia de disparo, de manera que pueda *simular habla* mezclada al concatenar los fragmentos de voces.

[https://drive.google.com/file/d/1r0gbmMcFY4lDBztvNtyfK41DhlXasLIc/view?usp=drive_link](https://drive.google.com/file/d/1r0gbmMcFY4lDBztvNtyfK41DhlXasLIc/view?usp=drive_link)

// continuar

## M√°quina Probabil√≠stica *(Experimento #32)*

En esta nueva iteraci√≥n sobre el primer experimento con TTS, consider√© algunos factores que favorecer√≠an al resultado sonoro que busqu√© desde el comienzo.

- Texto generado por ChatGPT en estilo nonsensical, ha demostrado ser bueno generando relatos sinsentido reminiscentes a alicia en el pa√≠s de las maravillas. Sint√°cticamente correcto, pero historias delirantes.
- genero de las voces, mismo texto, a dos voces. nueva CURADUR√çA T√çMBRICA DE VOCES.
- largo de los audios
- menor cantidad de word shuffling, sinsentido s√≥lo en el contenido, pero palabras reales.
- segmentos de habla m√°s largos en slicer audiostellar
- poner a prueba la maquina2 con estas nuevas condiciones
- observaciones.
- Para Guaran√≠ se encontr√≥ este modelo: [https://huggingface.co/facebook/mms-tts-grn](https://huggingface.co/facebook/mms-tts-grn) s√≥lo provee 1 voz de hombre.
- no inventa idiomas, genera conversaciones y multitudes.

[https://drive.google.com/file/d/1PmvJszNEmjLkqitroxBEGfrv-eUTSuGu/view?usp=drive_link](https://drive.google.com/file/d/1PmvJszNEmjLkqitroxBEGfrv-eUTSuGu/view?usp=drive_link)

HISTORIA SIEMPRE LA MISMA:

> *En una galaxia muy, muy lejana, los ping√ºinos bailan tango con sand√≠as y estrellas fugaces como luces de ne√≥n cubiertas de chocolate. El sol es como una gran naranja y la luna es como un gran algod√≥n de az√∫car. Los r√≠os fluyen con jarabe de arce y las nubes parecen pasteles de lim√≥n. Los peces saltan del agua y tocan el piano mientras las mariposas pintan dibujos en sus alas. Las carreteras est√°n cubiertas de chocolate derretido y las casas tienen techos hechos a medida. Las estrellas fugaces se convierten en caramelos de frambuesa y los mosquitos tocan el viol√≠n en mitad de la noche. Todo esto es parte de un dulce sue√±o donde las olas del mar est√°n hechas de salsa de caramelo y las estrellas brillan como caramelos de diamantes. De las nubes llueven macarrones y el atardecer es como un cuadro de chocolate. Es un mundo de imaginaci√≥n, donde los arco√≠ris son la escalera hacia los sue√±os celestiales y los atardeceres saben a mil helados diferentes. Los barcos flotan en los r√≠os con caramelo y las monta√±as se convierten en pasteles dulces.*
> 
- add video de cambios 2024

## Mi propia voz *(Experimento #33)*

A partir de los resultados sonoros de los experimentos anteriores, mi inquietud por el habla as√©mica crece a√∫n m√°s, como si esquivara cualquier forma de conclusi√≥n y ese universo se expandiera hacia un sinf√≠n de posibilidades. Me di cuenta que necesitaba emparejar los timbres de las voces si quer√≠a realmente comenzar a *inventar idiomas*. Pero, ¬øqu√© timbre? ¬øqu√© voz ser√≠a la que represente este nuevo lenguaje vocalizado por computadora? Por supuesto la respuesta apareci√≥ de inmediato, la m√≠a. De aqu√≠ surgen dos posibilidades:

- A partir de los fragmentos de audio del experimento anterior, encontrar alg√∫n sistema Speech-to-Speech que transforme esas voces generadas por TTSMaker a mi propia voz, de manera que el contenido *a-sem√°ntico* sea extra√≠do de esos audios, incluyendo la cadencia, la entonaci√≥n, las palabras y la pronunciaci√≥n, pero no el timbre, que ser√° el de mi voz.
- Clonar mi propia voz y luego con ese modelo inferir (generar audio a partir del modelo) los textos que hab√≠a generado como recurso fuente para los TTS.

De cualquiera de estas maneras lograr√≠a obtener muchos fragmentos de la misma persona (yo) hablando una gran cantidad de idiomas con la posibilidad de mezclarlos de forma temporal pero no morfol√≥gica. Esta distinci√≥n es importante para aclarar que estos √∫ltimos 3 experimentos pueden dar un acercamiento a esta invenci√≥n de idiomas con una operaci√≥n 

Finalmente decid√≠ pagar ElevenLabs‚Ä¶ 

COMPLETAR

Tambi√©n tom√© la decisi√≥n de quedarme con la [fantasiosa historia](https://www.notion.so/Un-recorrido-tecno-sonoro-hacia-el-habla-as-mica-31eaddcc2bb841bcaa124ed890ed8ca7?pvs=21) inventada por ChatGPT. Cada una de las versiones no fue traducida sino que le ped√≠a nuevamente el texto en el idioma que quisiera luego reproducir con mi voz clonada. En los experimentos anteriores hab√≠a textos de otros generadores de *gibberish* pero para este caso quer√≠a mantener este sinsentido en el relato, d√°ndole m√°s sentido a este gesto. Este relato fue pedido en los 29 idiomas que proporciona el modelo **Eleven Multilingual v2** (√°rabe, b√∫lgaro, chino, croata, checo, dan√©s, holand√©s, ingl√©s, filipino, finland√©s, franc√©s, alem√°n, griego, hindi, indonesio, italiano, japon√©s, coreano, malayo, polaco, portugu√©s, rumano, ruso, eslovaco, espa√±ol sueco, tamil, turco, ucraniano).

Settings del modelo: **Stability** entre 35% y 50%, **Similarity** entre 75% y 90%, **Style Exaggeration** 0% (porque no afecta en modo TTS, s√≠ en modo STS), **Speaker Boost** encendido.

## Las voces de mi gente

COMPLETAR clone la voz de amigues. 

clonar voz 2x de wsp? 

clonar la voz de mi viejo

## Generador de subt√≠tulos en tiempo real

[Subtitle generator](https://codepen.io/jaunmatrin/pen/gOqgmgx)

[https://codepen.io/jaunmatrin/pen/gOqgmgx](https://codepen.io/jaunmatrin/pen/gOqgmgx)

# An√°lisis del algoritmo cart√≥grafo

[Exploraci√≥n manual en AudioStellar, evaluando algoritmo MFCC.](https://drive.google.com/file/d/1QwicLDBVO5t_YAx4S1HWOblirh6Jc4eh/view?usp=drive_link)

Exploraci√≥n manual en AudioStellar, evaluando algoritmo MFCC.

// qu√© onda la similitud de idiomas en el mapa? tener en cuenta que este se explor√≥ en modo mapa, en los experimentos anteriores el mapa casi que ni importaba porque se exploraba por OSC por idioma.

# Uso no convencional de las herramientas

ttsmaker + gibberish generators / tacotron / el autoencoder?

# El valor est√©tico del habla

La experiencia de escuchar hablar idiomas que no conocemos deja autom√°ticamente el contenido sem√°ntico fuera de la ecuaci√≥n. Esto hace que no podamos considerar otra cosa que su sonoridad. Prestaremos atenci√≥n a lo extra√±o, a la emocionalidad, nuestras diferencias, al timbre de la voz, a los sonidos que esta genera, qu√© hace ese aparato fonador para generar esos sonidos, casi como un instrumento musical.

// agregar aspecto sensible

# C√≥mo comunico mi hacer

As√≠ como noto que hay idiomas entre idiomas, observo la distancia entre las *matem√°gicas* complejas y los resultados sonoros, las matrices y m√©todos de clases de librer√≠as y los espectrogramas generados. ¬øD√≥nde me encuentro? ¬øes acaso la intersecci√≥n perfecta entre las ganas de hacer ruido y la man√≠a del ni√±x que quiere desarmar y armar de nuevo un aparato a su manera? aqu√≠ me encuentro como artista investigador en la intersecci√≥n del arte y la ciencia, de la b√∫squeda est√©tica y la t√©cnica.

# Ense√±arle a hablar a una computadora

Trabajar con -para m√≠- nuevas tecnolog√≠as, en este caso de inteligencia artificial, desarrollar y entrenar modelos de redes neuronales, requiere de un entendimiento total, hol√≠stico, acerca de c√≥mo funciona la arquitectura y todos los componentes del sistema. En otros casos, para otro tipo de desarrollos, he notado que no hace falta tener el conocimiento completo acerca de las tecnolog√≠as en cuesti√≥n. Por ejemplo, a la hora de realizar un frontend en Javascript, uno no necesita entender qu√© sucede en la memoria de la computadora, o qu√© partes corren dentro de la GPU o del CPU, simplemente le pedimos un bot√≥n que al ser presionado dar√° resultado a determinado comportamiento o acci√≥n. Esta abstracci√≥n de alto nivel permite concentrarse en una idea m√°s general, m√°s elevada, m√°s af√≠n a la creaci√≥n y a la experiencia.

Mi trabajo pretende proponer una experiencia, es decir una situaci√≥n en la que se interpelen los sentidos y as√≠ poder compartir *sensaciones*. Verme involucrado en tareas de bajo nivel para resolver un problema matem√°tico gener√≥ que me sienta alejado de la situaci√≥n que deseo presentar. El trabajo con estos modelos no siempre resulta en un *error* de sintaxis que detiene el programa sino con resultados v√°lidos pero *sem√°nticamente incorrectos.* Esto me trae a la cuesti√≥n del sentido como met√°fora del hacer. 

# Devenires auxiliares

## gla 1.0b ‚ÄìMenci√≥n CCEBA‚Äì

[https://drive.google.com/file/d/12xkWZWs5wQlyENe7-0yCX_c96YhYxh2e/view?usp=sharing](https://drive.google.com/file/d/12xkWZWs5wQlyENe7-0yCX_c96YhYxh2e/view?usp=sharing)

## Qu√© dice? ‚ÄîTTS‚Äî

pruebas tacotron uberduck 

## Subtitle Generator

[https://codepen.io/jaunmatrin/pen/gOqgmgx](https://codepen.io/jaunmatrin/pen/gOqgmgx)

## Salida

No concluye, sino genera devenires.

---

# Glosario

A continuaci√≥n, algunas definiciones espec√≠ficas de las cuestiones t√©cnicas abordadas en este trabajo.

## As√©mico

Sin sentido, que no provee contenido dentro del mensaje. Se utiliza especialmente al referirse al uso de canales de comunicaci√≥n gr√°ficos o sonoros ‚Äìverbales‚Äì que expresan ideas sino que se centran en el aspecto formal del mismo.

## TTS

Sistemas de conversi√≥n de texto a habla, por sus siglas en ingles: Text-to-Speech.

- capaz no hace falta porque est√° explicado
    
    ## Modelo
    
    Modelo de red neuronal creado con tecnolog√≠as de inteligencia artificial, machine learning y deep learning. Su funcionamiento es similar a las redes neuronales biol√≥gicas, pero expresadas en forma de un modelo matem√°tico que contiene nodos (que contienen un valor y aristas (con peso).
    
    $$
    c_i = \sum_{j=1}^{T_x} \alpha_{ij}h_j
    \alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}
    e_{ij} = a(s_{i-1}, h_j)
    $$
    
    ## Dataset
    
    Conjunto de informaci√≥n recopilada y ordenada de manera tal que pueda ser alimentada a un modelo de inteligencia artificial y este aprenda a partir de la misma para luego generalizar la informaci√≥n y *comprenderla*. As√≠ poder inferir nuevos resultados.
    
    ## Espacio Latente
    

## Fine-Tuning

Es el proceso de ajustar un modelo de inteligencia artificial pre-entrenado en un conjunto de datos espec√≠fico y relacionado para adaptarlo a una tarea espec√≠fica, generalmente mediante el ajuste de los pesos de las capas del modelo.

## Inferencia

Se refiere al proceso de utilizar un modelo de inteligencia artificial entrenado para realizar predicciones o generar resultados a partir de nuevos datos de entrada, una vez que el modelo ha sido entrenado y ajustado.

## Transientes

Son los sonidos que ocurren brevemente durante una se√±al de audio, como los clics, los golpes o los ruidos repentinos, que tienen una duraci√≥n corta en comparaci√≥n con la se√±al total.

## OSC

Es un protocolo de comunicaci√≥n para la transmisi√≥n de mensajes de control entre dispositivos y software de audio en tiempo real, utilizado para la comunicaci√≥n y el control en aplicaciones musicales, de arte sonoro y multimedia. Apareci√≥ como una alternativa moderna con mayor flexibilidad y resoluci√≥n que el protocolo MIDI.

## API

Es un conjunto de reglas y protocolos que permiten que diferentes piezas de software se comuniquen entre s√≠, facilitando la interacci√≥n y el intercambio de datos entre aplicaciones y sistemas inform√°ticos de manera estandarizada y program√°tica.

# Bibliograf√≠a

traer de zotero

Akten, M. (n.d.). *Grannma MagNet ‚Äì Granular Neural Music & Audio with Magnitude Networks (2018) ‚Äì Memo Akten | Mehmet Selim Akten | The Mega Super Awesome Visuals Company*. Retrieved 19 June 2023, from https://www.memo.tv/works/grannma-magnet/

*Archivo P.A.I.S.: Cuatro hip√≥tesis de trabajo ‚Äì nicol√°s varchausky*. (n.d.). Retrieved 21 June 2023, from https://www.varchausky.com.ar/archivo-pais/archivo-p-a-i-s-cuatro-hipotesis-de-trabajo/

Bengio, Y., Courville, A., & Vincent, P. (2013). Representation Learning: A Review and New Perspectives. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, *35*(8), 1798‚Äì1828. https://doi.org/10.1109/TPAMI.2013.50

Chomsky, N. (2002). *Syntactic Structures*. Mouton de Gruyter. https://books.google.com.ar/books?id=a6a\_b-CXYAkC

*Dadabots*. (n.d.). Retrieved 21 June 2023, from https://dadabots.com/science.php

Deleuze, G., Guattari, F., & V√°zquez, J. (2020). *Mil mesetas: Capitalismo y esquizofrenia*. Editorial Pre-Textos. https://books.google.com.ar/books?id=k1d9zgEACAAJ

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., & Bengio, Y. (2014). Generative Adversarial Nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, & K. Q. Weinberger (Eds.), *Advances in Neural Information Processing Systems* (Vol. 27). Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf

Kingma, D. P., & Welling, M. (2022). *Auto-Encoding Variational Bayes*.

Ladefoged, P., & Maddieson, I. (1996). *The Sounds of the World‚Äôs Languages*. Wiley. https://books.google.com.ar/books?id=h1byJz\_rWUcC

Oord, A. van den, Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A. W., & Kavukcuoglu, K. (2016). WaveNet: A Generative Model for Raw Audio. *CoRR*, *abs/1609.03499*. http://arxiv.org/abs/1609.03499

Papamakarios, G., Nalisnick, E., Rezende, D. J., Mohamed, S., & Lakshminarayanan, B. (2021). *Normalizing Flows for Probabilistic Modeling and Inference*.

Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). *High-Resolution Image Synthesis with Latent Diffusion Models*.

Varchausky, N. (n.d.). *Archivo P.A.I.S.* Retrieved 21 June 2023, from http://archivopais.org/

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., & Polosukhin, I. (2017). Attention is All you Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, & R. Garnett (Eds.), *Advances in Neural Information Processing Systems* (Vol. 30). Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf

Wekinator. (n.d.). *Wekinator | Software for real-time, interactive machine learning*. Retrieved 21 June 2023, from http://www.wekinator.org/

Elliott, R., & Bull, M. (2017). *The Sound of Nonsense*. Bloomsbury Academic. [https://books.google.com.ar/books?id=4rU7DwAAQBAJ](https://books.google.com.ar/books?id=4rU7DwAAQBAJ)