# Experimentos

Para generar un software de habla que no diga nada necesitaba realizar un desarrollo que logre generar sonidos en tiempo real, similar a un instrumento, pero de habla. El puntapi√© para el comienzo del desarrollo fue la b√∫squeda de modelos de machine learning que puedan generalizar la voz y devolvernos sonidos, similar a lo que hace un sistema TTS, con la diferencia de que yo no quer√≠a darle contenido sem√°ntico, y tampoco deb√≠a hablar correctamente, es decir, hacer sentido en su dicci√≥n. Solamente buscaba que suene como si hablara, pero que no haya palabras reconocibles por dilucidar en lo que se dice.

A lo largo de la investigaci√≥n y el desarrollo, colabor√© principalmente utilizando la versi√≥n gratuita de [ChatGPT](https://chat.openai.com/), desarrollado por [OpenAI](https://openai.com/), que en este momento es el modelo gpt-3.5-turbo. Est√° entrenado con informaci√≥n recopilada de todo internet hasta septiembre de 2021. Comprend√≠ que si se le da contexto y se le define un rol responde de manera muy precisa. Especialmente en tareas peque√±as y espec√≠ficas, pero tambi√©n como puntapi√© inicial hacia las sub-investigaciones que fui llevando a cabo a lo largo del proceso.

Dada la limitaci√≥n del modelo gratuito de OpenAI, tambi√©n recurr√≠ a otros LLMs como [Bing Chat](https://www.bing.com/), de [Microsoft](https://www.microsoft.com/), que cuenta con la habilidad de buscar en internet, de hecho, es un buscador similar a [Google](https://www.google.com/), pero con _quien_ uno puede establecer una conversaci√≥n donde afinar sus resultados. El gigante de internet Google tambi√©n cuenta con [Bard](https://bard.google.com/chat), pero de momento se encuentra en una etapa experimental, muy revisada por humanos y con poco soporte en idiomas que no sean ingl√©s. Por √∫ltimo prob√© [LLaVA](https://llava.hliu.cc/): Large Language and Vision Assistant, que si bien promete y es de c√≥digo abierto, no cuenta con conversaciones que pueda almacenar en hilos de contexto como s√≠ lo hace ChatGPT, teniendo que iniciar nuevas conversaciones en cada sesi√≥n de uso.

A continuaci√≥n relatar√© los experimentos que realic√© en funci√≥n al objetivo del habla as√©mica. Si bien dejar√© las definiciones t√©cnicas para el final, el relato se ver√° entrelazado con cuestiones t√©cnicas propias del desarrollo.

## Interpolaci√≥n de lenguas

Leandro Garber me recomend√≥ que un buen experimento inicial es entrenar un modelo con la arquitectura Autoencoder, en particular uno desarrollado por Pablo Riera. Al ser alimentado con audio de idiomas hablados, tuvimos la hip√≥tesis de que este podr√≠a generalizar lo suficiente sus caracter√≠sticas como para luego generar nuevos resultados sonoros sin necesidad de contenido sem√°ntico, qued√°ndose s√≥lo con la prosodia de la lengua.

El desarrollo de Riera no s√≥lo consiste en un modelo que puede ser entrenado con archivos de audio, sino que cuenta adem√°s con una especie de sintetizador de inferencia de audio a partir del modelo que se le cargue. Un modelo es en esencia un archivo en la computadora que este software puede cargar y ofrecer en su interfaz una forma de interactuar con su espacio latente: esa abstracci√≥n de pocas dimensiones que permite manipular esa generalizaci√≥n de lo que entiende por idioma hablado en forma de sonido.

Para entrenar el modelo necesit√© un dataset o conjunto de datos preparados o formateados de forma precisa. El primero con el que trabaj√© fue [common_language](https://huggingface.co/datasets/common_language). Est√° compuesto por grabaciones de voz de idiomas seleccionados cuidadosamente de la base de datos [Mozilla CommonVoice](https://commonvoice.mozilla.org/). La duraci√≥n total de las grabaciones de audio es de 45.1 horas (1 hora de material para cada idioma). Est√° destinado a entrenar sistemas de identificaci√≥n de idiomas.

A continuaci√≥n algunos ejemplos de los archivos incluidos en el dataset. Son archivos `.wav`, es decir que no tienen p√©rdida de calidad:

<audio controls src="_media/common_voice_fr_19598591.wav" title="common_voice_fr_19598591.wav"></audio>
<audio controls src="_media/common_voice_fr_19598240.wav" title="common_voice_fr_19598240.wav"></audio>
<audio controls src="_media/common_voice_fr_19140964.wav" title="common_voice_fr_19140964.wav"></audio>

N√≥tese que los fragmentos est√°n dichos por diferentes personas, con distintas calidades y condiciones de grabaci√≥n. Esto se debe a que es necesaria esta variabilidad para enriquecer el modelo, que logre generalizar el habla y la voz humana, pero a ninguna voz en particular.

Para entrenar el Autoencoder deb√≠a alimentarlo con un s√≥lo archivo de audio. El dataset cuenta con una carpeta por idioma con peque√±os fragmentos, por lo que con el siguiente comando en la terminal de la computadora logr√© concatenar inmediatamente todos los archivos de una carpeta en un archivo de audio √∫nico:

```bash
# Los archivos est√°n nombrados de la siguiente forma:
# common_voice_<lang>_<n√∫mero>.wav
# Por ejemplo: common_voice_fr_19140964.wav
$ sox common_* frances.wav
# Obtengo `frances.wav` como resultado.
```

`sox` es una utilidad para manipular audio desde la consola, funciona en macOS, Linux y Windows. De esa forma evit√© el trabajo manual de concatenar todos los archivos.

<audio controls src="_media/french.mp3" title="Archivo de audio mp3 61.8kbps VBR para fines demostrativos, pero se utiliz√≥ el wav para entrenar el modelo."></audio>

<p class="caption">Archivo de audio mp3 61.8kbps VBR para fines demostrativos, pero se utiliz√≥ el wav para entrenar el modelo.</p>

Lo mismo pero para el Espa√±ol:

<audio controls src="_media/common_voice_es_18681301.wav" title="common_voice_es_18681301.wav"></audio>
<audio controls src="_media/common_voice_es_19125377.wav" title="common_voice_es_19125377.wav"></audio>
<audio controls src="_media/common_voice_es_19602100.wav" title="common_voice_es_19602100.wav"></audio>

Y su versi√≥n concatenada:

<audio controls src="_media/spanish.mp3" title="Archivo convertido a mp3 con ffmpeg a 61.7kbps VBR"></audio>

<p class="caption">Archivo convertido a mp3 con ffmpeg a 61.7kbps VBR</p>

Para entrenar el modelo prepar√© un Colab con el c√≥digo a ejecutar y un entorno donde ejecutarlo, que pod√≠a ser la nube (Google) o mi computadora. Luego de realizar unos _benchmarks_ -pruebas de rendimiento- observ√© que mi computadora ser√≠a un poco m√°s r√°pida que los recursos que ofrece el _free tier_ de Google Colab. As√≠ que instal√© Anaconda para montar un entorno con la versi√≥n requerida de Python y todas las dependencias requeridas para poder ejecutar mi propio Colab, incluso la configuraci√≥n de seguridad y redes para poder acceder remotamente desde cualquier lado, como si tuviera mi propia nube en mi casa, manteniendo la soberan√≠a de mi sistema computacional.

Realic√© el entrenamiento con los archivos de audio concatenados que arm√© en la etapa del dataset. Esto result√≥ en modelos _uni-lengua_ ya que s√≥lo podr√≠an generalizar el habla en una sola lengua. Por esto fue que el siguiente paso fue realizar un proceso de entrenamiento con dos lenguas que, como explicar√© [m√°s adelante](https://www.notion.so/Un-recorrido-tecno-sonoro-hacia-el-habla-as-mica-31eaddcc2bb841bcaa124ed890ed8ca7?pvs=21), podr√≠a generar una cruza interesante. Concaten√© una conversaci√≥n en Franc√©s con otra en Japon√©s como dataset para luego generar un modelo _multi-lengua_ o en este primer caso biling√ºe.

Ahora bien, luego de m√∫ltiples intentos de que el proceso de entrenamiento funcione, sumado a las horas que este proceso demora, obtuve mi archivo de idioma. ¬øUn archivo que representa un idioma? ¬øSer√≠a as√≠ de simple? ¬øEn unas horas la computadora sabr√≠a lo suficiente para sonar exactamente como suena una persona hablando Franc√©s?

El proceso de entrenamiento que utilic√© incluye la posibilidad de obtener el audio de entrada al entrenamiento como un resultado reconstruido por el mismo modelo, de manera que pod√≠a r√°pidamente saber si este al menos podr√≠a recrear lo mismo con lo que fue entrenado. Esto es una forma de hacer inferencia.

Dado este audio:

<audio controls src="_media/common_voice_es_19609042.wav" title="Audio original alimentado a la red"></audio>

Se reconstruy√≥ el siguiente audio directamente desde el c√≥digo. Para eso se insertan en el espacio latente del modelo los mismos valores que devuelve el entrenamiento. Resultando en:

<audio controls src="_media/reconstructed.mp3" title="Audio reconstruido"></audio>

![Espectrograma del resultado reconstruido.](_media/predicted_spectrogram.png "Espectrograma del resultado reconstruido.")

![Camino del espacio latente de 4 dimensiones que genera el audio reconstruido.](_media/Z_latent_score.png "Camino del espacio latente de 4 dimensiones que genera el audio reconstruido.")

Al solicitarle [inferencia](https://www.notion.so/Un-recorrido-tecno-sonoro-hacia-el-habla-as-mica-31eaddcc2bb841bcaa124ed890ed8ca7?pvs=21) al modelo para generar nuevos sonidos a partir del entrenamiento con los mostrados anteriormente, los resultados no eran muy favorables para mi investigaci√≥n por su alto nivel de ruido, por lo que decid√≠ buscar videos en YouTube de Podcasts o conversaciones para armar un nuevo dataset. Encontr√© voces limpias de gente con buena calidad de grabaci√≥n, sent√≠ que con estos el modelo podr√≠a generalizar voces m√°s n√≠tidas.

- Podcast en franc√©s. mp3 55.7kbps VBR para fines demostrativos:

  <audio controls src="_media/french-clean.mp3" title="Audio extra√≠do de https://www.youtube.com/watch?v=N55d9mPm_HA"></audio>

  <p class="caption">Audio extra√≠do de https://www.youtube.com/watch?v=N55d9mPm_HA</p>

- Japon√©s:
  
  <audio controls src="_media/japanese-clean.mp3" title="Title"></audio>

  <p class="caption">Audio extra√≠do de https://www.youtube.com/watch?v=trySjnuJfJY</p>

### Espacio Latente

La idea es explorar este espacio para encontrar nuevos [_caminos_](https://www.notion.so/Un-recorrido-tecno-sonoro-hacia-el-habla-as-mica-31eaddcc2bb841bcaa124ed890ed8ca7?pvs=21) que extraigan las caracter√≠sticas principales del sonido con el que fue entrenado el modelo. Con caminos me refiero a datos en serie de tantas dimensiones como tenga el espacio latente ‚Äìn√∫meros‚Äì que representan esta exploraci√≥n, y al ser decodificados generan nueva informaci√≥n con las caracter√≠sticas principales de sus datos de entrada.

En principio, junto con Leandro Garber, pensamos en enviar datos aleatorios de caminos posibles para comenzar a ver qu√© hab√≠a en ese espacio latente. Para eso armamos un peque√±o patch en Pure Data para enviar los valores en tiempo real al modelo y que este genere audio a partir de los datos de entrada.

El [**2ASMRSynth**](https://github.com/pabloriera/2ASMRS) tambi√©n programado por Pablo Riera es el sintetizador basado en [JUCE](https://juce.com/) para generar inferencia sonora en tiempo real a partir de un modelo de tipo [torchscript](https://pytorch.org/docs/stable/jit.html). Como expliqu√© anteriormente, uno carga el modelo en el sintetizador y puede mover los par√°metros dentro del espacio latente y escucharlo en tiempo real, como un instrumento. La sonoridad de este instrumento depender√° del modelo cargado.

![2ASMRSynth Sintetizador de inferencia de modelos](_media/2ASMRSynth.png "Captura de pantalla del 2ASMRSynth corriendo en Linux Mint.")

Cada _fader_ del instrumento representa una de las dimensiones del espacio latente, y al moverlos le estamos asignando valores a cada una. La parte del decodificador del modelo genera la salida de audio. Mover estos valores manualmente, por ejemplo con el mouse, no resulta interesante ya que se debe orquestar un movimiento coordinado de todos los par√°metros para que la salida haga _sentido_. Por supuesto que cuenta con la posibilidad de ser controlado mediante el protocolo MIDI.

Con el modelo [multi-lengua](https://www.notion.so/Un-recorrido-tecno-sonoro-hacia-el-habla-as-mica-31eaddcc2bb841bcaa124ed890ed8ca7?pvs=21) la idea fue alimentar las dimensiones del espacio latente con los datos de entrenamiento de un modelo entrenado en Franc√©s anteriormente. La pregunta era ¬øqu√© pasar√≠a si a un modelo biling√ºe le hago _hablar_ uno de los idiomas que conoce?

<audio controls src="_media/JF-MODEL_japanese-fantasmagorico.mp3" title="JF-MODEL_japanese-fantasmagorico"></audio>

<p class="caption">JF-MODEL_japanese-fantasmagorico.mp3</p>

Este audio es el resultado de alimentar el modelo biling√ºe con los datos de entrenamiento que representan los movimientos dentro del espacio latente para segmento de habla en Franc√©s.

Las siguientes pruebas consisten en cargar los modelos entrenados en el sintetizador _2ASMRSynth_ y luego alimentar los par√°metros del modelo (n√≥tese el err√°tico movimiento de los faders) con una lista de cada valor en el tiempo. En el siguiente ejemplo son 8 listas, una para cada dimensi√≥n del espacio latente representada en los faders.

<video width="100%" controls src="_media/French8dim.mp4" title="Title"></video>

<p class="caption center">Franc√©s 8 dimensiones</p>

Para lograr esto se desarroll√≥ un patch de Pure Data con el que se cargan los valores num√©ricos de un archivo de texto (generado durante la etapa de entrenamiento) y se _mapean_ o traducen a controles MIDI (rango de 0-127).

<video width="100%" controls src="_media/French4dim_drunkYlatent.mp4" title="Title"></video>

<p class="caption center">Franc√©s 4 dimensiones recorrido con algoritmo 'drunk'</p>

Esta herramienta que posibilita la ejecuci√≥n en tiempo real del modelo trae un problema inherente de su m√©todo de control: el protocolo MIDI est√° limitado a valores de 0-127. Esta resoluci√≥n no es lo suficientemente precisa para _expresar_ la voz en cada una de sus variables internas del espacio latente. Como resultado sonoro aparecen sonidos similares a una voz cuyos movimientos son reminiscentes a algo artificial, quiz√°s medio rob√≥tico, con saltos abruptos en su calidad vocal _con_ <em>con</em>sonantes <em>con</em>stantes. La caracter√≠stica fantasmag√≥rica, a su vez, est√° producida por el problema de la aplicaci√≥n de una fase aleatoria en el audio. El asunto de la fase es un cl√°sico problema en la generaci√≥n de audio por IA que explicar√© posteriormente en los Conceptos T√©cnicos.

## Separaci√≥n en fonemas/fragmentos

Luego de haber aprendido lo suficiente acerca del workflow y las tecnicalidades de la tecnolog√≠a de inteligencia artificial con la que encar√© el proyecto (el modelo Autoencoder de Riera y su sintetizador compa√±ero int√©rprete), llegu√© a resultados que no eran exactamente los que esperaba. Por un momento me detuve a contemplar la diferencia entre lo que esperaba y lo que suced√≠a, ¬øfr*acaso acaso*?. Empec√© a dudar de si lo que hab√≠a pensado ‚Äìcrear un sinsentido a partir de un modelo l√≥gico‚Äì ten√≠a sentido, pero mi instinto me obligaba a probarlo para poder llegar a esta conclusi√≥n.

Inmediatamente, as√≠ como fabul√© este desarrollo y sus resultados, comenc√© a hilar un plan B m√°s elemental, uno que quiz√°s tendr√≠a que haber sido la primera prueba, pero mi predicci√≥n acerca del resultado sonoro era que no iba a ser muy preciso, o escalable, o lo que en mi imaginario buscaba. Esta alternativa consiste en separar grabaciones de habla en fragmentos de unidades b√°sicas que la componen. Un **fonema** es la unidad b√°sica de sonido en un idioma o sistema de comunicaci√≥n. Representa un sonido distintivo que puede cambiar el significado de una palabra en ese idioma, como sabemos a partir del estudio de la fonolog√≠a. Pueden variar de un idioma a otro y a menudo se representan con s√≠mbolos fon√©ticos en la ling√º√≠stica para describir c√≥mo se pronuncian en el habla _(ej. /u/, /s/)_.

Estos recortes vocales, audios segmentados en fonemas, o peque√±as porciones, ser√≠an importados en AudioStellar, lo cual brinda dos resultados/piezas que me interesa observar. Por un lado, un mapa de sonidos que quiz√°s est√©n re-ordenados por sus cualidades sonoras y no por grupos de idiomas, trazando una cartograf√≠a diferente en el espacio sonoro que en el geogr√°fico. Adem√°s y principalmente, la capacidad de recorrer estos fonemas de las diversas formas que ofrece AudioStellar, haciendo que estos suenen de manera concatenada, por lo que formar√≠an habla.

La distribuci√≥n de los fonemas en el mapa 2D de AudioStellar es lo que define esta experiencia. He aqu√≠ el _trade-off_: una cartograf√≠a diferente a la geopol√≠tica de las lenguas versus la interpolaci√≥n de idiomas al recorrer los bordes entre grupos de fonemas. Paso a explicar:

- Si los fonemas se agrupan geopol√≠ticamente, ie. por idioma, ser√° posible recorrer un idioma espec√≠fico a trav√©s de sus fonemas, sin hacer sentido, pero utilizando contenido -sonidos- propios de un s√≥lo idioma. Esto nos da la ventaja de tambi√©n poder escuchar qu√© hay en los bordes de cada grupo de fonemas e interpolar entre idiomas cercanos.
- Si se agrupan por sus caracter√≠sticas sonoras, se podr√°n contemplar fonemas similares de los distintos idiomas, se barren las barreras geopol√≠ticas, trazando un mapa donde el sonido hace nuestro sentido (ref sound of nonsense), es nuestra fuente de verdad y no vamos a interpolar entre idiomas sino entre sonidos de diferentes or√≠genes, agrupados por sus cualidades similares. As√≠ encontramos una nueva [_Cartograf√≠a Mundial del Habla_](https://www.notion.so/Un-recorrido-tecno-sonoro-hacia-el-habla-as-mica-31eaddcc2bb841bcaa124ed890ed8ca7?pvs=21).

La entrevista con Mateo Amaral me hizo pensar en el uso no convencional de las herramientas, en este caso tecnolog√≠as de generaci√≥n de habla como los TTS, como un recurso propio de nuestro proceso creativo como artistas trabajando en piezas que luego componen obras. A lo largo del desarrollo de estos experimentos que se describen a continuaci√≥n, los distintos pasos me hacen pensar en _trucos_ que uso para perseguir mi determinada b√∫squeda est√©tica.

Al llegar a [TTSmaker.com](http://ttsmaker.com/) y gracias a la charla con Mateo, la idea de no involucrar texto me pareci√≥ caduca. Haciendo pruebas con todas las herramientas online que encontr√©, ide√© un flujo que me pareci√≥ interesante en cuanto a proceso y sus resultados. Escribir sus instrucciones como _performance inform√°tica_ es un nuevo devenir de este proceso que describo a continuaci√≥n. Invito y celebro su interpretaci√≥n y modificaci√≥n.

### Instrucciones para un experimento TTS _(Experimento #31)_

1. Generar texto aleatorio en [RandomTextGenerator](https://www.randomtextgenerator.com/) en alg√∫n idioma.

    a. Si no est√° el idioma, [DeepL](https://www.deepl.com/translator) o bien [Google Translate](https://translate.google.com/) para traducir al idioma deseado.

    b. Revisar el texto generado utilizando un traductor como el mencionado, para ver su contenido. En lo posible que no repita tantas palabras, ni que haga sentido. Si hay coherencia en las oraciones pero el contenido del mensaje es surreal, vale para los prop√≥sitos de esta investigaci√≥n.

2. Pasar por [Gibberish Generator](https://thinkzone.wlonk.com/Gibber/GibGen.htm)

    a. Revisar Gibberish Level. Probar en 3. Si el texto no parece muy coherente, o est√° muy mezclado (juzgado a partir de una noci√≥n del idioma) o el resultado sonoro ‚Äúno coincide tanto con el imaginario de esa lengua‚Äù subir un punto.

3. Generar habla con [TTSMaker](https://ttsmaker.com/)

    a. Probar distintas voces

    b. Intentar que el audio resultante est√© entre 30 y 40 segundos

4. La estructura de carpetas para la organizaci√≥n de sonidos resultantes podr√≠a ser la siguiente:

  ![Estructura de carpeta TTS](_media/tts-folder.png "Estructura de carpeta TTS")

  O bien generalizando:

    ```yaml
    <Carpeta de un idioma>
    - Textos.txt # Documento de texto que contiene el texto que es reproducido por el TTS.
    - ttsmaker-file-<fecha> <n√∫mero>.wav # Donde n√∫mero corresponde al p√°rrafo dentro del archivo de texto.
    + <Otra carpeta de otro idioma>
    - Textos.txt
    - ttsmaker-file-<fecha> <n√∫mero>.wav
    ...
    # Tantos como idiomas se deseen agregar.
    ```

5. Un ejemplo del contenido de los archivos de texto es:

  ```txt
  1
  De se so soubemos matada pentadessem. Alimindo comedefez os mas obederraderetado se muda no te se. Cum ques emponge janter. Cartessa esta notimido ho qua te conros te confuncido filigas tu vidos as be. Aponge as manda noivem em te ha olvempesfacominar mente mando. Carra endos ha. Alimpros has manar as em. Apos manteis avellam expleto se cape. Sei irrana dosa pitadadaveo nosa noivo. De mausoube. Papantao. Bar larra cape. La pendivido do imeta seguessemo procebo erra em erradomina neliguietticahindo

  2
  Ja sr litterato tolica patuscadas tratado por sujeitava. Vintens nervoso um fio. Entribue aos mal. Caminhar arreligioso vao rez inda vao religioso vao rez acces levantado ser que estincoes le fez accento tem nos affligioso dor sujeitae. Esmeros tento causara identes. Fazendo pe. Caminhava rajadas um oh fechava. Nos affligir symbolica documentes. Fazenda vao esmeros teu rez accesso um oh fechava. Retento por evitae. Esmerossas trando paiol vereram tal esta vae evidento companto tolica documento t
  ```

6. Recortar archivos de audio con AudioStellar o `VOSK-phoneme-slicer.py` para generar segmentos peque√±os, uno por cada fonema.

   ‚ÑπÔ∏è **NOTA:** En este caso, utilic√© exclusivamente AudioStellar para esta tarea dado que este separa a partir de transientes. Esta estrategia √∫nica para todos los idiomas permite cierta homogeneidad de c√≥mo los analizamos, sin considerar reglas espec√≠ficas de cada uno. Caso contrario, se deber√≠a conseguir un modelo entrenado por cada lengua ‚Äìo desarrollarlo‚Äì para realizar una separaci√≥n de fonemas espec√≠fica debido a que no todos tienen las mismas reglas fon√©ticas. Esto complejiza y alarga bastante la tarea y no aporta considerablemente al resultado final sonoro. Arm√© un nuevo script para utilizar los m√©todos que AudioStellar utiliza en su c√≥digo para realizar los recortes. Para esto Leandro me indic√≥ en qu√© parte del c√≥digo de ASt se encuentran estas funcionalidades y tom√© lo que necesitaba para armar el script (ver c√≥digo en Ap√©ndice).

  De esta forma, con la estructura de carpetas del paso anterior, se generar√≠an las _tajadas_ de cada idioma en lote, de forma program√°tica, en cuesti√≥n de minutos. S√≥lo se debe especificar los siguientes par√°metros en el script, que son los mismos que ofrece la interfaz de AudioStellar:

  ```python
  input_folder = dir # Directorio contenedor de las carpetas de idiomas
  window_max = 0.10
  window_avg = 0.05
  delta = 0.10
  backtrack = True
  fade = 1000
  normalize = True
  ```

  Para mayor informaci√≥n, revisar la [documentaci√≥n de `librosa`](https://librosa.org/doc/main/generated/librosa.onset.onset_detect.html#librosa.onset.onset_detect).

7. Crear mapa en AudioStellar utilizando MFCC, algoritmo que ignora caracter√≠sticas de pitch ‚Äìideal para voces de diferentes personas‚Äì pero conserva caracter√≠sticas t√≠mbricas ‚Äìprosodia‚Äì de manera que agrupe los fonemas sin importar quien lo diga. MFCC es el algoritmo m√°s cercano a una subjetividad perceptiva del sonido.

8. üí° Experimentar.

?> Este experimento es altamente escalable, es decir que cuanto m√°s largos, o m√°s archivos generados por el TTS se utilicen, habr√° mayor riqueza de fonemas, m√°s contenido por idioma. La cantidad de lenguas, y cu√°les, depender√° de la b√∫squeda sonora del experimento.

<video id="ast-explorer" width="100%" controls src="_media/AudioStellar exploracion manual.mp4" title="Exploraci√≥n manual con units de AudioStellar"></video>

<p class="caption center">Exploraci√≥n manual con units de AudioStellar.</p>

#### Consideraci√≥n sobre idiomas con ideogramas

El Chino, por ejemplo, est√° compuesto por ideogramas, por lo que mezclar sus caracteres no hace mucho sentido. Es decir que no podemos inventar, al menos con este tipo de herramientas, palabras que no existan a la hora de ser pronunciadas. Quiz√°s la oraci√≥n no tenga sentido pero las palabras que la componen s√≠, y esto es diferente a los otros casos dado el algoritmo utilizado en la herramienta que convierte texto a gibberish (ver c√≥digo fuente en el mismo [sitio web](https://thinkzone.wlonk.com/Gibber/GibGen.htm)).
Una estrategia para mezclar el texto y evitar sonidos repetidos es borrar caracteres aleatorios en el texto a reproducir, por lo que se sugiere generar uno largo al cual se le puedan eliminar fragmentos hasta lograr el efecto y la duraci√≥n deseada.

#### Alternativa a paso 1 y 2

Cuando el generador de texto aleatorio no ofrece determinado idioma, y si se traduce textualmente no favorece a la sonoridad _conocida_ conviene utilizar ChatGPT ‚Äìu otro modelo de lenguaje‚Äì para generar texto en el idioma deseado.

Por ejemplo, para turco esta fue la conversaci√≥n con ChatGPT:

`üí¨ generate a random turkish text of 100 words`

  > Certainly, here's a random 100-word gibberish text in Turkish:
  >
  > "Gibberish metinleri, yabancƒ± dillerin seslerini taklit eden ilgin√ß bir yolculuƒüa davet ediyor. Zƒ±pƒ±r zƒ±pƒ±r harf sƒ±ralamalarƒ±, d√º≈ü√ºnce d√ºnyasƒ±nƒ± farklƒ± bir evrene ta≈üƒ±yor. Bu tuhaf dilin anlamƒ± yok, ama onun sesleri ve ritmi ≈üa≈üƒ±rtƒ±cƒ± derecede ger√ßek√ßi. Ku≈ülar zƒ±pƒ±rdƒ±yor, r√ºzgar zƒ±pƒ±rdƒ±yor ve g√ºne≈ü yava≈ü√ßa batƒ±yor. Bu yazƒ±larƒ± anlamasanƒ±z bile, zƒ±plamak istediƒüiniz hissine kapƒ±lacaksƒ±nƒ±z. Haydi, zƒ±playalƒ±m ve bu dilin sƒ±rlarƒ±nƒ± ke≈üfedelim. Zƒ±pƒ±r zƒ±pƒ±r zƒ±p, zƒ±pƒ±r zƒ±pƒ±r zƒ±p, zƒ±pƒ±r zƒ±pƒ±r zƒ±p!"
  >
  > Bu yazƒ± tamamen anlamsƒ±zdƒ±r ve T√ºrk√ße ile ilgisi yoktur, ama ses ve harf sƒ±ralamalarƒ± T√ºrk√ßeyi taklit etmektedir.

`üí¨ make it sound more turkish not so gibberish`

  > Certainly, here's a 100-word text in Turkish that may not be gibberish but is still non-sensical:
  >
  > "Bug√ºn g√ºzel bir g√ºn. G√ºne≈ü parlƒ±yor, ku≈ülar cƒ±vƒ±l cƒ±vƒ±l ≈üarkƒ±lar s√∂yl√ºyor. Etrafta ye≈üil tepeler ve mavi g√∂ky√ºz√º ile huzur dolu bir manzara var. ƒ∞nsanlar g√ºler y√ºzl√º, sokaklarda gezip dola≈üƒ±yorlar. Ama aynƒ± zamanda, kƒ±rmƒ±zƒ± armutlar d√∂k√ºl√ºyor aƒüa√ßlardan ve ≈üeftali √ßi√ßekleri kƒ±≈üƒ±n a√ßƒ±yor. Her ≈üey m√ºthi≈ü bir uyum i√ßinde, ama aslƒ±nda bu tamamen hayal √ºr√ºn√º bir d√ºnya. Biraz daha yakƒ±ndan bakƒ±nca, aniden ye≈üil elma yaƒümurunun yaƒüdƒ±ƒüƒ±nƒ± ve √ßikolata √ßi√ßeklerinin g√ºl√ºmsediƒüini g√∂receksiniz. Her ≈üey m√ºmk√ºn bu harika d√ºnyada."
  >
  > This text is constructed to sound more like a coherent passage in Turkish, even though the content is still nonsensical.

`üí¨ make another one, emphasizing how turkish it may sound when spoken aloud`

  > Certainly, here's a 100-word text in Turkish that emphasizes the Turkish language's phonetic and rhythmic qualities when spoken aloud:
  >
  > "G√ºn doƒüar, ku≈ülar kanat √ßƒ±rpar, r√ºzgar hafif√ße eserken, doƒüanƒ±n melodisi kulaklarƒ± √ßƒ±nlatacak kadar g√ºzeldir. Denizin dalgalarƒ± sahile vurur, √ßocuklar g√ºlerek oynar, ya≈üamƒ±n ritmi hƒ±zlƒ±ca akar. K√∂yde simit√ßiler simitlerini taze taze satar, √ßaycƒ±lar √ßaylarƒ±nƒ± demler ve √ßay bardaklarƒ±nƒ± birer birer dizerek servis eder. Sohbetler kahkahalarla dolup ta≈üar, t√ºm renkler birbirine karƒ±≈üƒ±r ve g√ºl√º≈üler y√ºrekleri ƒ±sƒ±tƒ±r. Her bir kelime, her bir hece, bu g√ºzel dili konu≈üurken sanki birer nota gibi √ßalar. T√ºrk√ße, kulaklara ho≈ü gelen bir ≈üarkƒ±dƒ±r."
  >
  > This text aims to capture the phonetic and rhythmic qualities of the Turkish language when spoken aloud, while the content remains non-sensical.

Los √∫ltimos dos ejemplos son los que luego pas√© por TTSmaker y utilizados en el [experimento de la cartograf√≠a con ASt](#ast-explorer).
