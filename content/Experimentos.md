# Experimentos

Para crear un software de habla que no diga nada necesitaba realizar un desarrollo que logre generar sonidos en tiempo real, similar a un instrumento, pero de habla. El puntapi√© para el comienzo del desarrollo fue la b√∫squeda de modelos de machine learning que puedan generalizar la voz y devolvernos sonidos, similar a lo que hace un sistema TTS, con la diferencia de que yo no quer√≠a darle contenido sem√°ntico a trav√©s de texto, y tampoco deb√≠a hablar correctamente, es decir, hacer sentido en su dicci√≥n. Buscaba que suene como si hablara, pero que no haya palabras reconocibles por dilucidar en lo que se dice, quiz√°s solamente en qu√© idioma se dice.

Esta idea de sintetizador de habla as√©mica donde se pueden elegir lenguas y mezclarlas, o incluso interpolarlas, a trav√©s de una simple interfaz de control requerir√≠a cierto software, pero imagin√© un devenir f√≠sico que lo acompa√±e para su presentaci√≥n y operaci√≥n que provea la experiencia de escucha. Este [proyecto particular](#gla-10b-generador-de-lenguaje-as√©mico-medialab-cceba-) fue mencionado en el programa de [Apoyo a la Producci√≥n 2023](https://www.cceba.org.ar/medialab/apoyo-a-la-produccion-2023) del MediaLab [CCEBA](https://www.cceba.org.ar/).

A lo largo de la investigaci√≥n y el desarrollo, utilic√© tecnolog√≠as de inteligencia artificial en distintos medios y aplicaciones. Comenzando con los [LLMs](#LLM), colabor√© principalmente utilizando la versi√≥n gratuita de [ChatGPT](https://chat.openai.com/), desarrollado por [OpenAI](https://openai.com/), que en este momento es el modelo `gpt-3.5-turbo`. Est√° entrenado con informaci√≥n recopilada de todo internet hasta septiembre de 2021. Comprend√≠ que si se le da contexto y se le define un rol responde de manera muy precisa. Especialmente en tareas peque√±as y espec√≠ficas, pero tambi√©n como puntapi√© inicial hacia las sub-investigaciones que fui llevando a cabo a lo largo del proceso.

Dada la limitaci√≥n del modelo gratuito de OpenAI, tambi√©n recurr√≠ a otros LLMs como [Bing Chat](https://www.bing.com/), de [Microsoft](https://www.microsoft.com/), que cuenta con la habilidad de buscar en internet, de hecho, es un buscador similar a [Google](https://www.google.com/), pero con _quien_ uno puede establecer una conversaci√≥n donde afinar sus resultados. El gigante de internet Google tambi√©n cuenta con [Bard](https://bard.google.com/chat), pero de momento se encuentra en una etapa experimental, muy revisada por humanos y con poco soporte en idiomas que no sean ingl√©s. Por √∫ltimo prob√© [LLaVA](https://llava.hliu.cc/): Large Language and Vision Assistant, que si bien promete y es de c√≥digo abierto, no cuenta con conversaciones que pueda almacenar en hilos de contexto como s√≠ lo hace ChatGPT, teniendo que iniciar nuevas conversaciones en cada sesi√≥n de uso.

A continuaci√≥n relatar√© los experimentos que realic√© en funci√≥n al objetivo del habla as√©mica. Si bien dejar√© las definiciones duras para el final, el relato se ver√° entrelazado con cuestiones t√©cnicas propias del desarrollo.

## Interpolaci√≥n de lenguas

Leandro Garber me recomend√≥ que un buen experimento inicial es entrenar un modelo con la arquitectura Autoencoder, en particular uno desarrollado por Pablo Riera (Riera et¬†al., 2017). Al ser alimentado con audio de idiomas hablados, tuvimos la hip√≥tesis de que este podr√≠a generalizar lo suficiente sus caracter√≠sticas como para luego generar nuevos resultados sonoros sin necesidad de contenido sem√°ntico, qued√°ndose s√≥lo con la prosodia de la lengua.

El desarrollo de Riera no s√≥lo consiste en un modelo que puede ser entrenado con archivos de audio, sino que cuenta adem√°s con una especie de sintetizador de inferencia de audio a partir del modelo que se le cargue. Un modelo es en esencia un archivo en la computadora que este software puede cargar y ofrecer en su interfaz una forma de interactuar con su espacio latente: esa abstracci√≥n de pocas dimensiones que permite manipular esa generalizaci√≥n de lo que entiende por idioma hablado en forma de sonido.

Para entrenar el modelo necesit√© un dataset o conjunto de datos preparados o formateados de forma precisa. El primero con el que trabaj√© fue [common_language](https://huggingface.co/datasets/common_language). Est√° compuesto por grabaciones de voz de idiomas seleccionados cuidadosamente de la base de datos [Mozilla CommonVoice](https://commonvoice.mozilla.org/). La duraci√≥n total de las grabaciones de audio es de 45.1 horas (1 hora de material para cada idioma). Est√° destinado a entrenar sistemas de identificaci√≥n de idiomas.

A continuaci√≥n algunos ejemplos de los archivos incluidos en el dataset. Son archivos `.wav`, es decir que no tienen p√©rdida de calidad:

<div class="grid">
  <div class="full-width center">
    <audio controls src="_media/common_voice_fr_19598591.wav" title="common_voice_fr_19598591.wav"></audio>
  </div>
  <div class="full-width center">
    <audio controls src="_media/common_voice_fr_19598240.wav" title="common_voice_fr_19598240.wav"></audio>
  </div>
  <div class="full-width center">
    <audio controls src="_media/common_voice_fr_19140964.wav" title="common_voice_fr_19140964.wav"></audio>
  </div>
</div>

N√≥tese que los fragmentos est√°n dichos por diferentes personas, con distintas calidades y condiciones de grabaci√≥n. Esto se debe a que es necesaria esta variabilidad para enriquecer el modelo, que logre generalizar el habla y la voz humana, pero a ninguna voz en particular.

Para entrenar el Autoencoder deb√≠a alimentarlo con un s√≥lo archivo de audio. El dataset cuenta con una carpeta por idioma con peque√±os fragmentos, por lo que con el siguiente comando en la terminal de la computadora logr√© concatenar inmediatamente todos los archivos de una carpeta en un archivo de audio √∫nico:

```bash
# Los archivos est√°n nombrados de la siguiente forma:
# common_voice_<lang>_<n√∫mero>.wav
# Por ejemplo: common_voice_fr_19140964.wav
$ sox common_* frances.wav
# Obtengo `frances.wav` como resultado.
```

`sox` es una utilidad para manipular audio desde la consola, funciona en macOS, Linux y Windows. De esa forma evit√© el trabajo manual de concatenar todos los archivos.

<audio controls src="_media/french.mp3" title="Archivo de audio mp3 61.8kbps VBR para fines demostrativos, pero se utiliz√≥ el wav para entrenar el modelo."></audio>

<p class="caption center">Archivo de audio mp3 61.8kbps VBR para fines demostrativos, pero se utiliz√≥ el wav para entrenar el modelo.</p>

Lo mismo pero para el Espa√±ol:

<div class="grid">
  <div class="full-width center">
    <audio controls src="_media/common_voice_es_18681301.wav" title="common_voice_es_18681301.wav"></audio>
  </div>
  <div class="full-width center">
    <audio controls src="_media/common_voice_es_19125377.wav" title="common_voice_es_19125377.wav"></audio>
  </div>
  <div class="full-width center">
    <audio controls src="_media/common_voice_es_19602100.wav" title="common_voice_es_19602100.wav"></audio>
  </div>
</div>

Y su versi√≥n concatenada:

<audio controls src="_media/spanish.mp3" title="Archivo convertido a mp3 con ffmpeg a 61.7kbps VBR"></audio>

<p class="caption center">Archivo convertido a mp3 con ffmpeg a 61.7kbps VBR</p>

Para entrenar el modelo prepar√© un Colab con el c√≥digo a ejecutar y un entorno donde ejecutarlo, que pod√≠a ser la nube (Google) o mi computadora. Luego de realizar unos _benchmarks_ -pruebas de rendimiento- observ√© que mi computadora ser√≠a un poco m√°s r√°pida que los recursos que ofrece el _free tier_ de Google Colab. As√≠ que instal√© Anaconda para montar un entorno con la versi√≥n requerida de Python y todas las dependencias requeridas para poder ejecutar mi propio Colab, incluso la configuraci√≥n de seguridad y redes para poder acceder remotamente desde cualquier lado, como si tuviera mi propia nube en mi casa, manteniendo la soberan√≠a de mi sistema computacional.

<a id="multi-lengua-training"></a>

Realic√© el entrenamiento con los archivos de audio concatenados que arm√© en la etapa del dataset. Esto result√≥ en modelos _uni-lengua_ ya que s√≥lo podr√≠an generalizar el habla de una sola lengua. Por esto fue que el siguiente paso fue realizar un proceso de entrenamiento con dos lenguas que, como explicar√© [m√°s adelante](#multi-lengua), podr√≠a generar una cruza interesante. Concaten√© una conversaci√≥n en Franc√©s con otra en Japon√©s como dataset para luego generar un modelo _multi-lengua_ o en este primer caso biling√ºe.

Ahora bien, luego de m√∫ltiples intentos de que el proceso de entrenamiento funcione, sumado a las horas que este proceso demora, obtuve mi archivo de idioma. ¬øUn archivo que representa un idioma? ¬øSer√≠a as√≠ de simple? ¬øEn unas horas la computadora sabr√≠a lo suficiente para producir sonidos similares a una persona hablando Franc√©s?

El proceso de entrenamiento que utilic√© incluye la posibilidad de reconstruir el audio de entrada utilizado para el entrenamiento como un resultado generado por el mismo modelo, de manera que pod√≠a r√°pidamente saber si este al menos podr√≠a recrear el mismo material con lo que fue entrenado. Esto es una forma de hacer [inferencia](#Inferencia).

Dado este audio:

<audio controls src="_media/common_voice_es_19609042.wav" title="Audio original alimentado a la red"></audio>

Se reconstruy√≥ el siguiente audio directamente desde el c√≥digo. Para eso se insertan en el espacio latente del modelo los mismos valores que devuelve el entrenamiento. Resultando en:

<audio controls src="_media/reconstructed.mp3" title="Audio reconstruido"></audio>

![Espectrograma del resultado reconstruido.](../_media/predicted_spectrogram.png "Espectrograma del resultado reconstruido.")

![Camino de valores del espacio latente de 4 dimensiones que genera el audio reconstruido.](../_media/Z_latent_score.png "Camino de valores del espacio latente de 4 dimensiones que genera el audio reconstruido.")

Al solicitarle inferencia al modelo para generar nuevos sonidos a partir del entrenamiento con los mostrados anteriormente, los resultados no eran muy favorables para mi investigaci√≥n por su alto nivel de ruido, por lo que decid√≠ buscar videos en YouTube de Podcasts o conversaciones para armar un nuevo dataset. Encontr√© voces limpias de gente con buena calidad de grabaci√≥n, sent√≠ que con estos el modelo podr√≠a generalizar voces m√°s n√≠tidas.

- Podcast en franc√©s. mp3 55.7kbps VBR para fines demostrativos:

  <audio id="podcast-frances" controls src="_media/french-clean.mp3" title="Audio extra√≠do de https://www.youtube.com/watch?v=N55d9mPm_HA"></audio>

  <p class="caption center">Audio extra√≠do de https://www.youtube.com/watch?v=N55d9mPm_HA</p>

- Japon√©s:
  
  <audio controls src="_media/japanese-clean.mp3" title="Title"></audio>

  <p class="caption center">Audio extra√≠do de https://www.youtube.com/watch?v=trySjnuJfJY</p>

### Espacio Latente

La idea es explorar este espacio para encontrar nuevos [_caminos_](#caminos) que extraigan las caracter√≠sticas principales del sonido con el que fue entrenado el modelo. Con caminos me refiero a datos en serie de tantas dimensiones como tenga el espacio latente ‚Äìn√∫meros‚Äì que representan esta exploraci√≥n, y al ser decodificados generan nueva informaci√≥n con las caracter√≠sticas principales de sus datos de entrada.

<a id="asmrsynth"></a>

Para facilitar la exploraci√≥n contamos con el [**2ASMRSynth**](https://github.com/pabloriera/2ASMRS) tambi√©n programado por Pablo Riera. Es una especie de sintetizador basado en [JUCE](https://juce.com/) que puede generar inferencia sonora en tiempo real a partir de un modelo de tipo [`torchscript`](https://pytorch.org/docs/stable/jit.html). Como expliqu√© anteriormente, uno carga el modelo en la aplicaci√≥n y puede mover los par√°metros dentro del espacio latente y escucharlo en tiempo real, como un instrumento. La sonoridad de este instrumento depender√° del modelo cargado.

![2ASMRSynth Sintetizador de inferencia de modelos](../_media/2ASMRSynth.png "Captura de pantalla del 2ASMRSynth corriendo en Linux Mint.")

Cada _fader_ del instrumento representa una de las dimensiones del espacio latente, y al moverlos le estamos asignando valores a cada una. La parte del decodificador del modelo genera la salida de audio. Mover estos valores manualmente, por ejemplo con el mouse, no resulta interesante ya que se debe orquestar un movimiento coordinado de todos los par√°metros para que la salida haga _sentido_. Por supuesto que cuenta con la posibilidad de ser controlado mediante el protocolo MIDI.

En principio, junto con Leandro Garber, pensamos en enviar datos aleatorios de caminos posibles para comenzar a ver qu√© hab√≠a en ese espacio latente. Para eso armamos un peque√±o patch en Pure Data para enviar los valores en tiempo real al modelo y que este genere audio a partir de los datos de entrada. El algoritmo aleatorio que utilizamos fue el de [**borracho**](https://en.wikipedia.org/wiki/Random_walk) o _drunk_, que consiste en moverse aleatoriamente en pasos de a uno en cualquier direcci√≥n y sentido. El resultado fue un sonido ca√≥tico, pero interesante, que se puede escuchar en la primera parte del siguiente video. En la segunda parte intent√© cargar los datos de entrada del entrenamiento para ver si pod√≠a reconstruir el audio original, volveremos sobre esto en las pruebas siguientes.

<video width="100%" controls src="_media/French4dim_drunkYlatent.mp4" title="French4dim_drunkYlatent"></video>

<p class="caption">Exploraci√≥n de modelo de 4 dimensiones con algoritmo Drunk</p>

<a id="multi-lengua"></a>

Con el modelo [multi-lengua](#multi-lengua-training) la idea fue alimentar las dimensiones del espacio latente con los datos de entrenamiento de un modelo entrenado en Franc√©s anteriormente. La pregunta era ¬øqu√© pasar√≠a si a un modelo biling√ºe le hago _hablar_ uno de los idiomas que conoce?

<audio controls src="_media/JF-MODEL_japanese-fantasmagorico.mp3" title="JF-MODEL_japanese-fantasmagorico"></audio>

<p class="caption center">JF-MODEL_japanese-fantasmagorico.mp3</p>

Este audio es el resultado de alimentar el modelo biling√ºe con los datos de entrenamiento que representan los movimientos dentro del espacio latente para segmento de habla en Franc√©s.

Las siguientes pruebas consisten en cargar los modelos entrenados en el sintetizador _2ASMRSynth_ y luego alimentar los par√°metros del modelo (n√≥tese el ca√≥tico movimiento de los faders) con una lista de cada valor en el tiempo. En el siguiente ejemplo son 8 listas, una para cada dimensi√≥n del espacio latente representada en los faders.

<video width="100%" controls src="_media/French8dim.mp4" title="French8dim"></video>

<p class="caption center">Franc√©s 8 dimensiones</p>

Para lograr esto se desarroll√≥ un patch de Pure Data con el que se cargan los valores num√©ricos de un archivo de texto (generado durante la etapa de entrenamiento) y se _mapean_ o traducen a controles MIDI (rango de 0-127).

<video width="100%" controls src="_media/yYvX1yVfKI.mp4" title="Exploraci√≥n ca√≥tica"></video>

<p class="caption center">Franc√©s 4 dimensiones recorrido con algoritmo 'drunk'</p>

Esta herramienta que posibilita la ejecuci√≥n en tiempo real del modelo trae un problema inherente de su m√©todo de control: el protocolo MIDI est√° limitado a valores de 0-127. Esta resoluci√≥n no es lo suficientemente precisa para _expresar_ la voz en cada una de sus variables internas del espacio latente. Como resultado sonoro aparecen sonidos similares a una voz cuyos movimientos son reminiscentes a algo artificial, quiz√°s medio rob√≥tico, con saltos abruptos en su calidad vocal <em class="rainbow-text">con con</em>sonantes <em class="rainbow-text">con</em>stantes. La caracter√≠stica fantasmag√≥rica, a su vez, est√° producida por el problema de la aplicaci√≥n de una fase aleatoria en el audio. El asunto de la fase es un t√≠pico problema en la generaci√≥n de audio por IA que explicar√© posteriormente en los [Conceptos T√©cnicos](#problema-fase).

## Separaci√≥n en fonemas/fragmentos

Luego de haber aprendido lo suficiente acerca del workflow y las tecnicalidades de la tecnolog√≠a de inteligencia artificial con la que encar√© el proyecto (el modelo Autoencoder de Riera y su sintetizador compa√±ero int√©rprete), llegu√© a resultados que no eran exactamente los que esperaba. Por un momento me detuve a contemplar la diferencia entre lo que esperaba y lo que suced√≠a, ¬øfr*acaso acaso*?. Empec√© a dudar de si lo que hab√≠a pensado ‚Äìcrear un sinsentido a partir de un modelo l√≥gico‚Äì ten√≠a sentido, pero mi instinto me obligaba a probarlo para poder llegar a esta conclusi√≥n.

Inmediatamente, as√≠ como fabul√© este desarrollo y sus resultados, comenc√© a hilar un plan B m√°s elemental, uno que quiz√°s tendr√≠a que haber sido la primera prueba, pero mi predicci√≥n acerca del resultado sonoro era que no iba a ser muy preciso, o escalable, o lo que en mi imaginario buscaba. Esta alternativa consiste en separar grabaciones de habla en fragmentos de unidades b√°sicas que la componen. Un **fonema** es la unidad b√°sica de sonido en un idioma o sistema de comunicaci√≥n. Representa un sonido distintivo que puede cambiar el significado de una palabra en ese idioma, como sabemos a partir del estudio de la fonolog√≠a. Pueden variar de un idioma a otro y a menudo se representan con s√≠mbolos fon√©ticos en la ling√º√≠stica para describir c√≥mo se pronuncian en el habla _(ej. /u/, /s/)_.

Estos recortes vocales, audios segmentados en fonemas, o peque√±as porciones, ser√≠an importados en AudioStellar, lo cual brinda dos resultados/piezas que me interesa observar. Por un lado, un mapa de sonidos que quiz√°s est√©n re-ordenados por sus cualidades sonoras y no por grupos de idiomas, trazando una cartograf√≠a diferente en el espacio sonoro que en el geogr√°fico. Adem√°s y principalmente, la capacidad de recorrer estos fonemas de las diversas formas que ofrece AudioStellar, haciendo que estos suenen de manera concatenada, por lo que formar√≠an habla.

La distribuci√≥n de los fonemas en el mapa 2D de AudioStellar es lo que define esta experiencia. He aqu√≠ el _trade-off_: una cartograf√≠a diferente a la geopol√≠tica de las lenguas versus la interpolaci√≥n de idiomas al recorrer los bordes entre grupos de fonemas. Paso a explicar:

- Si los fonemas se agrupan geopol√≠ticamente, ie. por idioma, ser√° posible recorrer un idioma espec√≠fico a trav√©s de sus fonemas, sin hacer sentido, pero utilizando contenido -sonidos- propios de un s√≥lo idioma. Esto nos da la ventaja de tambi√©n poder escuchar qu√© hay en los bordes de cada grupo de fonemas e interpolar entre idiomas cercanos.
<a name="Cartograf√≠a Mundial del Habla" id="cartografias-explicacion"></a>
- Si se agrupan por sus caracter√≠sticas sonoras, se podr√°n contemplar fonemas similares de los distintos idiomas, se barren las barreras geopol√≠ticas, trazando un mapa donde el sonido hace nuestro sentido (ref sound of nonsense), es nuestra fuente de verdad y no vamos a interpolar entre idiomas sino entre sonidos de diferentes or√≠genes, agrupados por sus cualidades similares. As√≠ encontramos una nueva [_Cartograf√≠a Mundial del Habla_](#audiostellar-puredata-experimento-32).

Organic√© una serie de [entrevistas](#Entrevistas) o encuentros con personas con experiencia en la generaci√≥n de habla por computadora a modo de extender la mano y salir de mis propias ideas. En todos los casos fueron mencionados los [TTS](#TTS). Con el entusiasmo de las conversaciones, indagu√© brevemente en el famoso [Tacotron](https://pytorch.org/hub/nvidia_deeplearningexamples_tacotron2/). Este modelo de generaci√≥n de habla es muy popular en la comunidad de inteligencia artificial y se basa en la arquitectura de un _encoder_ y un _decoder_ que se entrenan conjuntamente para convertir texto en habla. Aunque no profundic√© en su uso, obtuve algunos r√°pidos resultados sonoros bas√°ndome en [este Colab](https://colab.research.google.com/github/Tomiinek/Multilingual_Text_to_Speech/blob/master/notebooks/multi_training_demo.ipynb).

<div class="grid">
  <div class="center full-width">
    <audio controls src="_media/tacotron0-spanish.wav" title="Tacotron Espa√±ol"></audio>
    <p class="caption center mt-1">Tacotron Espa√±ol</p>
  </div>
  <div class="center full-width">
    <audio controls src="_media/tacotron1-french.wav" title="Tacotron Franc√©s"></audio>
    <p class="caption center mt-1">Tacotron Franc√©s</p>
  </div>
  <div class="center full-width">
    <audio controls src="_media/tacotron2-german.wav" title="Tacotron Alem√°n"></audio>
    <p class="caption center mt-1">Tacotron Alem√°n</p>
  </div>
  <div class="center full-width">
    <audio controls src="_media/tacotron3-greek.wav" title="Tacotron Griego"></audio>
    <p class="caption center mt-1">Tacotron Griego</p>
  </div>
  <div class="center full-width">
    <audio controls src="_media/tacotron4-dutch.wav" title="Tacotron Holand√©s"></audio>
    <p class="caption center mt-1">Tacotron Holand√©s</p>
  </div>
  <div class="center full-width">
    <audio controls src="_media/tacotron5-finnish.wav" title="Tacotron Finland√©s"></audio>
    <p class="caption center mt-1">Tacotron Finland√©s</p>
  </div>
  <div class="center full-width">
    <audio controls src="_media/tacotron6-hungarian.wav" title="Tacotron H√∫ngaro"></audio>
    <p class="caption center mt-1">Tacotron H√∫ngaro</p>
  </div>
  <div class="center full-width">
    <audio controls src="_media/tacotron7-chinese.wav" title="Tacotron Chino"></audio>
    <p class="caption center mt-1">Tacotron Chino</p>
  </div>
  <div class="center full-width">
    <audio controls src="_media/tacotron8-japanese.wav" title="Tacotron Japon√©s"></audio>
    <p class="caption center mt-1">Tacotron Japon√©s</p>
  </div>
  <div class="center full-width">
    <audio controls src="_media/tacotron9-russian.wav" title="Tacotron Ruso"></audio>
    <p class="caption center mt-1">Tacotron Ruso</p>
  </div>
</div>

El _prompt_, es decir, el texto que <em class="rainbow-text">hablan</em> estas voces fue el siguiente:
  
  ```python
  inputs = [
          "fdhjk dfshjkf dsahjk fdsahjfds hjklfds sfhjkds fhsjfsehfcnvvnenvwci hznvjei nwifnewau idjksv ncjdw adjsfncv.|spanish",
          "fdhjk dfshjkf dsahjk fdsahjfds hjklfds sfhjkds fhsjfsehfcnvvnenvwci hznvjei nwifnewau idjksv ncjdw adjsfncv.|french",
          "fdhjk dfshjkf dsahjk fdsahjfds hjklfds sfhjkds fhsjfsehfcnvvnenvwci hznvjei nwifnewau idjksv ncjdw adjsfncv.|german",
          "fdhjk dfshjkf dsahjk fdsahjfds hjklfds sfhjkds fhsjfsehfcnvvnenvwci hznvjei nwifnewau idjksv ncjdw adjsfncv.|greek",
          "fdhjk dfshjkf dsahjk fdsahjfds hjklfds sfhjkds fhsjfsehfcnvvnenvwci hznvjei nwifnewau idjksv ncjdw adjsfncv.|dutch",
          "fdhjk dfshjkf dsahjk fdsahjfds hjklfds sfhjkds fhsjfsehfcnvvnenvwci hznvjei nwifnewau idjksv ncjdw adjsfncv.|finnish",
          "fdhjk dfshjkf dsahjk fdsahjfds hjklfds sfhjkds fhsjfsehfcnvvnenvwci hznvjei nwifnewau idjksv ncjdw adjsfncv.|hungarian",
          "fdhjk dfshjkf dsahjk fdsahjfds hjklfds sfhjkds fhsjfsehfcnvvnenvwci hznvjei nwifnewau idjksv ncjdw adjsfncv.|chinese",
          "fdhjk dfshjkf dsahjk fdsahjfds hjklfds sfhjkds fhsjfsehfcnvvnenvwci hznvjei nwifnewau idjksv ncjdw adjsfncv.|japanese",
          "fdhjk dfshjkf dsahjk fdsahjfds hjklfds sfhjkds fhsjfsehfcnvvnenvwci hznvjei nwifnewau idjksv ncjdw adjsfncv.|russian",
  ]
  ```

Si bien el resultado est√©tico y formal de estas voces me result√≥ divertido, no se adecuaba exactamente en mi investigaci√≥n de la invenci√≥n de nuevos idiomas dado que la manera en la que estos modelos trabajan es en diferido, y mi capricho por la generaci√≥n en tiempo real me llev√≥ a buscar otras alternativas. En este punto, la idea de separar en fonemas y recorrerlos en AudioStellar me pareci√≥ una salida exploratoria m√°s directa.

La [entrevista con Mateo Amaral](#mateo-amaral) me hizo pensar en el uso no convencional de las herramientas, en este caso tecnolog√≠as de generaci√≥n de habla como los TTS, como un recurso propio de nuestro proceso creativo como artistas trabajando en piezas que luego componen obras. A lo largo del desarrollo de estos experimentos que se describen a continuaci√≥n, los distintos pasos me hacen pensar en _trucos_ que uso para perseguir mi determinada b√∫squeda est√©tica.

Al llegar a [TTSmaker.com](http://ttsmaker.com/) y gracias a la charla con Mateo, la idea de no involucrar texto me pareci√≥ caduca. Haciendo pruebas con todas las herramientas online que encontr√©, ide√© un flujo que me pareci√≥ interesante en cuanto a proceso y sus resultados. Escribir sus instrucciones como _performance inform√°tica_ es un nuevo devenir de este proceso que describo a continuaci√≥n. Invito y celebro su interpretaci√≥n y modificaci√≥n.

### Instrucciones para un experimento TTS _(Experimento #31)_

1. Generar texto aleatorio en [RandomTextGenerator](https://www.randomtextgenerator.com/) en alg√∫n idioma.

    a. Si no est√° el idioma, [DeepL](https://www.deepl.com/translator) o bien [Google Translate](https://translate.google.com/) para traducir al idioma deseado.

    b. Revisar el texto generado utilizando un traductor como el mencionado, para ver su contenido. En lo posible que no repita tantas palabras, ni que haga sentido. Si hay coherencia en las oraciones pero el contenido del mensaje es surreal, vale para los prop√≥sitos de esta investigaci√≥n.

2. Pasar por [Gibberish Generator](https://thinkzone.wlonk.com/Gibber/GibGen.htm)

    a. Revisar Gibberish Level. Probar en 3. Si el texto no parece muy coherente, o est√° muy mezclado (juzgado a partir de una noci√≥n del idioma) o el resultado sonoro ‚Äúno coincide tanto con el imaginario de esa lengua‚Äù subir un punto.

3. Generar habla con [TTSMaker](https://ttsmaker.com/)

    a. Probar distintas voces

    b. Intentar que el audio resultante est√© entre 30 y 40 segundos

4. La estructura de carpetas para la organizaci√≥n de sonidos resultantes podr√≠a ser la siguiente:

  ![Estructura de carpeta TTS](../_media/tts-folder.png "Estructura de carpeta TTS")

  O bien generalizando:

    ```yaml
    <Carpeta de un idioma>
    - Textos.txt # Documento de texto que contiene el texto que es reproducido por el TTS.
    - ttsmaker-file-<fecha> <n√∫mero>.wav # Donde n√∫mero corresponde al p√°rrafo dentro del archivo de texto.
    + <Otra carpeta de otro idioma>
    - Textos.txt
    - ttsmaker-file-<fecha> <n√∫mero>.wav
    ...
    # Tantos como idiomas se deseen agregar.
    ```

5. Un ejemplo del contenido de los archivos de texto es:

  ```txt
  1
  De se so soubemos matada pentadessem. Alimindo comedefez os mas obederraderetado se muda no te se. Cum ques emponge janter. Cartessa esta notimido ho qua te conros te confuncido filigas tu vidos as be. Aponge as manda noivem em te ha olvempesfacominar mente mando. Carra endos ha. Alimpros has manar as em. Apos manteis avellam expleto se cape. Sei irrana dosa pitadadaveo nosa noivo. De mausoube. Papantao. Bar larra cape. La pendivido do imeta seguessemo procebo erra em erradomina neliguietticahindo

  2
  Ja sr litterato tolica patuscadas tratado por sujeitava. Vintens nervoso um fio. Entribue aos mal. Caminhar arreligioso vao rez inda vao religioso vao rez acces levantado ser que estincoes le fez accento tem nos affligioso dor sujeitae. Esmeros tento causara identes. Fazendo pe. Caminhava rajadas um oh fechava. Nos affligir symbolica documentes. Fazenda vao esmeros teu rez accesso um oh fechava. Retento por evitae. Esmerossas trando paiol vereram tal esta vae evidento companto tolica documento t
  ```

6. Recortar archivos de audio con AudioStellar o [`vosk_phoneme_slicer.py`](#vosk_phoneme_slicer) para generar segmentos peque√±os, uno por cada fonema.

   ?> En este caso, utilic√© exclusivamente AudioStellar para esta tarea dado que este separa a partir de transientes. Esta estrategia √∫nica para todos los idiomas permite cierta homogeneidad de c√≥mo los analizamos, sin considerar reglas espec√≠ficas de cada uno. Caso contrario, se deber√≠a conseguir un modelo entrenado por cada lengua ‚Äìo desarrollarlo‚Äì para realizar una separaci√≥n de fonemas espec√≠fica debido a que no todos tienen las mismas reglas fon√©ticas. Esto complejiza y alarga bastante la tarea y no aporta considerablemente al resultado final sonoro. Arm√© un nuevo script para utilizar los m√©todos que AudioStellar utiliza en su c√≥digo para realizar los recortes. Para esto Leandro me indic√≥ en qu√© parte del c√≥digo de ASt se encuentran estas funcionalidades y tom√© lo que necesitaba para armar el script (ver c√≥digo en Ap√©ndice).

  De esta forma, con la estructura de carpetas del paso anterior, se generar√≠an las _tajadas_ de cada idioma en lote, de forma program√°tica, en cuesti√≥n de minutos. S√≥lo se debe especificar los siguientes par√°metros en el script, que son los mismos que ofrece la interfaz de AudioStellar:

  ```python
  input_folder = dir # Directorio contenedor de las carpetas de idiomas
  window_max = 0.10
  window_avg = 0.05
  delta = 0.10
  backtrack = True
  fade = 1000
  normalize = True
  ```

  Para mayor informaci√≥n, revisar la [documentaci√≥n de `librosa`](https://librosa.org/doc/main/generated/librosa.onset.onset_detect.html#librosa.onset.onset_detect).

1. Crear mapa en AudioStellar utilizando MFCC, algoritmo que ignora caracter√≠sticas de pitch ‚Äìideal para voces de diferentes personas‚Äì pero conserva caracter√≠sticas t√≠mbricas ‚Äìprosodia‚Äì de manera que agrupe los fonemas sin importar quien lo diga. MFCC es el algoritmo m√°s cercano a una subjetividad perceptiva del sonido.

2. üí° Experimentar.

?> Este experimento es altamente escalable, es decir que cuanto m√°s largos, o m√°s archivos generados por el TTS se utilicen, habr√° mayor riqueza de fonemas, m√°s contenido por idioma. La cantidad de lenguas, y cu√°les, depender√° de la b√∫squeda sonora del experimento.

<video id="ast-explorer" width="100%" controls src="_media/AudioStellar exploracion manual.mp4" title="Exploraci√≥n manual con units de AudioStellar"></video>

<p class="caption center">Exploraci√≥n manual con units de AudioStellar.</p>

#### Consideraci√≥n sobre idiomas con ideogramas

El Chino, por ejemplo, est√° compuesto por ideogramas, por lo que mezclar sus caracteres no hace mucho sentido. Es decir que no podemos inventar, al menos con este tipo de herramientas, palabras que no existan a la hora de ser pronunciadas. Quiz√°s la oraci√≥n no tenga sentido pero las palabras que la componen s√≠, y esto es diferente a los otros casos dado el algoritmo utilizado en la herramienta que convierte texto a gibberish (ver c√≥digo fuente en el mismo [sitio web](https://thinkzone.wlonk.com/Gibber/GibGen.htm)).
Una estrategia para mezclar el texto y evitar sonidos repetidos es borrar caracteres aleatorios en el texto a reproducir, por lo que se sugiere generar uno largo al cual se le puedan eliminar fragmentos hasta lograr el efecto y la duraci√≥n deseada.

#### Alternativa a paso 1 y 2

Cuando el generador de texto aleatorio no ofrece determinado idioma, y si se traduce textualmente no favorece a la sonoridad _conocida_ conviene utilizar ChatGPT ‚Äìu otro modelo de lenguaje‚Äì para generar texto en el idioma deseado.

Por ejemplo, para turco esta fue la conversaci√≥n con ChatGPT:

<p class="chat A">generate a random turkish text of 100 words</p>

<div class="chat B">
  Certainly, here's a random 100-word gibberish text in Turkish:

  "Gibberish metinleri, yabancƒ± dillerin seslerini taklit eden ilgin√ß bir yolculuƒüa davet ediyor. Zƒ±pƒ±r zƒ±pƒ±r harf sƒ±ralamalarƒ±, d√º≈ü√ºnce d√ºnyasƒ±nƒ± farklƒ± bir evrene ta≈üƒ±yor. Bu tuhaf dilin anlamƒ± yok, ama onun sesleri ve ritmi ≈üa≈üƒ±rtƒ±cƒ± derecede ger√ßek√ßi. Ku≈ülar zƒ±pƒ±rdƒ±yor, r√ºzgar zƒ±pƒ±rdƒ±yor ve g√ºne≈ü yava≈ü√ßa batƒ±yor. Bu yazƒ±larƒ± anlamasanƒ±z bile, zƒ±plamak istediƒüiniz hissine kapƒ±lacaksƒ±nƒ±z. Haydi, zƒ±playalƒ±m ve bu dilin sƒ±rlarƒ±nƒ± ke≈üfedelim. Zƒ±pƒ±r zƒ±pƒ±r zƒ±p, zƒ±pƒ±r zƒ±pƒ±r zƒ±p, zƒ±pƒ±r zƒ±pƒ±r zƒ±p!"

  Bu yazƒ± tamamen anlamsƒ±zdƒ±r ve T√ºrk√ße ile ilgisi yoktur, ama ses ve harf sƒ±ralamalarƒ± T√ºrk√ßeyi taklit etmektedir.
</div>

<p class="chat A">make it sound more turkish not so gibberish</p>

<div class="chat B">
  Certainly, here's a 100-word text in Turkish that may not be gibberish but is still non-sensical:
  
  "Bug√ºn g√ºzel bir g√ºn. G√ºne≈ü parlƒ±yor, ku≈ülar cƒ±vƒ±l cƒ±vƒ±l ≈üarkƒ±lar s√∂yl√ºyor. Etrafta ye≈üil tepeler ve mavi g√∂ky√ºz√º ile huzur dolu bir manzara var. ƒ∞nsanlar g√ºler y√ºzl√º, sokaklarda gezip dola≈üƒ±yorlar. Ama aynƒ± zamanda, kƒ±rmƒ±zƒ± armutlar d√∂k√ºl√ºyor aƒüa√ßlardan ve ≈üeftali √ßi√ßekleri kƒ±≈üƒ±n a√ßƒ±yor. Her ≈üey m√ºthi≈ü bir uyum i√ßinde, ama aslƒ±nda bu tamamen hayal √ºr√ºn√º bir d√ºnya. Biraz daha yakƒ±ndan bakƒ±nca, aniden ye≈üil elma yaƒümurunun yaƒüdƒ±ƒüƒ±nƒ± ve √ßikolata √ßi√ßeklerinin g√ºl√ºmsediƒüini g√∂receksiniz. Her ≈üey m√ºmk√ºn bu harika d√ºnyada."
  
  This text is constructed to sound more like a coherent passage in Turkish, even though the content is still nonsensical.
</div>

<p class="chat A">make another one, emphasizing how turkish it may sound when spoken aloud</p>

<div class="chat B">
  Certainly, here's a 100-word text in Turkish that emphasizes the Turkish language's phonetic and rhythmic qualities when spoken aloud:

  "G√ºn doƒüar, ku≈ülar kanat √ßƒ±rpar, r√ºzgar hafif√ße eserken, doƒüanƒ±n melodisi kulaklarƒ± √ßƒ±nlatacak kadar g√ºzeldir. Denizin dalgalarƒ± sahile vurur, √ßocuklar g√ºlerek oynar, ya≈üamƒ±n ritmi hƒ±zlƒ±ca akar. K√∂yde simit√ßiler simitlerini taze taze satar, √ßaycƒ±lar √ßaylarƒ±nƒ± demler ve √ßay bardaklarƒ±nƒ± birer birer dizerek servis eder. Sohbetler kahkahalarla dolup ta≈üar, t√ºm renkler birbirine karƒ±≈üƒ±r ve g√ºl√º≈üler y√ºrekleri ƒ±sƒ±tƒ±r. Her bir kelime, her bir hece, bu g√ºzel dili konu≈üurken sanki birer nota gibi √ßalar. T√ºrk√ße, kulaklara ho≈ü gelen bir ≈üarkƒ±dƒ±r."

  This text aims to capture the phonetic and rhythmic qualities of the Turkish language when spoken aloud, while the content remains non-sensical.
</div>

Los √∫ltimos dos ejemplos son los que luego pas√© por TTSmaker y utilizados en el [experimento de la cartograf√≠a con ASt](#ast-explorer).

### AudioStellar + PureData (Experimento #32)

Habiendo recorrido las nuevas [Cartograf√≠as Mundiales del Habla](#cartografias-explicacion) con las unidades provistas por AudioStellar, conversando con un amigo que fue parte de su equipo de desarrollo, [Tomas Ciccola](https://szgy.ahh.red/) me sugiere la idea de investigar nuevas formas de recorrer mi mapa mediante [OSC](#OSC) ya que dicho software provee una [API](#API) para ser [controlado con este protocolo](https://gitlab.com/ayrsd/audiostellar/-/blob/units/OSC_Documentation.md).

El mapa 2D de fragmentos de voces est√° categorizado en Clusters: conjunto de sonidos que pueden ser agrupados tanto por cercan√≠a (ajustable con ciertos par√°metros para definir sus caracter√≠sticas) o por la carpeta donde estos archivos residen. Dado que ten√≠a una carpeta por idioma, mis clusters estaban agrupados por idioma üëç. A su vez, est√°n desparramados por todo el mapa ya que la distribuci√≥n de cada punto -sonido- estaba dada por su _parecido t√≠mbrico_ para determinar la cercan√≠a entre s√≠. Esto permit√≠a que tenga la posibilidad de recorrerlos con las unidades de AudioStellar de manera espacial, como [hice antes](#ast-explorer), o bien, llamar por cluster mediante OSC.

```yaml
/play/cluster [clusterName] [[index]] [[volume]]
# Play a sound from a cluster named clusterName. If index is not present AudioStellar will choose a random one; note that the index will cycle through the number of sounds in the cluster. Volume is optional and is between [0,1].
```

Comenc√© a bocetar una aplicaci√≥n utilizando [Plug Data](https://plugdata.org/), una versi√≥n moderna de [Pure Data](https://puredata.info/) implementada con [JUCE](#asmrsynth). La idea era enviar estos mensajes hacia AudioStellar para poder reproducir los sonidos de cada idioma con determinada frecuencia de disparo, de manera que pueda _simular habla_ mezclada al concatenar los fragmentos de voces.

<video width="100%" controls src="_media/maquina idioma1-female.mp4" title="Maquina Idiomas 1"></video>

<p class="caption">Maquina de Idiomas 1</p>

Esta primera versi√≥n de mi patch de control de AudioStellar con Pure Data me permiti√≥ recorrer los clusters de fragmentos mezclados de cada idioma, pero no me permiti√≥ hacerlo de manera aleatoria, sino que deb√≠a elegir un cluster -idioma- y recorrerlo. Esto me llev√≥ a pensar en una nueva versi√≥n que me permitiera recorrer los clusters con determinada probabilidad que el usuario desee, acerc√°ndome nuevamente a la idea de interpolaci√≥n entre idiomas.

### M√°quina Probabil√≠stica _(Experimento #32.1)_

En esta nueva iteraci√≥n sobre el primer experimento con TTS, consider√© algunos factores que favorecer√≠an al resultado sonoro que busqu√© desde el comienzo. Para comenzar, agregu√© al patch de Pure Data la posibilidad de elegir el idioma de forma probabil√≠stica. Con un fader por idioma, el usuario puede elegir cu√°ntas chances hay de que se dispare un fragmento de ese idioma. Adem√°s, tambi√©n se puede controlar la duraci√≥n de este fragmento, y as√≠ controlar la _velocidad_ del habla.

Otra decisi√≥n fue la de quedarme con una [historia de fantas√≠a](#historia) inventada por ChatGPT al mejor estilo de Alicia en el Pa√≠s de las Maravillas. As√≠, podr√≠a tener palabras reales existentes para cada lengua. Cada una de las versiones no fue traducida sino que le ped√≠a nuevamente el texto en el idioma que quisiera luego reproducir con mi voz clonada. En los experimentos anteriores hab√≠a textos de otros generadores de _gibberish_ pero para este caso quer√≠a mantener este sinsentido en el relato, d√°ndole m√°s peso a este gesto. Para cada versi√≥n de la historia en cada lengua, gener√© dos audios, uno con voz femenina y otro con voz masculina, realizando una curadur√≠a t√≠mbrica de voces, y seg√∫n las ofrecidas en TTSMaker. El √∫nico idioma que no pude generar de la misma forma fue el Guaran√≠, pero encontr√© un modelo en [HuggingFace](https://huggingface.co/facebook/mms-tts-grn) que me permiti√≥ generar un fragmento de habla en este idioma de voz masculina.

<a id="historia"></a>

> _En una galaxia muy, muy lejana, los ping√ºinos bailan tango con sand√≠as y estrellas fugaces como luces de ne√≥n cubiertas de chocolate. El sol es como una gran naranja y la luna es como un gran algod√≥n de az√∫car. Los r√≠os fluyen con jarabe de arce y las nubes parecen pasteles de lim√≥n. Los peces saltan del agua y tocan el piano mientras las mariposas pintan dibujos en sus alas. Las carreteras est√°n cubiertas de chocolate derretido y las casas tienen techos hechos a medida. Las estrellas fugaces se convierten en caramelos de frambuesa y los mosquitos tocan el viol√≠n en mitad de la noche. Todo esto es parte de un dulce sue√±o donde las olas del mar est√°n hechas de salsa de caramelo y las estrellas brillan como caramelos de diamantes. De las nubes llueven macarrones y el atardecer es como un cuadro de chocolate. Es un mundo de imaginaci√≥n, donde los arco√≠ris son la escalera hacia los sue√±os celestiales y los atardeceres saben a mil helados diferentes. Los barcos flotan en los r√≠os con caramelo y las monta√±as se convierten en pasteles dulces._

Las voces generadas con TTSMaker fueron recortados con el [script que desarroll√©](#script-para-experimento-31) para generar los segmentos de audio de cada idioma. Esta vez, modificando los par√°metros para obtener fragmentos m√°s largos que aumentaran la inteligibilidad o distinci√≥n de la lengua. Estos fueron importados en AudioStellar y organizados en clusters por idioma al estar organizados por carpeta. Finalmente, con el patch de Pure Data, se control√≥ la reproducci√≥n de estos fragmentos de habla en tiempo real, similar a la iteraci√≥n anterior, pero esta vez los fragmentos de habla son m√°s largos para mantener la integibilidad de cada idioma.

<video width="100%" controls src="_media/maquina chance1 - male.mp4" title="Maquina Probabil√≠stica 1"></video>

Al probar esta aplicaci√≥n, me reson√≥ haber hallado esa multiplicidad de voces en diferentes idiomas, similar a aquel primer disparador de este trabajo al escuchar el bullicio en espacios p√∫blicos de ciudades cosmopolitas. Al barrer entre los idiomas aparece ese coro de personas hablando en diferentes lenguas, fragmentos de conversaciones que se mezclan en el aire. A√∫n no sent√≠ haber llegado a la invenci√≥n de un nuevo idioma, pero s√≠ a una multitud de conversaciones virtuales.

![Patch de control probabil√≠stico por OSC para AudioStellar](../_media/custom_osc_ast.png "Patch de control probabil√≠stico por OSC para AudioStellar")

### Mi propia voz _(Experimento #32)_

A partir de los resultados sonoros de los experimentos anteriores, mi inquietud por el habla as√©mica crece a√∫n m√°s, como si esquivara cualquier forma de conclusi√≥n y ese universo se expandiera hacia un sinf√≠n de posibilidades. Me di cuenta que necesitaba emparejar los timbres de las voces si quer√≠a realmente comenzar a _inventar idiomas_. Pero, ¬øqu√© timbre? ¬øqu√© voz ser√≠a la que represente este nuevo lenguaje vocalizado por computadora? Por supuesto la respuesta apareci√≥ de inmediato, ¬°la m√≠a! ¬øQui√©n sino con mayor disponibilidad y entrega para esta experimentaci√≥n? De aqu√≠ surgen dos posibilidades:

- A partir de los fragmentos de audio del experimento anterior, encontrar alg√∫n sistema Speech-to-Speech que transforme esas voces generadas por TTSMaker a mi propia voz, de manera que el contenido <em class="rainbow-text">a-sem√°ntico</em> sea extra√≠do de esos audios, incluyendo la cadencia, la entonaci√≥n, las palabras y la pronunciaci√≥n, pero no el timbre, que ser√° el de mi voz.
- Clonar mi propia voz y luego con ese modelo inferir (generar audio a partir del modelo) aquellos textos que hab√≠a generado como recurso fuente para los TTS.

De cualquiera de estas maneras lograr√≠a obtener muchos fragmentos de la misma persona hablando una gran cantidad de idiomas con la posibilidad de mezclarlos de forma temporal pero no del todo morfol√≥gica, aunque con las posibilidades que ofrece AudioStellar se acerca bastante a la interpolaci√≥n entre idiomas.

Finalmente decid√≠ pagar una suscripci√≥n de [ElevenLabs](https://elevenlabs.io/) que me permiti√≥ r√°pidamente clonar mi propia voz con alrededor de 5 minutos de grabaci√≥n que hice con un micr√≥fono Shure SM57 sin ruidos de fondo. Si bien hace falta mucho menos tiempo para obtener un modelo de voz, la calidad de la clonaci√≥n es proporcional al tiempo de grabaci√≥n, hasta los 5 minutos (por el momento), luego de eso ya deja de mejorar el resultado (proablemente cause _overfitting_). El relato fue generado en audio en los 29 idiomas que proporciona el modelo **Eleven Multilingual v2** (√°rabe, b√∫lgaro, chino, croata, checo, dan√©s, holand√©s, ingl√©s, filipino, finland√©s, franc√©s, alem√°n, griego, hindi, indonesio, italiano, japon√©s, coreano, malayo, polaco, portugu√©s, rumano, ruso, eslovaco, espa√±ol sueco, tamil, turco, ucraniano).

?> Settings del modelo: **Stability** entre 35% y 50%, **Similarity** entre 75% y 90%, **Style Exaggeration** 0% (porque no afecta en modo TTS, s√≠ en modo STS), **Speaker Boost** encendido.

Hice una prueba poniendo un texto sinsentido, ilegible, golpeando cualquier tecla del teclado, para ver c√≥mo sonaba, ¬øen qu√© idioma lo interpretar√≠a? ü§î

> nvuejia njsogrh nv nifdn sgiurehjns finv nvfdsj gejrk lsnfdlfdsv fhdjsl hjfdsgj fdkls;g jeve v vmdn ncx gfdg nfgv ngdg l!@!@Jl jlj dfwjh
fkdp jJN Jnj  . fd, fm ,wef ejfiow f,f f .fj ikoewfjekofjewf .d fs koejjfio. jfnksoj feowijf ont 32o mnfdsm f. . jfsdjnf #@Rmn nfn ewfneso enn5kkl nmdlf . j jfsf j ,fes, 4ejiofjsiojfs 3 jmkldmfkldsf ,. dsk pJPIO%U$#)( j kljdS%U*()*#) jkgn ,. mfdsko jiow jgj fod3cmcx,lz.,/;vncxv nmzv b eowyterwui bkfs  guireyuwdfhj sbnm,ds .afd m, bbhvcxuidoqyufer

<audio controls src="_media/ElevenLabs_Juan_GIBBERISH.wav" title="ElevenLabs_Juan_GIBBERISH"></audio>
<p class="caption center">Clon Juan Balbuceo</p>

Los siguientes son con la [historia de fantas√≠a](#historia) para cada idioma, algunos ejemplos:

<div class="grid">
  <div class="center full-width">
    <audio controls src="_media/ElevenLabs_Juan_spanish.wav" title="Juan Espa√±ol"></audio>
    <p class="caption center mt-1">Clon Juan Espa√±ol</p>
  </div>
  <div class="center full-width">
    <audio controls src="_media/ElevenLabs_Juan_ingles.wav" title="Juan Ingl√©s"></audio>
    <p class="caption center mt-1">Clon Juan Ingl√©s</p>
  </div>
  <div class="center full-width">
    <audio controls src="_media/ElevenLabs_Juan_aleman.wav" title="Juan Alem√°n"></audio>
    <p class="caption center mt-1">Clon Juan Alem√°n</p>
  </div>
  <div class="center full-width">
    <audio controls src="_media/ElevenLabs_Juan_arabe.wav" title="Juan √Årabe"></audio>
    <p class="caption center mt-1">Clon Juan √Årabe</p>
  </div>
  <div class="center full-width">
    <audio controls src="_media/ElevenLabs_Juan_chino.wav" title="Juan Chino"></audio>
    <p class="caption center mt-1">Clon Juan Chino</p>
  </div>
  <div class="center full-width">
    <audio controls src="_media/ElevenLabs_Juan_frances.wav" title="Juan Franc√©s"></audio>
    <p class="caption center mt-1">Clon Juan Franc√©s</p>
  </div>
  <div class="center full-width">
    <audio controls src="_media/ElevenLabs_Juan_italiano.wav" title="Juan Italiano"></audio>
    <p class="caption center mt-1">Clon Juan Italiano</p>
  </div>
  <div class="center full-width">
    <audio controls src="_media/ElevenLabs_Juan_japones.wav" title="Juan Japon√©s"></audio>
    <p class="caption center mt-1">Clon Juan Japon√©s</p>
  </div>
  <div class="center full-width">
    <audio controls src="_media/ElevenLabs_Juan_koreano.wav" title="Juan Coreano"></audio>
    <p class="caption center mt-1">Clon Juan Coreano</p>
  </div>
  <div class="center full-width">
    <audio controls src="_media/ElevenLabs_Juan_portugues.wav" title="Juan Portug√©s"></audio>
    <p class="caption center mt-1">Clon Juan Portug√©s</p>
  </div>
</div>

!> Dato curioso: estaba resfriado cuando grab√© mi voz, por lo que el modelo clonado tambi√©n ü§ß

<video width="100%" controls src="_media/ASt-ElevenLabs-JM_1.mp4" title="AudioStellar + ElevenLabs"></video>

<p class="caption center">AudioStellar con mi voz clonada en 29 idiomas</p>

La escucha de estos experimentos gener√≥ ideas para mejoras en su _performatividad_. Una nueva _feature_ (caracter√≠stica üöÄ) fue la adici√≥n de rango m√°ximo y m√≠nimo en velocidad de habla, de forma que la velocidad de disparo de cada fragmento sea aleatoria pero controlable y acotada. Esto permiti√≥ que la m√°quina de idiomas sea m√°s _org√°nica_ en su habla, m√°s _humana_, ya que evita r√≠tmicas constantes. Sin embargo, si se acota a determinada velocidad constante, podr√≠a utilizarse a efectos musicales.

![√öltima iteraci√≥n del patch de control adaptado a los idiomas provistos por ElevenLabs para AudioStellar](../_media/custom_osc_ast_elevenLabsJM.png "√öltima iteraci√≥n del patch de control adaptado a los idiomas provistos por ElevenLabs para AudioStellar")

Las pruebas que realic√© me trajeron nuevas sonoridades que no esperaba utilizando √∫nicamente la voz. En principio intent√© acotarme a las formas b√°sicas de sampling (muestreo), es decir, reproducir los audios, m√°s r√°pido o m√°s lento, concatenarlos, que se pisen o no, el orden de los fragmentos, y dem√°s operaciones b√°sicas, ya que no quer√≠a perder la esencia de la voz y ver si lograba nuevas formas de habla. Sin embargo, el conjunto de herramientas y la forma de explorarlas me llev√≥ a ir m√°s all√° y correrme de la b√∫squeda del idioma as√©mico que refleje la voz sino a nuevas sonoridades como experiencia est√©tica sin ni siquiera tener la voz como eje. En esta instancia me llamaron m√°s la atenci√≥n aqellos resultados que se acercaban m√°s al <em class="rainbow-text">noise</em> y se alejaban de la voz, hasta incluso a sonoridades m√°s musicales.

<video width="100%" controls src="_media/ASt-ElevenLabs-JM_1-tekno_websd.mp4" title="Improvisaci√≥n musical con voz"></video>

<p class="caption center">Improvisaci√≥n musical con mi voz generada</p>

Habiendo experimentado con mi propia voz en idiomas que no hablo fue interesante. Adem√°s, la posibilidad de crear modelos de clones con el timbre de la voz de cualquier persona, incluso conocidas con tan s√≥lo tomar audios de WhatsApp, y hacer que digan lo que uno quiera, resulta sorprendente y a la vez escalofriante. Se me presenta como un dilema √©tico del uso de estas herramientas, pero tambi√©n la convierte en una utilidad creativa muy poderosa. Sin embargo, esta es una discusi√≥n que excede el alcance de este trabajo aunque no deja de ser un tema tangencial en cuanto a la generaci√≥n de voces.
